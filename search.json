[
	
		{
		  "title" : "Improving Drag in React JS: A Smoother Approach to Draggable Elements",
		  "category" : "technology",
		  "url" : "/technology/improving-drag-in-reactjs-a-smoother-approach-to-draggable-elements/",
		  "date" : "2024-10-15 00:00:00 +0530",
		  "content"	: "When building interactive web components , such as a draggable element , you might run into the problem where the native draggable attribute leaves the original element in place while dragging a semi-transparent copy. This behavior can feel clunky and disrupts the user experience. In this blog , I’ll show you how to build a smooth draggable component where the element itself follows the mouse pointer , rather than a ghost image.<br /><br /><br />  <br /><br /><br />By default , when you set an HTML element to draggable=&quot;true&quot; , the browser shows a semi-transparent copy of the element that moves with your mouse. While this behavior is native and functional , it often doesn’t look great. You want to make sure that the original element moves smoothly with the cursor without showing the browser’s default ghost image.<br /><br />Solution<br /><br />The solution is to create an overlay element that follows the mouse pointer while hiding the original element during dragging. This approach improves the user experience and creates a smoother drag-and-drop interaction.<br /><br />Step-by-Step Guide<br /><br /><br />  <br />    Setup Initial State<br /><br />    We are going to need some state to handle the Drag &amp;amp; Drop.<br />  <br /><br /><br />const [isDragging , setIsDragging] = useState(false);<br />    const [offset , setOffset] = useState(null);<br />    const [position , setPosition] = useState(null);<br />    const draggableRef = useRef(null);<br /><br />isDragging: This will store the current drag state of the element.<br /><br />offset: When the user starts dragging , they can click anywhere within the draggable element. This stores the coordinate distance between the element’s origin point and the clicked position.<br /><br />position: This will store the current cursor position.<br /><br />draggableRef: We’ll use this to retrieve the origin position of the draggable element.<br /><br /><br />  <br />    Create the Draggable Element<br /><br />    First , let’s set up a simple HTML element to drag:<br />  <br /><br /><br />return (<br />      &#60;;div className=&ldquo;App&ldquo;&#62;;<br />        {<br />          isDragging &amp;amp;&amp;amp; position &amp;amp;&amp;amp; (<br />            &#60;;div<br />              className=&ldquo;draggable-item draggable-overlay&ldquo;<br />              style={ `top: ${position.y}px; left: ${position.x}px`}<br />            &#62;;<br />              Drag Me!<br />            &#60;;/div&#62;;<br />          )<br />        }<br />        &#60;;div<br />          ref={draggableRef}<br />          onMouseDown={handleMouseDown}<br />          className=&ldquo;draggable-item&ldquo;<br />          style={`opacity: ${isDragging ? 0 : 1};`}<br />        &#62;;<br />          Drag Me!<br />        &#60;;/div&#62;;<br />      &#60;;/div&#62;;<br />    );<br /><br /><br />  <br />    Handle Drag Start: Hide the Original and Create an Overlay<br /><br />    We are not going to use native drag events. Instead we will be using onMouseDown event as it will give us more flexibility.<br />  <br /><br /><br />const getInitialPosition = () =&#62;; {<br />      const draggableElement = draggableRef?.current?.getBoundingClientRect();<br />      return {<br />        x: (draggableElement?.x || 0) + window.scrollX , // It will handle edge case when there is scroll in the page.<br />        y: (draggableElement?.y || 0) + window.scrollY ,<br />      };<br />    };<br /><br />    const handleMouseDown = (event) =&#62;; {<br />      setIsDragging(true);<br />      const initialPosition = getInitialPosition();<br />      setOffset({<br />        x: event.clientX - initialPosition.x ,<br />        y: event.clientY - initialPosition.y ,<br />      });<br />      setPosition(initialPosition);<br />      document.addEventListener(&amp;#39;mouseup&amp;#39; , handleMouseUp);<br />    };<br /><br />    useEffect(() =&#62;; {<br />    if (offset) {<br />      document.addEventListener(&amp;#39;mousemove&amp;#39; , handleMouseMove);<br />    } else {<br />      document.removeEventListener(&amp;#39;mousemove&amp;#39; , handleMouseMove);<br />    }<br /><br />    return () =&#62;; {<br />      document.removeEventListener(&amp;#39;mousemove&amp;#39; , handleMouseMove);<br />    };<br />  } , [offset]);<br /><br /><br />  <br />    Move the Overlay with the Mouse<br /><br />    In last step we had added an eventListener on mousemove event. Now , let’s make sure that on mouse move the overlay follows it.<br />  <br /><br /><br />const handleMouseMove = (event) =&#62;; {<br />      if (!isDragging) return;<br /><br />      if (event.clientX &#62;; 0 &amp;amp;&amp;amp; event.clientY &#62;; 0) {<br />        setPosition({<br />          x: event.clientX - (offset?.x || 0) ,<br />          y: event.clientY - (offset?.y || 0) ,<br />        });<br />      }<br />    };<br /><br /><br />  <br />    Clean Up on Drag End<br /><br />    Once the dragging is finished , we need to remove the overlay and make the original element visible again. Also we need to remove the mousemove event listener.<br />  <br /><br /><br />const handleMouseUp = () =&#62;; {<br />      setIsDragging(false);<br />      setPosition(null);<br />      setOffset(null);<br /><br />      document.removeEventListener(&amp;#39;mouseup&amp;#39; , handleMouseUp);<br />    };<br /><br /><br />  <br />    Styling the Draggable &amp;amp; Overlay<br /><br />    Finally , you can style the overlay so that it looks like the original element. This CSS will ensure that the overlay matches the original element’s appearance.<br />  <br /><br /><br />.draggable-item {<br />      border: 2px solid #707070;<br />      padding: 7px 14px;<br />      border-radius: 8px;<br />      width: fit-content;<br />      cursor: grab;<br />      user-select: none;<br />      background-color: coral;<br />    }<br /><br />    .draggable-overlay {<br />      position: absolute;<br />      cursor: grabbing;<br />      box-shadow: 0px 20px 25px -5px #0000001a;<br />    }<br /><br />Conclusion<br /><br />By using an overlay element to follow the mouse and hiding the original element , you can avoid the default ghost image that the browser shows when dragging. This method provides a smoother and more visually pleasing drag-and-drop experience.<br /><br />Feel free to experiment with this solution in your projects , and let me know if you find any other creative ways to enhance the draggable experience!<br />"
		} ,
	
		{
		  "title" : "Book review: Never let me go - Kazuo Ishiguro",
		  "category" : "book review",
		  "url" : "/book-review/book-review-never-let-me-go-kazuo-ishiguro/",
		  "date" : "2024-09-27 00:00:00 +0530",
		  "content"	: "I was recently given 2 books as a gift by a close friend , who is a great fan of Murakami’s. The books in question are “Norwegian woods” by Murakami and “Never Let Me Go” by Kazuo Ishiguro.<br /><br />The task was to read both of these books , and to validate the said friend’s belief that Murakami ,as an author , is far superior to Ishiguro. Being a die-hard Murakami fan , she told me beforehand that “Norwegian Woods” is Murakami’s only story without any magical realism , and probably one of his lesser works , while on the other hand , “Never Let Me Go” is Ishiguro’s finest.<br /><br /><br />  <br /><br /><br />So you see , the balance was already distorted. Comparing Ishiguro’s finest to Murakami’s worst? Only to prove that Murakami was still the better author! Being a fan of neither , but a true friend , I believed what I was told , and picked up “Norwegian Woods” first.<br /><br />While much of what I felt while I read the book is already lost , for it has been around a month since , I remember the writing style quite vividly. “Norwegian Woods” follows a very level-headed and mature protagonist , with the tone of the book being serious mostly.<br /><br />It is a story of the past , present and future. Without going into much detail (since this blog is mostly about the second book) , the protagonist learns to remember and cherish , while also letting go of the past. He fights his demons , and finally accepts and embraces his present , while on the very last page it is hinted that he is finally looking towards his future.<br /><br />I felt like death was looming around the corner , throughout the book. And towards the end , it became quite predictable what was about to happen. Nevertheless , I was still distraught when what was supposed to happen , happened.<br /><br />The sadness that I felt throughout this book made me purchase a whole set of Murakami’s best works. If this is one of his worst books , I expect nothing short of heaven from his best.<br /><br />I next picked up “Never Let Me Go” by Kazuo Ishiguro , the “wannabe japanese” author , as i was told by my friend. I could instantly see the difference in the writing styles. While Murakami’s was a more somber and mature tone , Ishiguro’s was more childlike.<br /><br />With this bias already in my mind , I put the book down only after a few pages. But as luck would have it , I had to sit in a hospital for hours the next couple of days and I found this book in my bag. So , I gave the book another try , expecting to be let down. Needless to say , I couldn’t put down the book before finishing it. It was that good.<br /><br />Being as cautious as I can be about not giving spoilers , I’ll go through the essence of what I went through while I read this gem.<br /><br />“Never Let Me Go” makes you remember your childhood. It makes you remember what being innocent (that’s right , being and not pretending to be) felt like. And it also highlights how you come to lose your innocence. How you finally come to see this world for what it is , instead of what you want it to be.<br /><br />In this regard , there is this specific moment in the book where the students prank “Madame” - only to realize that while they could predict how she would react , they weren’t ready for the way it made them , and especially our protagonist , feel.<br /><br />Like I said above , I found the tone to be child-like. But only after I was engrossed in the book , did I realize that that was how it was supposed to be. Because the tone was child-like , I could feel the innocence of our protagonist. I could really believe that it was a child who was narrating the book.<br /><br />The book makes you go through a variety of emotions - love , jealousy , hatred , apathy , sympathy , and a plethora of others. There were times when I had tears in my eyes reading the book , only to feel hopeful a couple of pages later. And a few more pages down the line , I would go from being hopeful to feeling completely hopeless.<br /><br /><br />  <br /><br /><br />In particular , I loved the character of “Ruth”. A fighter , a leader , a go-getter. Someone who wanted to belong. Someone who wanted to be validated or accepted by others.<br /><br />I went from respecting the child she was , to hating the teenager she grew up to be , to slightly understanding , but also feeling apathy towards the adult she became , to finally coming to terms with the good-hearted person she really was , especially towards the end.<br /><br />I feel there is a “Ruth” inside all of us. All we want is to be accepted and validated. We would like to belong. Many times , I find myself doing things that I normally wouldn’t. Things I do in order to be validated by others.<br /><br />From laughing too hard to a joke that I didn’t really find funny , just to be a part of the group , to being extremely courteous towards elders , because that’s how I am supposed to behave. Not that there’s anything wrong with this , I just realize while doing these things , that I am not being myself.<br /><br />Towards the end , the book made me feel grateful for the freedom that I enjoy. It made me understand <br />How the most important things in our lives , we take for granted! <br />How stupid and shallow and narrow-minded we are.<br />How evil we really are.<br /><br />In summary , “Never Let Me Go” is a masterpiece. While fiction , I found it to be a representation of our current dystopian society. You read that right - <br />Not a “dystopian representation of our current society” but a “representation of our current dystopian society”.<br /><br />Once I was done reading , I immediately called my friend to let her know that Ishiguro was in no way inferior to Murakami.<br /><br />To Mr. Murakami , I look forward to reading your best. I have been suggested to read “Kafka on the shore”. I apologize for ever doubting you , Mr. Ishiguro. In my defense , I hadn’t read “Never Let Me Go.”<br /><br />"
		} ,
	
		{
		  "title" : "Mastering Multi Tenant setup with rails - background jobs",
		  "category" : "technology",
		  "url" : "/technology/mastering-multi-tenant-setup-with-rails-background-jobs/",
		  "date" : "2024-05-05 00:00:00 +0530",
		  "content"	: "Welcome back to the Rails multi-tenant architecture series! If you’re just joining in , be sure to check out Part 1 , where you’ll find an introduction to multi-tenancy and a detailed walkthrough on setting up a multi-tenant Rails application.<br /><br />Part 1<br /><br />Quick Recap<br />In the previous blog post , the focus was on delving into the concept of multi-tenancy in software design , with a specific emphasis on managing separate databases for each tenant. After exploring three types of multi-tenant application architectures , a step-by-step guide was provided for setting up a multi-tenant Rails blog application. This included configuring databases for each tenant , implementing automatic connection switching in Rails 6/7 , and using Nginx to run multiple databases simultaneously on different ports.<br /><br />Introduction<br />In this blog post , the focus is on background job processing within a multi-tenant Rails environment. Specifically , it addresses the challenges of running background jobs across multiple databases and proposes solutions to ensure seamless execution of jobs.<br /><br />Sidekiq<br />First we will setup Sidekiq , A popular background job processing library for Ruby. Here’s a quick guide on how to set it up:<br /><br /><br />  Add sidekiq( use &#62;; 6 version) in Gemfile. Follow This Guide for setup.<br />  Create a sidekiq job rails generate sidekiq:job multi_db_testing<br /><br /><br /># app/sidekiq/multi_db_testing_job.rb<br />class MultiDbTestingJob &#60;; ApplicationJob<br /><br />  def perform<br />    p &ldquo;Number of articles is #{Article.count}&ldquo;<br />  end<br />end<br /><br />Running up application along with sidekiq<br />To start both the Rails server and Sidekiq , follow these steps:<br /><br /><br />  Install foreman gem to start both rails server and sidekiq.<br />  In Gemfile add foreman gem &amp;amp; run bundle install.<br />  Create a Procfile to define the processes:<br /><br /><br /># procfile<br />web: bin/rails server --binding=0.0.0.0 --port=3000 --environment=development<br />sidekiq: bundle exec sidekiq<br /><br />Triggering Background jobs<br /><br />  Create a route and controller action to trigger the Sidekiq job:<br /><br /><br /># config/routes.rb<br />    resources :articles do<br />      collection do<br />        get :run_background_job<br />      end<br />    end<br /><br />    # app/controllers/articles_controller.rb  def run_background_job<br />      MultiDbTestingJob.perform_later<br /><br />      redirect_to root_path<br />    end<br /><br />    # app/views/articles/index.html.erb<br />    &#60;;%= link_to &ldquo;Run sidekiq job&ldquo; , run_background_job_articles_path %&#62;;<br /><br /><br />  Start the server using foreman start<br />  Navigate to http://localhost:3000 , and trigger the job.<br />  You’ll notice that the job is executed , but it retrieves data only from the default database. why? Continue reading to find out the reason.<br /><br /><br />Problem?<br />When a Sidekiq server initializes , it establishes a connection pool to manage database queries. During job execution , it retrieves a connection from this pool. If a specific database is not specified for the job , it defaults to the primary database (default - db 1).<br /><br />Addressing the Database Connection Issue<br />To ensure that background jobs access the correct database , we need to pass the database name as a parameter to each job and modify the job accordingly:<br /><br /># /app/controllers/articles_controller.rb<br />def run_background_job<br />  MultiDbTestingJob.perform_later(shard_name)<br /><br />  redirect_to root_path<br />end<br /><br /># /app/sidekiq/multi_db_testing.rb<br />class MultiDbTestingJob &#60;; ApplicationJob<br />  def perform(shard)<br />    ActiveRecord::Base.connected_to(shard: shard) do<br />      p &ldquo;Number of articles in DB is #{Article.count}&ldquo;<br />    end<br />  end<br />end<br /><br />Now , you’ll get the desired result for both databases.<br /><br />However , this approach has its drawbacks:<br /><br />  For each background job , we need to pass an additional parameter.<br />  We need to write additional code to connect to the correct database for each background job.<br /><br /><br />To address these issues , we can create a Sidekiq adapter that will decide which database to connect to based on the database that initiated the background job. But before creating the adapter , we need a global attribute to remember which database we are connected to. To achieve this , Rails CurrentAttributes and Sidekiq Middleware will be utilized.<br /><br />Current Attributes<br />From the definition of Current Attributes , Abstract super class that provides a thread-isolated attributes singleton , which resets automatically before and after each request. This allows you to keep all the per-request attributes easily available to the whole system.<br /><br /># app/models/current.rb<br />class Current &#60;; ActiveSupport::CurrentAttributes<br />  attribute :tenant<br />end<br /><br /># app/controllers/application_controller.rb<br />before_action :setup_tenant<br /><br />def setup_tenant<br />  tenants = Rails.application.config_for(:settings)[:tenants]<br />  current_tenant = tenants.keys.find { |key| tenants[key][:hosts].include?(request.env[&amp;#39;HTTP_HOST&amp;#39;]) } || :app1_shard<br />  Current.tenant = current_tenant.to_sym<br />end<br /><br />Note - Sidekiq also introduced the cattr feature , this will help in persisting the value of current attributes when sidekiq job runs.<br />Read More<br /><br />Sidekiq Middleware<br />It is a set of customizable modules that intercept and augment the behavior of Sidekiq job processing in Ruby on Rails applications. Sidekiq Middleware<br /><br /><br />  Create file config/initializers/sidekiq.rb and paste following code.<br /><br /><br /># config/initializers/sidekiq.rb<br />    require &amp;#39;sidekiq&amp;#39;<br />    require &amp;#39;sidekiq/web&amp;#39;<br />    require &amp;#39;sidekiq/middleware/current_attributes&amp;#39;<br />    require_relative &amp;#39;../../app/middleware/sidekiq_adapter&amp;#39;<br /><br />    Sidekiq::CurrentAttributes.persist(&amp;#39;Current&amp;#39;)<br /><br />    Sidekiq.configure_server do |config|<br />      config.server_middleware do |chain|<br />        chain.add Middleware::SidekiqAdapter<br />      end<br />    end<br /><br /><br />  Create file app/middleware/sidekiq_adapter.rb and paste following code.<br /><br /><br />module Middleware<br />      class SidekiqAdapter<br />        include Sidekiq::ServerMiddleware<br /><br />        def call(job_instance , job_payload , queue)<br />          shard = current_shard(job_payload)<br />          ApplicationRecord.connected_to(shard: shard , role: :writing) do<br />            yield<br />          end<br />        rescue StandardError =&#62;; e<br />          p &ldquo;Error occured #{e}&ldquo;<br />        end<br /><br />        def current_shard(job_payload)<br />          job_payload.try(:[] , &amp;#39;cattr&amp;#39;).try(:[] , &amp;#39;tenant&amp;#39;)&amp;amp;.to_sym<br />        end<br />      end<br />    end<br /><br /><br />  With the middleware in place , we can simplify our Sidekiq job and remove the shard logic from it. The middleware will handle connecting to the correct shard.<br /><br /><br /># multi_db_testing_job.rb<br />  class MultiDbTestingJob &#60;; ApplicationJob<br />    def perform<br />      p &ldquo;Number of articles in DB is #{Article.count}&ldquo;<br />    end<br />  end<br /><br /><br />  # /app/controllers/articles_controller.rb<br />  def run_background_job<br />    MultiDbTestingJob.perform_later(shard_name)<br /><br />    redirect_to root_path<br />  end<br /><br /><br />  Run the project again and subsqeuently run the sidekiq job to test it out.<br /><br /><br /><br />  <br /><br /><br /><br />  You will notice that with the middleware in place , when executing a background job , it connects to the correct database.<br />    Code - Github Link<br />  <br /><br /><br />Summary<br />In this blog post , we solved database issue with background job processing in a multi-tenant Rails application. We introduced a custom Sidekiq middleware adapter , that fixes the issue of running background jobs across multiple databases. This approach provides a robust &amp;amp; scalable framework for managing background job execution in complex multi-tenant environments.<br />"
		} ,
	
		{
		  "title" : "Mastering Multi Tenant setup with rails part 1",
		  "category" : "technology",
		  "url" : "/technology/mastering-multi-tenant-setup-with-rails-part-1/",
		  "date" : "2023-12-17 00:00:00 +0530",
		  "content"	: "Multi-tenancy is a software design where a single instance of a software application serves multiple customers or tenants (individual users or organizations). In a multi-tenant architecture , each tenant’s data and configuration are logically isolated from one another , providing a sense of individuality and privacy while sharing the same underlying infrastructure , codebase , and application instance.<br /><br />Single Tenant application<br />In a single-tenant application , each hosted instance has its dedicated database. Upon addition of a new organization that requires segregated data , a new application is hosted with a different database.<br /><br /><br />  <br /><br /><br />Multi Tenant Application types<br /><br /><br />  Single Database shared rows<br />    <br />      Each table in database will contain an additional row known as tenant_id.<br />      Whenever data is stored and retrieved from table this coloumn will be used to get/store the data.<br />      <br />        Only the data that belongs to a specific customer/tenant will be fetched.<br /><br />        <br />  <br /><br />      <br />    <br />  <br />  Single Database shared schema<br />    <br />      For each tenant a different table will be maintained in same database.<br />      <br />        Data will be segregated table wise.<br /><br />        <br />  <br /><br />      <br />    <br />  <br />  Dedicated Database for Each Tenant<br />    <br />      <br />        For each tenant a new database schema will be maintained , it can be termed as shard.<br /><br />        <br />  <br /><br />      <br />    <br />  <br /><br /><br />In this blog post , we’ll take an in-depth look at the third approach , where we opt to manage separate databases for each tenant. To demonstrate this , we’ll walk through the process of creating a basic Rails blog application from the ground up.<br /><br />Goal<br /><br />  Setting up a multi-tenant application in development mode.<br />  dynamically switching databases according to the requesting host name.<br /><br /><br />What features rails 6 brings in<br />Rails 6 introduced the multiple database setup with following features -<br /><br />  Multiple writer databases and a replica for each.<br />  Automatic connection switching for the model you’re working with.<br />  Automatic swapping between the writer and replica depending on the HTTP verb and recent writes.<br />  Rails tasks for creating , dropping , migrating , and interacting with the multiple databases.<br /><br /><br />Setup<br /><br />Create new rails app<br /><br /><br />  rails new multi_db_blog<br />  update gemfile to use mysql2 instead of sqlite3<br /><br /><br />Setup databases<br /><br />  In database.yml file update the database with name.<br /><br /><br />development:<br />  app1:<br />    adapter: mysql2<br />    encoding: utf8<br />    reconnect: false<br />    database: app1_development<br />    pool: 5<br />    username:<br />    password:<br />    socket: /tmp/mysql.sock<br />    host: 127.0.0.1<br />  app2:<br />    adapter: mysql2<br />    encoding: utf8<br />    reconnect: false<br />    database: app2_development<br />    pool: 5<br />    username:<br />    password:<br />    socket: /tmp/mysql.sock<br />    host: 127.0.0.1<br /><br /><br />  bin/rake db:create create databases for both the tenants.<br />  You have the option to execute specific rake commands for each database. For instance , you can create the app1 database using the command: bin/rake db:create:app1<br /><br /><br />Generate Models and Controller<br /><br />  <br />    Model<br /><br />    bin/rails generate model Article title:string body:text<br />  <br />  <br />    Run migrations<br /><br />    bin/rake db:migrate<br />  <br />  <br />    Controller<br /><br />    bin/rails generate controller Articles index --skip-routes<br />  <br />  <br />    update routes.rb file.<br />  <br /><br /><br />root &ldquo;articles#index&ldquo;<br />resources :articles<br /><br />Complete the Articles Controller , Model and respective views by following This Guide<br /><br />Start App<br /><br />  Run bin/rails s to start the server.<br />  By default rails will connect to db1 now.<br />  This will act as a default database for the current application.<br /><br /><br />Running up both databases simaltaneously<br /><br />Install nginx &amp;amp; paste the following code in nginx.conf file.<br /><br />http {<br />  server {<br />   listen 3000;<br />   server_name localhost;<br /><br />   location / {<br />        proxy_pass http://127.0.0.1:3000; # Rails app running on port 3000<br />        proxy_set_header Host $host:$server_port;<br />        proxy_set_header X-Real-IP $remote_addr;<br />        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;<br />    }<br />  }<br /><br />  server {<br />    listen 4000;<br />    server_name localhost; # Change this to your actual domain if needed<br /><br />    location / {<br />        proxy_pass http://127.0.0.1:3000; # Rails app running on port 3000<br />        proxy_set_header Host $host:$server_port;<br />        proxy_set_header X-Real-IP $remote_addr;<br />        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;<br />    }<br />  }<br />}<br />events { }<br /><br />Above nginx configurations listens to port 3000 and 4000 and redirect to rails application running in port 3000.<br /><br />Additional Rails changes<br /><br />Since We are using Rails 7 we can use automatic shard swap feature provided by rails. if using rails 6.1 or 6 , a middleware can be introduced to automatic switch the tenants depending on request. Visit next section for the details.<br /><br />Mention list of tenants in a .yml file. You can maintain these records in a separate database as well , for now I will create a settings.yml file.<br /><br />development:<br />  tenants:<br />    app1:<br />      hosts:<br />        - localhost:3000<br />    app2:<br />      hosts:<br />        - localhost:4000<br /><br />update application.rb with following configurations.<br /><br />Rails.application.configure do<br />  config.active_record.shard_selector = { lock: true }<br /><br />  tenants = Rails.application.config_for(:settings)[:tenants]  # maintaining list of tenants with host<br />  config.active_record.shard_resolver = -&#62;;(request) {<br />    tenants.keys.find { |key| tenants[key][:hosts].include?(request.env[&amp;#39;HTTP_HOST&amp;#39;]) } || :app1<br />  }<br />end<br /><br />update application_record.rb<br /><br /># connects_to shards: {<br />   #  app1: { writing: :app1 } ,<br />   #  app2: { writing: :app2 }<br />   # }<br />   # OR<br /><br />  TENANTS = Rails.application.config_for(:settings)[:tenants]<br />  connects_to TENANTS.keys.map { |shard| [shard , { writing: shard }] }.to_h<br /><br />Creating Middleware for automatic shard switching(ignore if using rails 7 or above)<br /><br />  Create a middleware named middleware/tenant_selector.rb<br />  Add following code<br /><br /><br />module Middleware<br />    class TenantSelector<br />      def initialize(app , tenants)<br />        @app = app<br />        @tenants = tenants<br />      end<br /><br />      attr_reader :tenants<br /><br />      def call(env)<br />        request = ActionDispatch::Request.new(env)<br />        tenant = selected_tenant(request)<br /><br />        set_tenant(tenant) do<br />          @app.call(env)<br />        end<br />      end<br /><br />      private<br />      def selected_tenant(request)<br />        tenants.keys.find { |key| tenants[key][:hosts].include?(request.env[&amp;#39;HTTP_HOST&amp;#39;]) } || :app1<br />      end<br /><br />      def set_tenant(tenant , &amp;amp;block)<br />        ActiveRecord::Base.connected_to(shard: tenant.to_sym , role: :writing) do<br />          yield<br />        end<br />      end<br />    end<br />  end<br /><br /><br />  Update application.rb file with following changes.<br /><br /><br />tenants = Rails.application.config_for(:settings)[:tenants]<br />    config.app_middleware.use Middleware::TenantSelector , tenants<br /><br />Final Steps<br /><br />Follow these final steps to confirm your multi-tenant Rails application is up and running smoothly:<br /><br /><br />  Run bin/rails s<br />  Access localhost:3000 to connect to db1<br />  Access localhost:4000 to connect to db2<br />  If you wish to add more databases , simply update the database.yml and settings.yml files<br /><br /><br />What Next?<br /><br />In the upcoming series of blog posts , we will delve into the following topics:<br /><br />  Maintaining Background Jobs.<br />  Running Rake Tasks with Cron Jobs for Multiple Databases.<br />  ActiveStorage Data Management with Different Storage Types for Each Tenant.<br />  Caching.<br /><br /><br />Summary<br /><br />In this blog post we covered creating a multi tenant application from scratch and setting it up in development environment. We were able to automatically switch databases according to type of database.<br /><br />References<br /><br />  Github Code<br />  Rails Multi Db introduction<br /><br />"
		} ,
	
		{
		  "title" : "An in-depth look at Database Indexing",
		  "category" : "technology",
		  "url" : "/technology/an-in-depth-look-at-database-indexing/",
		  "date" : "2023-12-10 00:00:00 +0530",
		  "content"	: "In this article , we will explore Database Indexing. We will begin by installing the Docker &amp;amp; running a Postgres container on it. Subsequently , to execute queries and comprehend how the database uses various indexing strategies , we will insert millions of rows into a Postgres table.<br /><br />Following that , we will explore different tools to gaining insights into the SQL query planner and optimizers. After that , we will delve into understanding database indexing , examining how various types of indexing works with examples , and do a comparison between different types of database scan strategies.<br /><br />Finally , we will then demystify how database indexes operates for the WHERE clause with the AND and OR operators.<br /><br />Prerequisite<br /><br /><br /><br/><br /><br/><br/>Installing Docker &amp;amp; Running a Postgres Container:<br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Install Docker by following the instructions provided in the getting started guide on the official Docker website.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Verify that Docker is installed by running the command docker --version<br /><br/><br/><br/><br /><br/><br/><br /><br/><br /><br/><br /><br/><br/><br /><br/><br/><br/>Running a PostgresSQL Container:<br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Spin up the Docker container by using the official Postgres image.<br /><br />docker run -e POSTGRES_PASSWORD=secret --name pg postgres<br /><br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Start the Postgres command shell:<br /><br />docker exec -it pg psql -U postgres<br /><br /><br/><br/><br/><br /><br/><br/><br /><br/><br /><br/><br /><br/><br/><br /><br/><br/><br/>Inserting a Million Rows into a Postgres Table:<br /><br/><br/><br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Create a table named employees:<br /><br />create table employees(id serial primary key , employeeid integer , name TEXT);<br /><br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Insert into the employees table using the generate_series function:<br /><br />create or replace function gen_random_string(length integer)<br />RETURNS VARCHAR as<br />$$<br />DECLARE<br />result VARCHAR := &amp;#39;&amp;#39;;<br />BEGIN<br />FOR i IN 1..length LOOP<br /><br/>result := result || chr((floor(random() * 26) + 65)::integer);<br />END LOOP;<br />RETURN result;<br />END;<br />$$ language plpgsql;<br /><br />INSERT INTO EMPLOYEES(employeeid , name)<br />SELECT * , gen_random_string((random() * 10 + 5)::integer)<br />FROM generate_series(0 , 1000000);<br /><br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Confirm the result by executing the count query:<br /><br />select count(*) from employees;<br />count<br />--------<br /><br/>1000001<br />(1 row)<br /><br /><br/><br/><br/><br /><br/><br/><br /><br/><br/>This sequence of steps creates a table named employees and inserts one million rows into it , generating random values for the employeeid and name columns. The final count query verifies the successful insertion of the specified number of rows.<br /><br/><br /><br /><br /><br/><br /><br/><br/>The SQL Query Planner and Optimizer:<br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Explanation:<br /><br/><br/><br/><br/><br/>The explain command displays the execution plan generated by the PostgresSQL planner for the provided statement. This plan illustrates how the table(s) referenced in the statement will be scanned , whether through plain sequential scans , index scans , etc.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Examples:<br /><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/>Select All Query:<br /><br />postgres=# explain select * from employees;<br />QUERY PLAN<br />-------------------------------------------------------------<br />Seq Scan on employees (cost=0.00..16139.01 rows=1000001 width=19)<br /><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br/>Seq Scan: Directly goes to the heap and fetches everything , similar to a Full Table Scan in other databases. In Postgres , with multiple threads , it's called Parallel Seq Scan.<br /><br/><br/><br/><br/><br/><br/><br/><br /><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br/>Cost=0.00..16139.01: The first number represents work before fetching (e.g. , aggregating , ordering) , and the second number is the total estimated execution time.<br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br/>rows=1000001: An approximate number of rows to be fetched.<br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br/>width=19: The sum of bytes for all columns.<br /><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br />      <br/><br/><br/>Select All Query with Order By (Indexed Column):<br /><br />postgres=# create index employees_employeeid_idx ON employees(employeeid);<br />CREATE INDEX<br />postgres=# explain select * from employees order by employeeid;<br />QUERY PLAN<br />-----------------------------------------------------------------<br />Index Scan using employees_employeeid_idx on employees (cost=0.42..32122.44 rows=1000001 width=19)<br /><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br />         <br/><br/><br/><br/>cost=0.42: Postgres performs work , ordering by employeeid. An index on employeeid leads to an Index Scan.<br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/>Select All Query with Order By (Non-Indexed Column):<br /><br />postgres=# explain select * from employees order by name;<br />QUERY PLAN<br />--------------------------------------------------------------<br />Sort (cost=136306.96..138806.96 rows=1000001 width=19)<br /><br/>Sort Key: name<br /><br/>-&#62;; Seq Scan on employees (cost=0.00..16139.01 rows=1000001 width=19)<br /><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br/>Seq Scan &amp;amp; Sort:  Seq Scan on the table , followed by sorting. Sorting cost is critical.<br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/>Select Only ID:<br /><br />postgres=# explain select id from employees;<br />QUERY PLAN<br />---------------------------------------------------<br />Seq Scan on employees (cost=0.00..16139.01 rows=1000001 width=4)<br /><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br/>width=4: Fetching only id , resulting in a smaller width of 4 bytes (integer).<br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/>Select All Query for a Particular ID:<br /><br />postgres=# explain select * from employees where id = 10;<br />QUERY PLAN<br />-------------------------------------------------------------------<br />Index Scan using employees_pkey on employees (cost=0.42..8.44 rows=1 width=19)<br /><br/>Index Cond: (id = 10)<br /><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br/>rows=1: Fetching only 1 record using the primary key index.<br /><br/><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br/><br /><br/><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br /><br/><br /><br /><br />What is Database indexing?<br /><br />An index is a data structure that speeds up data retrieval without needing to scan every row present in the table. Index improves lookup performance but decreases write performance because every time a new row is created , indexes need to be updated.<br /><br />Indexes are typically stored on the disk. An index is typically a small table with two columns: a primary/candidate key and address. Keys are made from one or more columns.<br /><br />The data structure used for storing the index is B+ Trees. In the simplest form , an index is a stored table of key-value pairs that allows searches to be conducted in O(logn) time using binary search on sorted data.<br /><br />Types of Indexes:<br /><br /><br /><br/><br /><br/><br/>Clustered Index<br /><br/><br/><br /><br/><br/><br/>Index and data reside together and are ordered by the key. A Clustered Index is basically a tree-organized table. Instead of storing the records in an unsorted Heap table space , the clustered index is actually B+Tree index having the Leaf Nodes , which are ordered by the clusters key column value , store the actual table records , as illustrated by the following diagram.<br /><br/><br/><br /><br/><br/><br /><br/><br /><br/><br/><br /><br/><br/>Nonclustered Index<br /><br/><br/><br /><br/><br/><br/>A nonclustered index contains the key values and each key value entry has a pointer to the data row that contains the key value. Since the Clustered Index is usually built using the Primary Key column values , if you want to speed up queries that use some other column , then you'll have to add a Secondary Non-Clustered Index.<br /><br /><br/><br/><br/>The Secondary Index is going to store the Primary Key value in its Leaf Nodes , as illustrated by the following diagram<br /><br/><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br /><br /><br />How database indexes works under the hood?<br /><br />We have already created a database index on the employeeid column in our employees table using the CREATE INDEX statement. Behind the scenes , Postgres creates a new pseudo-table in the database with two columns: a value for employeeid and a pointer to the corresponding record in the employees table. This pseudo-table is organized and stored as a binary tree with ordered values for the employeeid column. Consequently , the query operates with O(logn) efficiency and typically executes in a second or less.<br /><br /><br />Let’s delve into two scenarios:<br /><br /><br /><br/><br /><br/><br/>SELECT * FROM employees WHERE employeeid = 4;<br /><br/><br/><br /><br/><br/>Here , with an index on the employeeid column , the query initiates an Index Scan. The process begins by accessing the Index table , retrieving the reference for the Page number , and obtaining the row number for the specific record on that page. Subsequently , it navigates to the corresponding page in the heap and fetches the entire row. This method , known as an Index Scan.<br /><br/><br /><br/><br /><br/><br/>SELECT employeeid FROM employees WHERE employeeid = 4;<br /><br/><br/>In this instance , there is no need to access the heap to retrieve the complete record. Since the required value for employeeid is already present in the index table , the operation is streamlined , and it directly performs an Index Only Scan. This approach allows the system to retrieve the specific employeeid directly from the index table without the additional step of fetching the complete row from the heap. This can lead to improved performance , particularly when the index includes all the columns needed for the query , minimizing the amount of data that needs to be processed.<br /><br/><br /><br /><br />What are different type of database scan strategies?<br /><br /><br /><br/><br /><br/><br/>Index Only Scan<br /><br />postgres=# EXPLAIN ANALYZE select id from employees where id = 100;<br /><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br />-----------------------------------------------------------------------------<br /> Index Only Scan using employees_pkey on employees  (cost=0.42..4.44 rows=1 width=4) (actual time=2.529..2.542 rows=1 loops=1)<br />   Index Cond: (id = 100)<br />   Heap Fetches: 0<br /> Planning Time: 0.510 ms<br /> Execution Time: 2.708 ms<br /><br /><br/><br/>If we examine the given query , we retrieve the ID using a filter on the ID column , which serves as the primary key and has an index on it.<br /><br /><br/><br/><br /><br /><br/><br/>Let's break down the query output:<br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Index Only Scan: In the case of an Index Only Scan , Postgres scans the index table , resulting in faster performance as the Index table is significantly smaller than the actual table. With Index Only Scan , results are directly fetched from the Index table when querying columns for which indexes have been created.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Heap Fetches: 0: This indicates that the queried ID value did not necessitate accessing the heap table to retrieve information. The information was obtained inline , and this is referred to as an Inline query.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Planning Time: 0.510 ms: This represents the time taken by Postgres to determine whether to use the index or perform a full table scan.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Execution Time: 2.708 ms: This is the time taken by Postgres to actually fetch the records from the table.<br /><br/><br/><br/><br /><br/><br/><br /><br/><br /><br/><br /><br/><br/>Index Scan<br /><br />postgres=# EXPLAIN ANALYZE select name from employees where id = 1000;<br /><br/><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br />----------------------------------------------------------------------------<br /> Index Scan using employees_pkey on employees  (cost=0.42..8.44 rows=1 width=11) (actual time=1.250..1.260 rows=1 loops=1)<br />   Index Cond: (id = 1000)<br /> Planning Time: 0.703 ms<br /> Execution Time: 1.655 ms<br /><br /><br /><br/><br/>If we examine the given query , we are retrieving the name using a filter on the ID column , which serves as the primary key and has an index on it.<br /><br /><br/><br/>In this case , the process begins with an index scan on the Index table to retrieve information about the Page number and row number on the Heap. Since the name is not available in the Index table , we must go to the heap to fetch the name. This type of scan is referred to as an Index Scan.<br /><br/><br/><br/><br /><br /><br />postgres=# EXPLAIN ANALYZE select name from employees where id &#60;; 1000;<br />                      QUERY PLAN<br />----------------------------------------------------------------------------------------------------------<br /> Index Scan using employees_pkey on employees  (cost=0.42..40.75 rows=1047 width=32) (actual time=0.062..1.139 rows=999 loops=1)<br />   Index Cond: (id &#60;; 1000)<br /> Planning Time: 4.948 ms<br /> Execution Time: 1.215 ms<br />(4 rows)<br /><br /><br /><br/><br/>In this case , we are filtering the record using the filter on the id with '&#60;;' operator and filtering out record which have id less than 1000. So the process begins with an Index scanning on the Index table then fetching the rows from the heap. Same as in case of fetching single id.<br /><br/><br/><br/><br /><br /><br />postgres=# EXPLAIN ANALYZE select name from employees where id &#62;; 1000;<br />                    QUERY PLAN<br />---------------------------------------------------------------------------<br /> Seq Scan on employees  (cost=0.00..18639.01 rows=998953 width=32) (actual time=0.104..168.884 rows=999001 loops=1)<br />   Filter: (id &#62;; 1000)<br />   Rows Removed by Filter: 1000<br /> Planning Time: 0.158 ms<br /> Execution Time: 198.259 ms<br />(5 rows)<br /><br /><br /><br/><br/>In this case , we are filtering the record using the filter on the id with '&#62;;' operator and filtering out records which have id greater than 1000. So , in this case , as Postgres knows it has to fetch 99% of the data anyway , it prefers to use the Seq Scan on the heap table. Rather than going to the Index table to filter out the records and then again going to the heap to filter those Index-scanned rows.<br /><br /><br/><br/><br/><br /><br/><br /><br/><br /><br/><br/>Parallel Seq Scan<br /><br /><br />postgres=# EXPLAIN ANALYZE select id from employees where name = &amp;#39;WABOY&amp;#39;;<br /><br/><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br />----------------------------------------------------------------------------<br /> Gather  (cost=1000.00..12347.44 rows=1 width=4) (actual time=3.970..120.383 rows=1 loops=1)<br />   Workers Planned: 2<br />   Workers Launched: 2<br />   -&#62;;  Parallel Seq Scan on employees  (cost=0.00..11347.34 rows=1 width=4) (actual time=64.894..102.448 rows=0 loops=3)<br /><br/><br/> Filter: ((name)::text = &amp;#39;WABOY&amp;#39;::text)<br /><br/><br/> Rows Removed by Filter: 333333<br /> Planning Time: 0.898 ms<br /> Execution Time: 120.850 ms<br />(8 rows)<br /><br /><br /><br/><br/>If we examine the given query , we are retrieving the id using a filter on the name column , which doesn't have an index on it.<br /><br /><br/><br/>As we don't have an index on the name column , that means we have to actually search for the name WABOY one by one and perform a sequential scan on the employees table. Postgres efficiently addresses this by executing multiple worker threads and conducting a parallel sequential scan.<br /><br/><br/><br /><br/><br /><br/><br /><br/><br/>Bitmap Scan<br /><br/><br/><br /><br/><br/>Let's create a Bitmap Index on the name column to get started.<br /><br /><br />postgres=# CREATE INDEX employees_name_idx ON employees(name);<br />CREATE INDEX<br /><br /><br/><br/>Let's explore how Bitmap Scan works in PostgreSQL.<br /><br/><br/><br /><br/><br/>Heap pages are stored on disk , and loading a page into memory can be expensive. When using an Index Scan , if the query yields a large number of rows , the query's performance may suffer because each row's retrieval involves loading a page into memory.<br /><br/><br/><br /><br/><br/>In contrast , with a Bitmap Scan , instead of loading rows into memory , we set a bit to 1 in an array of bits corresponding to heap page numbers. The operation then works on top of this bitmap.<br /><br/><br/><br /><br/><br/><br /><br /><br/><br/>Here's a simplified breakdown of above image:<br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>In a bitmap index scan , rows are not loaded into memory. PostgreSQL sets the bit to 1 for page number 1 when the name is 'CD' and 0 for other pages.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>When the name is 'BC' , page number 2 is set to 1 , and others are set to 0.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Subsequently , a new bitmap is created by performing an OR operation on both bitmaps.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Finally , PostgreSQL executes a Bitmap Heap Scan where it fully scans each heap page and rechecks the conditions.<br /><br/><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br/>This approach minimizes the need to load entire pages into memory for individual rows , improving the efficiency of the query. If the query results in a lot of rows located in only a limited number of heap pages then this strategy will be very efficient.<br /><br/><br/><br /><br/><br/><br /><br/><br/>Now let's filter out the id , name by the name<br /><br /><br />postgres=# EXPLAIN ANALYZE select id , name from employees where name = &amp;#39;WABOY&amp;#39;;<br />                  QUERY PLAN<br />--------------------------------------------------------------------------------<br /> Bitmap Heap Scan on employees  (cost=111.17..6277.29 rows=5000 width=36) (actual time=0.348..0.369 rows=1 loops=1)<br />   Recheck Cond: (name = &amp;#39;WABOY&amp;#39;::text)<br />   Heap Blocks: exact=1<br />   -&#62;;  Bitmap Index Scan on employees_name_idx  (cost=0.00..109.92 rows=5000 width=0) (actual time=0.274..0.274 rows=1 loops=1)<br />         Index Cond: (name = &amp;#39;WABOY&amp;#39;::text)<br /> Planning Time: 0.905 ms<br /> Execution Time: 0.734 ms<br /><br /><br/><br/>Upon analyzing the provided query , we extract the id and name by applying a filter on the name column , which has an index.<br /><br/><br/><br /><br/><br/>Let's clarify the process:<br /><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/><br/>Bitmap Index Scan: This step involves scanning the index table for the name column since an index exists on it. It retrieves the page number and row number to obtain references to the corresponding records in the heap.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br/>Bitmap Heap Scan: Since we are filtering based on both id and name , this step is necessary to visit the heap and retrieve the values for both attributes for a specific record. The reference to the record is obtained from the preceding Bitmap Index Scan.<br /><br/><br/><br/><br /><br/><br/><br/><br /><br/><br /><br /><br />Combining Database Indexes<br /><br /><br /><br/><br /><br/><br/>Prerequisite: Let's create a table to learn how to combine indexes.<br /><br /><br />CREATE TABLE NUMBERS(id serial primary key , a integer , b integer , c integer);<br />INSERT INTO NUMBERS(a , b , c) select (random() * 100)::integer , (random() * 1000)::integer , (random() * 2000)::integer from generate_series(0 , 10000000);<br /><br /><br/><br /><br/><br /><br/><br/>Now let's create index on the columns A and B.<br /><br /><br />CREATE INDEX numbers_a_idx on numbers(a);<br />CREATE INDEX numbers_b_idx on numbers(b);<br /><br /><br/><br /><br /><br /><br /><br/><br /><br/>Select column c for a particular value of column a<br /><br />postgres=# EXPLAIN ANALYZE SELECT c FROM numbers WHERE a = 88;<br />                  QUERY PLAN<br /><br />---------------------------------------------------------------------------------<br /> Bitmap Heap Scan on numbers  (cost=1101.09..57496.05 rows=98665 width=4) (actual time=41.110..683.631 rows=99888 loops=1)<br />   Recheck Cond: (a = 88)<br />   Heap Blocks: exact=45619<br />   -&#62;;  Bitmap Index Scan on numbers_a_idx  (cost=0.00..1076.42 rows=98665 width=0) (actual time=29.403..29.403 rows=99888 loops=1)<br />         Index Cond: (a = 88)<br /> Planning Time: 1.569 ms<br /> Execution Time: 687.152 ms<br /><br /><br /><br/><br/>Here , we can analyze that since we have an index only on column a , a bitmap index scan is performed on column a. To retrieve column c , it jumps to the heap and performs a bitmap heap scan.<br /><br/><br /><br/><br /><br/><br /><br/><br/>Select column c but we are going to query on both a and b with AND operation<br /><br /><br />postgres=# EXPLAIN ANALYZE SELECT c FROM numbers WHERE a = 90 AND b = 500;<br />                  QUERY PLAN<br />-----------------------------------------------------------------------------<br /> Bitmap Heap Scan on numbers  (cost=1320.12..1746.88 rows=110 width=4) (actual time=32.300..38.262 rows=107 loops=1)<br />   Recheck Cond: ((b = 500) AND (a = 90))<br />   Heap Blocks: exact=107<br />   -&#62;;  BitmapAnd  (cost=1320.12..1320.12 rows=110 width=0) (actual time=32.079..32.081 rows=0 loops=1)<br />         -&#62;;  Bitmap Index Scan on numbers_b_idx  (cost=0.00..110.88 rows=9926 width=0) (actual time=4.494..4.494 rows=9974 loops=1<br />)<br />               Index Cond: (b = 500)<br />         -&#62;;  Bitmap Index Scan on numbers_a_idx  (cost=0.00..1208.93 rows=111000 width=0) (actual time=26.799..26.800 rows=99868 l<br />oops=1)<br />               Index Cond: (a = 90)<br /> Planning Time: 3.362 ms<br /> Execution Time: 38.604 ms<br /><br /><br /><br/><br/>Here , we can analyze the following:<br /><br/><br/><br /><br/><br/><br/>PostgreSQL executed a bitmap index scan on column 'A'.<br /><br/><br/><br/>Concurrently , a bitmap index scan was performed on column 'B'.<br /><br/><br/><br/>Subsequently , PostgreSQL executed a bitmap AND operation to combine the results of the scans on 'A' and 'B'.<br /><br/><br/><br/>After obtaining the references for the rows to be retrieved , PostgreSQL proceeds to perform a bitmap heap scan.<br /><br/><br/><br /><br/><br /><br/><br /><br/><br /><br/><br/>Select column c but we are going to query on both a and b with OR operation.<br /><br /><br />postgres=# EXPLAIN ANALYZE SELECT c FROM numbers WHERE A = 50 OR B = 500;<br />                QUERY PLAN<br /><br />-------------------------------------------------------------------------------<br /> Bitmap Heap Scan on numbers  (cost=1164.23..57490.68 rows=101835 width=4) (actual time=37.957..600.439 rows=109466 loops=1)<br />   Recheck Cond: ((a = 50) OR (b = 500))<br />   Heap Blocks: exact=46998<br />   -&#62;;  BitmapOr  (cost=1164.23..1164.23 rows=101926 width=0) (actual time=25.625..25.626 rows=0 loops=1)<br />         -&#62;;  Bitmap Index Scan on numbers_a_idx  (cost=0.00..1002.43 rows=92000 width=0) (actual time=24.309..24.309 rows=99602 lo<br />ops=1)<br />               Index Cond: (a = 50)<br />         -&#62;;  Bitmap Index Scan on numbers_b_idx  (cost=0.00..110.88 rows=9926 width=0) (actual time=1.313..1.314 rows=9974 loops=1<br />)<br />               Index Cond: (b = 500)<br /> Planning Time: 1.135 ms<br /> Execution Time: 604.165 ms<br /><br /><br /><br/><br/>Here , we can analyze the following:<br /><br/><br/><br /><br/><br/><br/>PostgreSQL executed a bitmap index scan on column a.<br /><br/><br/><br/>Concurrently , a bitmap index scan was performed on column b.<br /><br/><br/><br/>Subsequently , PostgreSQL executed a BitmapOr operation to combine the results of the scans on columns a and b.<br /><br/><br/><br/>After obtaining the references for the rows to be retrieved , PostgreSQL proceeds to perform a bitmap heap scan.<br /><br/><br/><br /><br/><br /><br /><br /><br /><br/>Composite Index<br /><br/><br /><br/><br/>First , we need to drop the indexes on both columns a and b , and then create a composite index on columns a and b.<br /><br /><br />postgres=# CREATE INDEX numbers_a_b_idx on numbers(a , b);<br />CREATE INDEX<br /><br /><br/><br /><br/><br /><br/><br/><br /><br/><br/><br/>Select column c for a particular value of column a<br /><br /><br />postgres=# EXPLAIN ANALYZE SELECT c FROM numbers WHERE a = 70;<br /><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br /><br />-----------------------------------------------------------------------<br />Bitmap Heap Scan on numbers  (cost=1189.93..56830.03 rows=106000 width=4) (actual time=38.779..610.173 rows=99789 loops=1)<br /><br/>Recheck Cond: (a = 70)<br /><br/>Heap Blocks: exact=45549<br /><br/>-&#62;;  Bitmap Index Scan on numbers_a_b_idx  (cost=0.00..1163.43 rows=106000 width=0) (actual time=27.796..27.797 rows=99789 loops<br />=1)<br /><br/><br/>Index Cond: (a = 70)<br />Planning Time: 5.188 ms<br />Execution Time: 613.305 ms<br />(7 rows)<br /><br /><br/><br/><br /><br/><br/><br/>Here , we can analyze the following:<br /><br/><br/><br/><br /><br/><br/><br/><br/>This time , PostgreSQL decided to use the composite index numbers_ab_idx on both columns a and b.<br /><br/><br/><br/><br/>Subsequently , it performs a Bitmap Heap Scan on the selected rows based on the composite index.<br /><br/><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br/><br/>Select column c for a particular value of column b<br /><br/><br/><br/><br /><br />postgres=# EXPLAIN ANALYZE SELECT c FROM numbers WHERE b = 900;<br /><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br />-----------------------------------------------------------------------<br />Gather  (cost=1000.00..108130.94 rows=9926 width=4) (actual time=24.402..395.326 rows=10027 loops=1)<br /><br/>Workers Planned: 2<br /><br/>Workers Launched: 2<br /><br/>-&#62;;  Parallel Seq Scan on numbers  (cost=0.00..106138.34 rows=4136 width=4) (actual time=9.913..317.809 rows=3342 loops=3)<br /><br/><br/>Filter: (b = 900)<br /><br/><br/>Rows Removed by Filter: 3329991<br />Planning Time: 0.574 ms<br />JIT:<br /><br/>Functions: 12<br /><br/>Options: Inlining false , Optimization false , Expressions true , Deforming true<br /><br/>Timing: Generation 4.899 ms , Inlining 0.000 ms , Optimization 3.039 ms , Emission 25.030 ms , Total 32.968 ms<br />Execution Time: 398.820 ms<br />(12 rows)<br /><br /><br /><br/><br/><br/>Here , we can analyze the following:<br /><br/><br/><br/><br /><br/><br/><br/><br/><br /><br/><br/><br/><br/>This time , Postgres did not use the index numbers_a_b_idx. Even though we have a composite index on both columns a and b. Why? Because we cannot use this composite index when scanning a filter. The filter condition is on column a , and the composite index can be used for conditions involving both columns a and b or just column a. However , it cannot be used for conditions involving only column b. Therefore , if we have a composite index on columns a and b , querying on column b alone will not utilize the index.<br /><br/><br/><br/><br/><br /><br/><br/><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br/><br/>Select column c but we are going to query on both A and B with AND operation<br /><br /><br />postgres=# EXPLAIN ANALYZE SELECT C FROM numbers WHERE A = 60 AND B = 600;<br /><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br />-------------------------------------------------------------------------<br />Bitmap Heap Scan on numbers  (cost=5.44..386.39 rows=98 width=4) (actual time=0.732..6.281 rows=102 loops=1)<br /><br/>Recheck Cond: ((a = 60) AND (b = 600))<br /><br/>Heap Blocks: exact=101<br /><br/>-&#62;;  Bitmap Index Scan on numbers_a_b_idx  (cost=0.00..5.42 rows=98 width=0) (actual time=0.513..0.513 rows=102 loops=1)<br /><br/><br/>Index Cond: ((a = 60) AND (b = 600))<br />Planning Time: 0.756 ms<br />Execution Time: 6.659 ms<br /><br /><br /><br/><br/><br/>Here , the situation remains the same as earlier when we had an index on both columns A and B.<br /><br/><br/><br /><br/><br/><br /><br/><br/><br /><br/><br/><br/>Select column c but we are going to query on both A and B with OR operation<br /><br />postgres=# EXPLAIN ANALYZE SELECT C FROM numbers WHERE A = 60  or B = 80;<br /><br/><br/><br/><br/><br/><br/><br/>QUERY PLAN<br />------------------------------------------------------------------------<br />Gather  (cost=1000.00..128404.51 rows=108495 width=4) (actual time=20.721..388.512 rows=109443 loops=1)<br /><br/>Workers Planned: 2<br /><br/>Workers Launched: 2<br /><br/>-&#62;;  Parallel Seq Scan on numbers  (cost=0.00..116555.01 rows=45206 width=4) (actual time=8.325..304.804 rows=36481 loops=3)<br /><br/><br/>Filter: ((a = 60) OR (b = 80))<br /><br/><br/>Rows Removed by Filter: 3296853<br />Planning Time: 1.009 ms<br />JIT:<br /><br/>Functions: 12<br /><br/>Options: Inlining false , Optimization false , Expressions true , Deforming true<br /><br/>Timing: Generation 5.795 ms , Inlining 0.000 ms , Optimization 2.561 ms , Emission 21.798 ms , Total 30.154 ms<br />Execution Time: 397.675 ms<br /><br /><br /><br/><br/><br/>Here , we can analyze the situation as follows:<br /><br/><br/><br/><br /><br/><br/><br/><br/><br /><br/><br/><br/><br/>As observed earlier , it's not feasible to use a composite index on column B individually. The option is either to use it on column A alone or on both columns A and B. Consequently , Postgres opts for a Parallel Sequential Scan in this scenario.<br /><br/><br/><br/><br/><br /><br/><br/><br/><br /><br/><br/><br /><br/><br /><br /><br />"
		} ,
	
		{
		  "title" : "How to setup AWS Cloudwatch alarm for your SES reputation metrics",
		  "category" : "technology",
		  "url" : "/technology/how-to-setup-aws-cloudwatch-alarm-for-your-ses-reputation-metrics/",
		  "date" : "2023-11-20 00:00:00 +0530",
		  "content"	: "Amazon Simple Email Service (SES) is an email platform that offers a straightforward and cost-effective way for you to send and receive emails using your own email addresses and domains.<br /><br />AWS SES has associated reputation metrics (Bounce &amp;amp; Complaint rate) , and if these metrics exceed the threshold limit , AWS may disable your email service , potentially causing a significant impact on your business.<br /><br />Why not create an alarm that monitors these reputation metrics and notifies you when they approach the threshold value? This way , you can prevent email service downtime.<br /><br />Fortunately , AWS provides a few services that , when combined , can help you easily set up the SES reputation metrics alarm.<br /><br /><br /><br />Amazon Simple Email Service<br /><br />I recommend following deliver the mail with Amazon SES and Rails <br />  article to set up AWS SES for your Ruby on Rails application , as I’ll be using Ruby on Rails as my backend language.<br /><br />Amazon Simple Notification Service<br /><br />Amazon Simple Notification Service (SNS) is a web service that coordinates and manages message delivery from publishers to subscribers. You can learn more about it here.<br /><br />We will configure SNS to send notifications to both an email address and an API endpoint in your backend server.<br /><br /><br />  Steps to create SNS topic<br />    <br />      Go to  AWS SNS dashboard and click on the Create Topic button.<br />      Select Standard as the type of topic.<br />      Type a name for the topic. For example , ses-reputation-notifier.<br />      Click on Create topic button.<br />    <br />  <br />  Steps to create subscription for SNS topic<br />    <br />      Go to  AWS SNS dashboard and click on the Create Subscription button.<br />      Select the SNS topic you created from Topic ARN<br />      Choose the protocol from the list of protocols.<br />        <br />          Email<br />            <br />              Select the Email protocol.<br />              Enter your email in the endpoint.<br />              You’ll receive a subscription URL on your email. Visit this URL to subscribe to the SNS topic.<br />            <br />          <br />          HTTP/HTTPS<br />            <br />              We will configure the HTTP/HTTPS protocol after creating the public API endpoint in later part of the blog.<br />            <br />          <br />        <br />      <br />    <br />  <br /><br /><br />Amazon Cloudwatch<br /><br />Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time. You can use CloudWatch to collect and track metrics , which are variables you can measure for your resources and applications. You can learn more about it here.<br /><br />For our SES reputation monitoring alarm , we’ll require four alarms. Two will be for monitoring bounce rate , and two will be for monitoring complaint rate. We create two alarms for each reputation metric because the first alarm triggers when the reputation metric exceeds the threshold , and the second alarm triggers when the reputation metric returns to normal.<br /><br />Steps to create alarm<br /><br />  <br />    1. Bounce rate (OK -&#62;; ALARM)<br /><br />    This alarm will activate when the Bounce rate surpasses the set limit. This transition will change the alarm state from OK to ALARM.<br />    <br />      Go to AWS Cloudwatch and click on Create alarm.<br />      Click on Select metrics and select SES &#62;; Account Metrics &#62;; Reputation.BounceRate and click on Select metrics<br />      Select 1 hour from period dropdown.<br />      Fill Define the threshold value with 0.05 (suggested by AWS).<br />      Click on Additional configuration and from dropdown select Treat Missing data as Good.<br />      From Alarm state trigger select In alarm.<br />      From Send a notification to… select the SNS topic you created.<br />      Type a name for the alarm. For example , bounce-rate-threshold-exceeded and click on Create alarm.<br />    <br />  <br />  <br />    2. Bounce rate (ALARM -&#62;; OK)<br /><br />    This alarm will activate when the Bounce rate returns within the specified limit. This transition will change the alarm state from ALARM to OK.<br />    <br />      The procedure will be same for as for Alarm #1.<br />      Just OK will be selected for Alarm state trigger.<br />      Type a name for the alarm. for example , bounce-rate-threshold-inlimit<br />    <br />  <br />  <br />    3. Complaint rate (OK -&#62;; ALARM)<br /><br />    This alarm will activate when the Complaint rate surpasses the set limit. This transition will change the alarm state from OK to ALARM.<br />    <br />      Go to AWS Cloudwatch and click on Create alarm.<br />      Click on Select metrics and select SES &#62;; Account Metrics &#62;; Reputation.ComplaintRate and click on Select metrics<br />      Select 1 hour from period dropdown.<br />      Fill Define the threshold value with 0.001 (suggested by AWS).<br />      Click on Additional configuration and from dropdown select Treat Missing data as Good.<br />      From Alarm state trigger select In alarm.<br />      From Send a notification to… select the SNS topic you created.<br />      Type a name for the alarm. For example , complaint-rate-threshold-exceeded and click on Create alarm.<br />    <br />  <br />  <br />    4. Complaint rate (ALARM -&#62;; OK)<br /><br />    This alarm will activate when the Complaint rate returns within the specified limit. This transition will change the alarm state from ALARM to OK.<br />    <br />      The procedure will be same for as for Alarm #3.<br />      Just OK will be selected for Alarm state trigger.<br />      Type a name for the alarm. for example , complaint-rate-threshold-inlimit<br />    <br />  <br /><br /><br />API endpoint to receive POST request<br /><br />You’ll need an API endpoint to receive POST requests from AWS SNS.<br /><br />Create a file in app &#62;; controllers &#62;; sns_notification_controller.rb<br /><br />class SnsNotificationController &#60;; ApplicationController<br />      skip_before_action :verify_authenticity_token<br />      before_action :authenticate_request<br /><br />      def ses_reputation_notifier<br />        case message_body[&amp;#39;Type&amp;#39;]<br />        when &amp;#39;SubscriptionConfirmation&amp;#39;<br />          Rails.logger.error(message_body[&amp;#39;SubscribeURL&amp;#39;])<br />        when &amp;#39;Notification&amp;#39;<br />          message = JSON.parse(message_body[&amp;#39;Message&amp;#39;])<br /><br />          alarm_active = message[&amp;#39;NewStateValue&amp;#39;] == &amp;#39;ALARM&amp;#39;<br />          // Your logic based on alarm status<br />        end<br /><br />        head :ok<br />      end<br /><br />      private<br /><br />        def authenticate_request<br />          head :unauthorized if raw_post.blank? || !message_verifier.authentic?(raw_post)<br />        end<br /><br />        def raw_post<br />          @raw_post ||= request.raw_post<br />        end<br /><br />        def message_body<br />          @message_body ||= JSON.parse(raw_post)<br />        end<br /><br />        def message_verifier<br />          @message_verifier ||= Aws::SNS::MessageVerifier.new<br />        end<br />    end<br /><br />The code above uses the official AWS SNS SDK.<br /><br />Add a route for the ses_reputation_notifier action in the sns_notification within the config/routes.rb file.<br /><br />post &amp;#39;/ses_reputation_notifier&amp;#39; , to: &amp;#39;sns_notification#ses_reputation_notifier&amp;#39;<br /><br />The API endpoint needs to be a public endpoint so that SNS can send notifications without requiring any token. Since it’s a public endpoint , we need to verify the authenticity of the request to ensure it comes from SNS.<br /><br />There are two types of notifications sent by SNS:<br /><br /><br />  Subscription Confirmation<br />  Notifications<br /><br /><br />Before being able to receive notifications , we must confirm the subscription by visiting the subscribeUrl sent in the request body. That’s why we log the subscribeURL. Once you visit that URL , you’ll be subscribed to the SNS topic. After subscription , you’ll start receiving notifications.<br /><br />Steps to create HTTP/HTTPS subscription for SNS topic<br /><br />  Select HTTP or HTTPS protocol.<br />  Enter the public API endpoint URL in the endpoint field. For example: https://your-domain/ses_reputation_notifier<br />  A POST request will be sent to the API endpoint , and the subscription URL will be logged (as specified in the code above). Visit this URL to confirm the subscription.<br /><br /><br />Now , your SNS topic is configured to publish messages to the specified email and API endpoint.<br /><br />Conclusion<br /><br />Now that we’ve set up a CloudWatch alarm to monitor SES reputation metrics , it will notify both via email and API endpoint using SNS. With the notifications received by the server , you can ensure that any potential issues won’t significantly impact the current flow.<br /><br />References<br /><br /><br />  AWS SES - Here<br />  AWS SNS - Here<br />  AWS Cloudwatch - Here<br />  AWS SNS SDK (Ruby) - Here<br />  Reputation monitoring alarms using CloudWatch - Here<br /><br /><br />"
		} ,
	
		{
		  "title" : "Puma: From Daemonization to Process Control with Systemctl and Monit",
		  "category" : "technology",
		  "url" : "/technology/puma-from-daemonization-to-process-control-with-systemctl-and-monit/",
		  "date" : "2023-10-21 00:00:00 +0530",
		  "content"	: "Puma is a popular Ruby web server that is known for its speed and scalability. It has undergone significant changes in recent versions(starting 5.0.0). One of the most notable alterations is the removal of the daemonization feature. But what does it mean?<br /><br />Daemonization , in the context of web servers , is a process that allows a program to run in the background as a system service. In older versions , Puma made it simple for users to daemonize their processes with a straightforward configuration snippet:<br /><br />#config/puma.rb<br />daemonize<br /><br />However , in recent versions , attempting to use the daemonize code will result in an error , as this functionality has been removed from the  codebase.<br /><br />Why daemonization should not be part of gem?<br />Incorporating daemonization directly within a gem can lead to undesirable consequences: as explained by Mike Perham in a Blog Post. Here are some key points that should be considered -<br /><br />  Complexity: Adding daemonization features to a gem can make its code more complex and challenging.<br />  Maintenance: The responsibility of maintaining daemonization , automatic restart , and similar core features becomes an additional burden.<br />  Efficiency: System processes are better equipped to manage tasks like daemonization. Delegating this function to the system ensures more efficient and reliable execution , rather than embedding it within the gem.<br /><br /><br />As a result of these considerations , Puma decided to remove the daemonization feature from the gem.<br /><br />This decision led us to make some changes in our setup to ensure the smooth running of our applications.<br /><br />Using Systemd<br /><br />We had previously implemented daemonization for Sidekiq , which was a process similar to Puma’s needs. Although there were some minor adjustments required for Puma. Here are steps to achieve daemnization through systemctl:<br /><br /><br />  Remove daemonization from config/puma.rb file<br />  <br />    Create a file in /lib/systemd/system/puma.service. Below is sample systemd service configuration example , modify it according to your needs.<br /><br />    <br />[Unit]<br />      Description=Puma HTTP Server<br />      After=network.target<br /><br />      [Service]<br />      Type=notify<br />      User=username<br /><br />      WorkingDirectory=/dir/path<br />      ExecStart=/bin/pumactl start -F /path/puma_config --environment env<br />      ExecStartPost=/bin/sh -c &amp;#39;/bin/echo $MAINPID &#62;; /usr/myapp/shared/pids/puma.pid&amp;#39;<br />      ExecStop=/bin/kill -TSTP $MAINPID<br /><br />      RestartSec=10<br />      Restart=on-failure<br /><br />      [Install]<br />      WantedBy=multi-user.target<br /><br />  <br />  <br />    Two prominent Puma restart strategies are Phased and Hot restarts. Phased restarts are slower but ensure that all workers finish their existing requests before restarting the server , while Hot restarts are faster but come with increased latency during the restart. <br />    To initiate Puma with a phased restart , you can pass the phased-restart option. This choice offers flexibility to adapt Puma's behavior according to specific needs. More about puma restarts Here.<br />  <br />  <br />    Monit configurations<br /><br />    Monit is a utility for managing and monitoring processes , programs , files , directories and filesystems on a Unix system Monit Docs. <br /><br />    Updated monitrc file<br />    <br />check process puma with pidfile &ldquo;/usr/myapp/shared/pids/puma.pid&ldquo;<br />      start program = &ldquo;/bin/bash -l -c &amp;#39;sudo systemctl start puma&amp;#39;&ldquo; with timeout 20 seconds<br />      stop program = &ldquo;/bin/bash -l -c &amp;#39;sudo systemctl stop puma&amp;#39;&ldquo; with timeout 20 seconds<br />      if totalmem is greater than 800 MB for 3 cycles then restart<br />      if cpu is greater than 65% for 2 cycles then exec &ldquo;/etc/monit/slack_notifier.sh&ldquo; else if succeeded then exec &ldquo;/etc/monit/slack_notifier.sh&ldquo;<br /><br />  <br />  <br />    To check if puma is running correctly follow the commands.<br /><br />    <br />ps aux | grep puma<br />    sudo monit summary<br /><br />  <br /><br />Exploring Other Alternatives<br /><br />As alternative to this we considered using puma-daemon gem , which essentially replicated the removed code and maintained it in a separate gem. However , after careful consideration , we chose not to adopt this alternative for the following reasons:<br /><br /><br />  Violation of system standards.<br />  Additional gem and maintainence burden.<br /><br /><br />Summary<br />While the removal of daemonization from Puma may require some adjustments , it aligns with the best practices of modern web server management Managing processes at the system level , using tools like systemd and Monit , is considered a more efficient and maintainable approach. Daemonizing processes within application code is discouraged , as it’s a task that falls under the system level. Ultimately , the shift towards system-level process management ensures the stability and efficiency of web applications.<br />"
		} ,
	
		{
		  "title" : "Demystifying Rails 7 System Tests: Configuring CI Pipeline",
		  "category" : "technology",
		  "url" : "/technology/demystifying-rails-7-system-tests-configuring-ci-pipeline/",
		  "date" : "2023-08-28 00:00:00 +0530",
		  "content"	: "In Rails 5.1 and later versions , system tests were introduced as a new type of test to simulate a user interacting with a web application. These tests use a headless browser , typically powered by Capybara and a WebDriver , to mimic a user’s actions like clicking buttons , filling forms , and navigating through the application.<br /><br />Why do we need System Tests?<br /><br /><br />  System tests let you test applications in the browser. Because system tests use a real browser experience , you can test all of your JavaScript easily from your test suite.<br />  Typically used for:<br />    <br />Acceptance testing: verify that the app has implemented a specific feature<br />Smoke testing: verify that the app is functional on a fundamental level and doesn't have code issues.<br />Characterization testing: is a type of software testing that involves examining and documenting the behavior of an existing system or application without making any modifications to its code<br /><br />  <br /><br /><br />How we can run System Test?<br /><br /><br />  System Test interacts with your app via an actual browser to run them.<br />  From a technical perspective , system tests aren’t necessarily required to interact with a real browser; they can be set up to utilize the rack test backend , which emulates HTTP requests and processes the HTML responses. While system tests based on rack_test run faster and more dependable than front-end tests involving an actual browser , they have notable limitations in mimicking a genuine user experience as they are incapable of executing JavaScript.<br /><br /><br /><br />The Anatomy of a System Test?<br /><br /><br /><br /><br /><br /><br />  Minitest<br />    <br />Minitest is a small and incredibly fast unit testing framework.<br />It provides the base classes for test cases.<br />  For Rails System Tests , Rails provides an ApplicationSystemTestCase base class which is in turn based on  ActionDispatch::SystemTestCase:<br /><br />  <br /><br /><br />require &ldquo;test_helper&ldquo;<br /><br />  class ApplicationSystemTestCase &#60;; ActionDispatch::SystemTestCase<br />    driven_by :selenium , using: :chrome , screen_size: [1400 , 1400]<br />  end<br /><br /><br />  In ActionDispatch::SystemTestCase we require the capybara/minitest library.<br />  It provides basics assertions like assert_equal , assert_nil , assert_same , assert_raises , assert_includes.<br />  A runner to run the tests and report on their success and failure.<br />  <br /><br /><br />  <br />    Capybara<br /><br />    <br /><br />  Capybara starts your app in a separate process before running the tests. This ensures that the tests are run against the correct version of your app.<br />  Capybara provides a high-level API that makes it easy to write tests in a natural way. For example , you can write a test that says &quot;click the button&quot; instead of having to write code to find the button and click it.<br />  Here is an example of a test written with Capybara's DSL (Domain Specific Language):<br /><br />  <br /><br /><br />visit(&amp;#39;/login&amp;#39;)<br />  fill_in(&amp;#39;email&amp;#39; , with: &amp;#39;user@example.com&amp;#39;)<br />  fill_in(&amp;#39;password&amp;#39; , with: &amp;#39;password&amp;#39;)<br />  click_button(&amp;#39;Login&amp;#39;)<br /><br /><br />  <br />    Selenium-Webdriver<br /><br />    <br />Capybara uses the Selenium Webdriver library to interact with real browsers. Selenium WebDriver is a cross-platform library that provides a way to control web browsers from code. Capybara uses Selenium WebDriver to translate its high-level DSL (Domain Specific Language) into low-level commands that the browser can understand.<br /><br />  <br /><br /><br />require &ldquo;selenium-webdriver&ldquo;<br /><br />  driver = Selenium::WebDriver.for :firefox<br />  driver.navigate.to &ldquo;http://google.com&ldquo;<br /><br />  element = driver.find_element(name: &amp;#39;q&amp;#39;)<br />  element.send_keys &ldquo;Hello WebDriver!&ldquo;<br />  element.submit<br /><br />  puts driver.title<br /><br />  Driver.quit<br /><br /><br />  You can see how it’s a bit lower-level than the Capybara example further up. The selenium-webdriver library translates these calls into WebDriver Protocol , which it speaks to a webdriver executable.<br />  <br /><br /><br />  <br />    Webdriver Protocol<br /><br />    <br /><br />The Selenium WebDriver library translates its calls into the WebDriver Protocol. The WebDriver Protocol is a HTTP-based wire protocol that is used to communicate between the Selenium WebDriver library and the web browser.<br />In order to start a chrome browser window and navigate to google.com. We need to startup geckodriver.<br />We send it a “new session” command with a HTTP post request<br /><br />  <br /><br /><br />curl -X POST &amp;#39;http://127.0.0.1:9515/session&amp;#39; -d &amp;#39;{&ldquo;capabilities&ldquo;:{&ldquo;firstMatch&ldquo;:[{&ldquo;browserName&ldquo;:&ldquo;firefox&ldquo;}]}}&amp;#39;<br /><br /><br /><br />  This return a session id along with data<br />  <br /><br />{ ... &ldquo;sessionId&ldquo;:&ldquo;f1776ba558e28309299dc5f62864e977&ldquo; ... }<br /><br /><br /><br />  Then we make another post request with a session id. And url in data parameters<br />  <br /><br />curl -X POST &amp;#39;http://127.0.0.1:9515/session/f1776ba558e28309299dc5f62864e977/url&amp;#39; -d &amp;#39;{&ldquo;url&ldquo;: &ldquo;https://google.com&ldquo;}&amp;#39;<br /><br /><br />  Webdriver<br />    <br /><br />Webdriver is a tool that speaks “Webdriver protocol” and controls the browser.<br />Every major browser there is an associated webdriver tool. Chrome has chromedriver. Firefox has a geckodriver. MS Edge has edgedriver. Safari has safaridriver.<br />WebDriver tools act as servers: when you execute them , they start a persistent process that listens for HTTP requests until it is terminated.<br /><br />  <br /><br /><br /><br /><br />  Webdrivers gem<br />    <br /><br />Before selenium-webdriver 4.11 , webdrivers gem automatically determines which WebDriver executable needs to be downloaded for your platform and selected browser , downloads it , and arranges for that executable to be used by selenium-webdriver.<br />From version 4.11 , they have incorporated the functionality in selenium-webdriver gem using selenium-manager.<br /><br /><br />  <br /><br /><br /><br /><br /><br />Running Rails 7 System Tests with Docker and Gitlab Runner on Arm64 and Amd64 linux machines<br /><br /><br />Step 1: Prepare the Rails 7 application for testing<br /><br /><br />  Run the command below to generate a very basic Ruby on Rails 7 app:<br /><br /><br />rails new minitest-rails-app<br /><br /><br />  Go ahead and open up the project in your favourite editor and proceed to the Gemfile , specifically to the test block:<br /><br /><br />group :test do<br />    # Use system testing [https://guides.rubyonrails.org/testing.html#system-testing]<br />    gem &ldquo;capybara&ldquo;<br />    gem &ldquo;selenium-webdriver&ldquo;<br />    gem &ldquo;webdrivers&ldquo;<br />  end<br /><br /><br />  Next , let’s do a quick scaffold generation to have something to work with:<br /><br /><br />rails generate scaffold Blog title:string body:text<br /><br /><br />  Usually , generating a scaffold will automatically generate the application_system_test_case.rb and everything you need for the system tests<br /><br /><br />application_system_test_case.rb (default) <br />  <br />  require &ldquo;test_helper&ldquo;<br />  <br />  class ApplicationSystemTestCase &#60;; ActionDispatch::SystemTestCase<br />    driven_by :selenium , using: :chrome , screen_size: [1400 , 1400]<br />  end<br /><br /><br />  Run the database commands<br /><br /><br />rails db:setup<br />  rails db:migrate<br /><br /><br />  Running a Basic System For the First Time<br /><br /><br />rails test:system<br /><br />Step 2: Exclude the gem webdrivers from the list of dependencies<br /><br />  Before selenium-webdriver 4.11 , webdrivers gem automatically download webdriver executable.<br />  From version 4.11 , they have incorporated the functionality in selenium-webdriver gem using selenium-manager.<br />  We can comment out the webdrivers line from Gemfile.<br />  After change , Gemfile looks like this<br /><br /><br />group :test do<br />  # Use system testing [https://guides.rubyonrails.org/testing.html#system-testing]<br />  gem &ldquo;capybara&ldquo;<br />  gem &ldquo;selenium-webdriver&ldquo; , &ldquo;~&#62;; 4.11&ldquo;<br />  #gem &ldquo;webdrivers&ldquo;<br />  end<br /><br />Step 3: Point the Selenium-webdriver to use the firefox browser<br /><br />  As chrome has not released binary compatible with linux/arm64 machine. So the test failed on the arm64 linux machine. I tried multiple approaches to make it work with headless_chrome , but didn’t work and commend the issue in details in this  issue tracker<br />  We need to change the browser to the firefox.<br /><br /><br />#application_system_test_case.rb (change driver to Firefox)<br /> <br />  require &ldquo;test_helper&ldquo;<br />  <br />  class ApplicationSystemTestCase &#60;; ActionDispatch::SystemTestCase<br />    driven_by :selenium , using: :firefox , screen_size: [1400 , 1400]<br />  end<br /><br />Step 4: Prepare the docker image<br /><br /><br />  Create Dockerfile<br /><br /><br />FROM ruby:3.1.2-slim-buster<br /><br />  RUN apt-get update<br />  RUN apt-get -y install gnupg curl wget xvfb unzip<br /><br />  ENV NODE_VERSION 19<br /><br />  RUN curl -fsSL https://deb.nodesource.com/setup_${NODE_VERSION}.x | bash -  &amp;amp;&amp;amp; \<br />  apt-get install --yes nodejs &amp;amp;&amp;amp; \<br />  apt-get install --yes libxss1 libappindicator1 libindicator7 python2<br /><br />  RUN apt-get update &amp;amp;&amp;amp; \<br />  apt-get install --yes software-properties-common build-essential libssl-dev sqlite3 libsqlite3-dev pkg-config ca-certificates firefox-esr<br /><br />  RUN apt-get install -y git-all<br />  RUN npm install yarn -g<br />  ADD . /data<br /><br /><br />  <br />    This Dockerfile sets up an image with Ruby 3.1.2 and Node.js 19 installed. It installs system dependencies like Git , Yarn , various libraries for sqlite and Firefox.<br />  <br />  <br />    Build Docker image<br />  <br /><br /><br />docker buildx build -t dockermanishelitmus/systemtest-rails-app:latest1.0 . --platform linux/amd64 ,linux/arm64 --push<br /><br /><br />  Command is building a Docker image using the buildx extension , targeting two different platforms (Intel/AMD 64-bit and ARM 64-bit) , tagging the image as latest1.0 , and pushing the resulting image to a container registry.<br /><br /><br />Step 5: Prepare the gitlab-runner<br /><br /><br />  In the project root directory create a file .gitlab-ci.yml with content<br /><br /><br />image: &ldquo;dockermanishelitmus/systemtest-rails-app:latest1.0&ldquo;<br />services:<br /> - redis:latest<br />variables:<br /> RAILS_ENV: &ldquo;test&ldquo;<br /><br />cache:<br /> paths:<br />   - vendor/ruby<br />   - node_modules/<br /><br />before_script:<br /> - gem install bundler  --no-document<br /> - bundle config set force_ruby_platform true<br /> - bundle install<br /> - bin/rake db:drop<br /> - bin/rake db:setup<br /> - bin/rake db:migrate<br /><br />stages:<br /> - tests<br /><br />SystemTests:<br /> stage: tests<br /> script:<br />   - yarn install<br />   - bin/rake assets:precompile<br />   - bin/rails test:system<br /> artifacts:<br />   when: on_failure<br />   name: &ldquo;$CI_JOB_NAME-$CI_COMMIT_REF_NAME&ldquo;<br />   paths:<br />     - coverage/<br />   expire_in: 1 day<br /><br /><br />  Finally run your test suite<br /><br /><br />gitlab-runner exec docker SystemTests<br /><br /><br />  Output<br /><br /><br />$ bin/rails test:system<br />  Running 4 tests in a single process (parallelization threshold is 50)<br />  Run options: --seed 13031<br /><br />  # Running:<br /><br />  Capybara starting Puma...<br />  * Version 5.6.7  , codename: Birdie&amp;#39;s Version<br />  * Min threads: 0 , max threads: 4<br />  * Listening on http://127.0.0.1:33385<br />  ....<br /><br />  Finished in 7.865541s , 0.5085 runs/s , 0.5085 assertions/s.<br />  4 runs , 4 assertions , 0 failures , 0 errors , 0 skips<br />  Saving cache for successful job<br />  Creating cache SystemTests/main...<br />  WARNING: vendor/ruby: no matching files. Ensure that the artifact path is relative to the working directory<br />  node_modules/: found 2 matching files and directories<br />  No URL provided , cache will not be uploaded to shared cache server. Cache will be stored only locally.<br />  Created cache<br />  Job succeeded<br /><br />Conclusion<br /><br />Now we have a setup that enables us to run system tests in both arm64 and amd64 linux machines with minimal customizations we may want to add. A few tips and tricks should help to get your first system tests up and running in CI pipeline.<br />"
		} ,
	
		{
		  "title" : "Building a Frontend Scoring Engine: Automating Frontend Evaluation",
		  "category" : "technology",
		  "url" : "/technology/building-a-frontend-scoring-engine-automating-frontend-evaluation/",
		  "date" : "2023-07-21 00:00:00 +0530",
		  "content"	: "The frontend scoring engine is a powerful tool designed to assess the frontend skills of candidates based on code quality , responsiveness , and functionality. It aims to streamline the evaluation process for frontend development by automating the assessment of code quality , best practices , and functionality.<br /><br />What you’ll learn from this blog<br /><br />In this blog , we will dive into the technical aspects of building a frontend scoring engine.<br /><br /><br />  The need for frontend scoring engine in today’s technology landscape.<br />  The technical requirements gathering and Research phase involved.<br />  Generation of Test script for Test automation using Puppeteer.<br />  Dockerizing the Application.<br />  Features and Process of building the application.<br /><br /><br />Need for the Frontend Scoring Engine<br /><br />In today’s technology-driven world , the demand for skilled frontend developers is at an all-time high. With the rapid evolution of web applications and user interfaces , companies are constantly seeking talented individuals who can create visually appealing , intuitive , and responsive frontend experiences. However , evaluating frontend development skills can be a complex and time-consuming task. This is where a frontend scoring engine comes into play Automating the Evaluation Process , Measurement of Code Quality and Ensuring Mobile Responsiveness. By allowing users to input HTML , CSS and JavaScript code , and generating scores based on predefined test cases , the scoring engine provides a comprehensive evaluation of candidates’ frontend skills.<br /><br />Research Work<br /><br />Before starting the implementation of the frontend scoring engine project , extensive research was conducted to understand the need for such a system , evaluate existing systems , explore testing tools , and plan the evaluation process. This research phase played a crucial role in shaping the project and ensuring its successful execution. Let’s take a brief look on highlight and the key areas of research conducted during the project’s inception.<br /><br /><br />  Evaluating Existing Systems :<br />To gain insights into the existing solutions available in the market , a comprehensive evaluation of similar systems was conducted. Various frontend scoring engines , online code editors were explored to understand their features , functionalities , strengths , and weaknesses. This evaluation provided valuable insights that influenced the design decisions and feature set of the new scoring engine. <br />Some similar existing systems:<br />    <br />      Codier.io<br />      Frontend Mentor<br />      CSS Battle<br />      Algoexpert.io Frontend<br />    <br />  <br />  Testing Tools and Technologies :<br />During our research , we explored various testing tools and technologies to find the perfect fit for executing test cases , assessing code quality , and evaluating frontend functionalities. The evaluation revolved around factors like capabilities , ease of use , and compatibility with our project requirements. Tools such as Selenium , Cypress , Jest , csslint , eslint were taken into consideration.<br />Read more about the tools:<br />    <br />      Selenium<br />      Cypress<br />      Jest<br />    <br />  <br />  Puppeteer :<br />Puppeteer was chosen over Selenium primarily due to its compatibility with Docker and its ability to control headless Chrome or Chromium instances. Docker provides an efficient and scalable environment for running tests , and Puppeteer seamlessly integrates with Docker containers. Additionally , Puppeteer offers a more modern and concise API , making it easier to write test scripts and perform browser automation tasks.<br />    <br />      Puppeteer vs Selenium<br />      Puppeteer Docs<br />    <br />  <br />  Docker Integration :<br />We explored the benefits of Docker , a widely-used containerization platform , and discovered how it could greatly enhance our project. Docker allows us to create lightweight , portable , and isolated containers , which provide a consistent and reproducible environment. Leveraging Docker , we encapsulated and ran our scoring engine , testing tools , and other dependencies , ensuring seamless integration and efficient execution. <br />We pulled various Docker images from Docker Hub , enabling us to set up the required tools effortlessly.<br />    <br />      csslint<br />      eslint<br />      jest<br />    <br />  <br />  Real-Time Code Editor :<br />To provide a user-friendly and real-time code editing experience , we started searching for frontend code editors and existing projects available on GitHub. Various code editor projects were evaluated , and their source code were studied to understand the implementation details. This research helped in selecting the most suitable code editor framework and implementing it within our frontend scoring engine. <br />    <br />      Codepen<br />      Fronteditor<br />      CodeG<br />    <br />  <br />  <br />    Problem Statement and Test Case Creation :<br />The goal was to design problem statements that accurately reflect real-world frontend development challenges and create test cases that thoroughly evaluate candidates’ code. Puppeteer test scripts were written to simulate user interactions , perform assertions , and capture screenshots for image comparison using the PixelMatch JavaScript library.<br />  <br />  Cloud Deployment and Infrastructure :<br />For our final Deployment and integration Amazon Web Services (AWS) was choosen. The research covered various AWS services , including EC2 instances for hosting the scoring engine , S3 for storage , and other relevant services for infrastructure setup. The deployment process , security considerations , and scaling options were thoroughly explored to ensure a robust and scalable deployment architecture.<br /><br /><br />Test Script Generation<br /><br />In the frontend scoring engine , we ensure evaluation of user-submitted HTML , CSS , and JavaScript code by subjecting it to comprehensive testing against predefined test cases. These tests are designed to assess the code quality , functionality , and adherence to best practices , providing a total assessment of candidates’ frontend development skills. By conducting these thorough evaluations , we can accurately determine the proficiency of developers in creating efficient and reliable frontend solutions. Throughout this section , you’ll get an overview of the various types of tests performed , explaining their significance in evaluating code quality and functionality.<br /><br /><br />  <br />    Heading/Element Testing<br />This test focuses on ensuring the presence and correctness of specific HTML elements within the user’s code. Test cases are designed to check if required headings , such as h1 , h2 , p or specific elements identified by ID or class , are present. The purpose of this test is to assess the structure and semantic correctness of the user’s HTML code.<br />  <br />  <br />    CSS Properties Testing<br />This test aims to verify the correct usage of CSS properties in the user’s code. It includes checking for the presence of essential CSS properties , such as margin , padding , font-size , or specific properties required for a particular problem statement. This test ensures that the user’s code adheres to the defined CSS requirements and best practices.<br />  <br />  <br />    Form Validation Testing<br />Form validation testing focuses on assessing the user’s code for proper form validation techniques. Test cases can include checking for required fields , validating email formats , enforcing password complexity , or implementing custom validation logic. This test ensures that the user’s code handles form validation correctly and provides appropriate error messages.<br />  <br />  <br />    Function Testing<br />This test evaluates the functionality and correctness of JavaScript functions implemented by the user. Test cases are designed to cover different scenarios and edge cases to ensure that the functions perform as expected. This test assesses the user’s ability to write functional and efficient JavaScript code.<br />  <br />  <br />    API Testing<br />API testing involves verifying the integration of API calls in the user’s code. Test cases may include checking if an API request is made , handling the API response correctly , and displaying the data from the API on the page. This test ensures that the user’s code effectively interacts with external APIs.<br />  <br />  <br />    Button Testing<br />Button testing focuses on evaluating the behavior and interactivity of buttons implemented by the user. Test cases may include checking if a button triggers a specific action , updates the UI , or performs a navigation action. This test ensures the proper functionality of user-defined buttons.<br />  <br />  <br />    Redirection Testing<br />This test aims to assess the behavior of navigation and redirection implemented by the user’s code. Test cases may include checking if clicking a link or a button redirects the user to the correct page or if the page refreshes as intended. This test ensures that the user’s code correctly handles navigation and redirection scenarios.<br />  <br /><br /><br />Dockerizing the Puppeteer with Chrome Browser Support<br /><br />Dockerfile:<br /><br /># Use the node:slim base image<br /><br />FROM node:slim<br /><br /># Set an environment variable to skip Puppeteer Chromium download during installation<br /><br />ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD true<br />RUN apt-get update &amp;amp;&amp;amp; apt-get install gnupg wget -y &amp;amp;&amp;amp; \<br /> wget --quiet --output-document=- https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor &#62;; /etc/apt/trusted.gpg.d/google-archive.gpg &amp;amp;&amp;amp; \<br /> sh -c &amp;#39;echo &ldquo;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&ldquo; &#62;;&#62;; /etc/apt/sources.list.d/google.list&amp;#39; &amp;amp;&amp;amp; \<br /> apt-get update &amp;amp;&amp;amp; \<br /> apt-get install google-chrome-stable -y --no-install-recommends &amp;amp;&amp;amp; \<br /> rm -rf /var/lib/apt/lists/\*<br /><br /># Set the working directory inside the container<br /><br />WORKDIR /usr/src/app<br /><br /># Copy the package.json file to the working directory<br /><br />COPY package.json ./<br /><br /># Install project dependencies using npm<br /><br />RUN npm install<br /><br /># Expose port 3000 to allow access to the app outside the container<br /><br />EXPOSE 3000<br /><br /># Run the app using the &ldquo;npm test&ldquo; command when the container starts<br /><br />CMD [&ldquo;npm&ldquo; , &ldquo;test&ldquo;]<br /><br />Build command:<br /><br />docker build -t bhushan21z/puppchrome .<br /><br />Publish it to Docker Hub:<br /><br />docker push bhushan21z/puppchrome:tagname<br /><br />Pull commnd:<br /><br />docker pull bhushan21z/puppchrome<br /><br />Run command:<br /><br />docker run -it --rm -v $(pwd)/files:usr/src/app/files puppeteerchrome<br /><br />Features and Architecture<br /><br /><br /><br /><br />Scoring Engine:<br /><br /><br />  Inputs: The scoring engine takes HTML , CSS , and JavaScript files created by users on the client side , as well as the test cases file generated on the backend.<br />  Code Quality Assessment: The engine assesses code quality using ESLint CSSlint and similar tools.<br />  Scoring: The engine generates a score based on code quality , along with the results of the test cases executed on the client-side code.<br />  Modular Architecture: The scoring engine is a separate entity , independent of the frontend and backend code.<br />  Technology Stack: Python Flask framework is used to implement the scoring engine.<br />  Working: Flask runs various Docker run commands to execute test script.<br /><br /><br />Backend:<br /><br /><br />  MySql Database: Schema Created with various tables such as users , questions , testcases and submissions.<br />  Node JS: Express framework is used to implement Rest APIs.<br />  User auth: Contains user register and login APIs.<br />  Questions: Questions create/get APIs.<br />  Test Cases: Testcases create/get APIs and joining it with Questions table with question id as foreign key.<br />  Scoring Engine: POST request to get user data and sending it to scoring engine and returning scoring engine response to frontend.<br />  Submissions: User Submissions create/get APIs and joing it with users table and questions table.<br /><br /><br />Frontend (Admin Side):<br /><br /><br />  Problem Creation: Admins can create problem statements , describing the problem to be solved.<br />  Problem Settings: Problems can include various settings such as score weightage , best practices to check , and mobile responsiveness evaluation.<br />  Test Cases: Admins can add multiple test cases related to each problem statement.<br />  Test Case Visibility: Some test case outputs will be visible to users , while others will be hidden , showing only whether the score passed or failed.<br />  User-Friendly Test Case Creation: Adding test cases are straightforward , even for users with limited programming knowledge.<br /><br /><br />Frontend (Client Side):<br /><br /><br />  Problem List: Users can view a list of problems on their screen.<br />  Code Editor: Users can write HTML , CSS , and JavaScript code for each problem , similar to the CodePen editor.<br />  Code Compilation: Users can compile their code and generate the output.<br />  Score Display: Users can view the scores generated by the scoring engine based on the performed test cases.<br /><br /><br />Tools &amp;amp; Technologies<br /><br />Frontend:<br /><br /><br />  ReactJS is used develop the frontend of the scoring engine.<br /><br /><br />Backend:<br /><br /><br />  Node.js is employed for building the backend of the scoring engine.<br />  MySQL is used as the database management system.<br /><br /><br />Scoring Engine<br /><br /><br />  Puppeteer is used for implementing testcases and browser testing.<br />  Docker containers are utilized for testing code quality and running test cases.<br />  Flask is used to make scoring engine server which takes data and interacts with docker.<br /><br /><br />Conclusion<br /><br />By implementing a frontend scoring engine , we can automate frontend development evaluation , resulting in a streamlined and efficient assessment process. This blog has explored the goals , research , features , technical requirements , and tools and technologies involved in developing a frontend scoring engine. The automation of code assessment , real-time editing , and integration of testing tools have resulted in an efficient and comprehensive evaluation platform. The challenges we faced during development have strengthened our understanding of frontend development and inspired innovative solutions. As we move forward , we remain committed to enhancing the scoring engine to meet the evolving needs of the tech industry. <br />If you have any questions , doubts or suggestions feel free to reach out to me on LinkedIn<br />"
		} ,
	
		{
		  "title" : "Revamping eLitmus.com | Stand-Alone Front-end Module",
		  "category" : "technology",
		  "url" : "/technology/revamping-elitmus-dot-com-stand-alone-front-end-module/",
		  "date" : "2023-07-20 00:00:00 +0530",
		  "content"	: "The current elitmus.com is a web application built with Ruby on Rails Framework , and the views are sent directly from the backend server whenever requested. This was quite good before , but in present scenario of internet and web technologies , these seem to lack some very basic requirements. And Hence , an upgradation is required.<br /><br />Formally , current elitmus.com has a monolithic structure i.e. the front-end and the back-end are tightly coupled together. As a result of this , it is not possible to divide the project’s logic and team for front-end and back-end. Only Full Stack Developers having knowledge of both the domains are required in order to work in this project. This somehow limits the people who are more expertised in one of the domains.<br /><br />Also , the present elitmus.com is not using the latest web technologies available. This greatly impacts the user experience.<br /><br />So , What’s the solution for this ?<br /><br /><br /><br />Well , we can separate the front-end and back-end. This will solve all the problems faced by the developers who work or tends to work in this project. This solves some of the major issues faced today by developers.<br /><br />Now , we can have a distributed system , with the views ( front-end ) in one place and the Models and Controllers in the other. The Front-end we plan to build can be built using the latest and efficient web technologies currently available. This helps to improve the User Experience as well.<br /><br />What Benefits ?<br /><br /><br />  Developer Experience<br />    <br />      Team Separation → We can have Dedicated teams for front-end and back-end , each expertised in their own domains<br />      Logic Separation → We can separate the Logic of course for the frontend and backend<br />      Easy to Manage<br />      Easy to Scale<br />    <br />  <br />  User Experience<br />    <br />      Latest Web Tech like React can be used to Build Views<br />      Improved Speed<br />      Improved Performance<br />      Consistency in design<br />    <br />  <br /><br /><br /><br /><br />How do we do it?<br /><br />Well , now that we know what we have to do. We are halfway there already ( Just Kidding ). Let’s discuss some of the things we can use to make the front-end efficient and reliable.<br /><br /><br />  React JS<br />    <br />      Its component architecture  , helps us building a consistent design across the site.<br />      It’s fast and performant.<br />    <br />  <br />  Tailwind CSS<br />    <br />      This is a light-weight CSS framework which is highly reliable and easy to use.<br />      This has a good community , which can help to borrow UI components rather than making it from scratch.<br />    <br />  <br />  Redux Toolkit<br />    <br />      Redux Toolkit is a light version of Redux , which extracts away a lot of boilerplate codes and provides us easy to use APIs to manage state.<br />    <br />  <br />  Jest<br />    <br />      Jest is the most popular library for writing tests in a react application. Infact , Create-React-App provides support for this out of the box when we initiate a new react project.<br />    <br />  <br /><br /><br />So , that’s all the core technologies we can use to build an efficient and reliable front-end. But , here is the catch: we can even improve more by following certain practices , which will be fruitful in the long run.<br /><br /><br /><br />What else can we Improve ?<br /><br />Following are some of the best practices that we can use to further improve the frontend application.<br /><br /><br />  ES Lint<br />    <br />      Enforcing a code style guide is important to maintain the source code of the application. It helps to maintain consistency across the application.<br />      More Particularly , we can use the AirBNB Style Guide. This is the most popular style guide for React Application.<br />      We can add rules as per our need and requirements in the .eslintrc.js<br />    <br />  <br />  Nested Routes<br />    <br />      This is one of the features of react. We can nest the routes under other routes to maintain a route intuition.<br />    <br />  <br /><br /><br />&#60;;Route path=&ldquo;/jobs&ldquo; element={&#60;;JobsAndInterviews /&#62;;}&#62;;<br />    &#60;;Route index element={&#60;;AllJobs /&#62;;} /&#62;;<br />    &#60;;Route path=&ldquo;my_jobs&ldquo; element={&#60;;MyJobs /&#62;;}&#62;;<br />      &#60;;Route index element={&#60;;ActiveJobs /&#62;;} /&#62;;<br />      &#60;;Route path=&ldquo;active&ldquo; element={&#60;;ActiveJobs /&#62;;} /&#62;;<br />      &#60;;Route path=&ldquo;inactive&ldquo; element={&#60;;InActiveJobs /&#62;;} /&#62;;<br />      &#60;;Route path=&ldquo;interviews&ldquo; element={&#60;;Interviews /&#62;;} /&#62;;<br />    &#60;;/Route&#62;;<br />    &#60;;Route path=&ldquo;all_jobs&ldquo; element={&#60;;AllJobs /&#62;;} /&#62;;<br />  &#60;;/Route&#62;;<br /><br />Like in this example snippet , we have a parent route for job and under that my_jobs and inside that we have active , inactive , interviews.<br /><br />  <br />    /jobs/my_jobs/active → this route path is really gives a lot of information of the pages.<br />  <br />  <br />    Dynamic Routes<br />    <br />      This is another feature of React Itself. This allows us to only load the pages that are requested by the user and not all.<br />      Just imagine , our site has hundreds of pages. When the user wants to visit the homepage , we are trying to send him all the hundred pages. This doesn’t make any sense right ?<br />    <br />  <br /><br /><br />// Jobs Page Routes<br />  export const JobsAndInterviews = lazy(() =&#62;; import(&amp;#39;../pages/Jobs&amp;#39;));<br />  export const AllJobs = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/AllJobs&amp;#39;));<br />  export const ApplyJob = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/ApplyJob&amp;#39;));<br />  export const JobDetails = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/JobDetails&amp;#39;));<br />  export const MyJobs = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/MyJobs&amp;#39;));<br />  export const ActiveJobs = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/MyJobs/Active&amp;#39;));<br />  export const InActiveJobs = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/MyJobs/Inactive&amp;#39;));<br />  export const Interviews = lazy(() =&#62;; import(&amp;#39;../pages/Jobs/MyJobs/Interviews&amp;#39;));<br /><br />This above snippet shows how to import the components dynamically. But for this thing to work , we need to wrap the Routes in a Suspense Component which takes fallback.<br />  The component given inside the fallback is rendered in between the dynamic loads. So , we can put our page loader here. Below is the snippet showing how to do it.<br /><br />import jobsRoutes from &amp;#39;./routes/jobsRoutes&amp;#39;;<br />  const App = () =&#62;; (<br />  &#60;;Router&#62;;<br />    &#60;;Provider store={store}&#62;;<br />      &#60;;Layout&#62;;<br />        &#60;;Suspense fallback={() =&#62;; &#60;;Loader /&#62;;}&#62;;<br />          &#60;;Routes&#62;;<br />            {jobsRoutes}<br />          &#60;;/Routes&#62;;<br />        &#60;;/Suspense&#62;;<br />      &#60;;/Layout&#62;;<br />    &#60;;/Provider&#62;;<br />  &#60;;/Router&#62;;<br />  );<br /><br />Now , this makes the website immensely faster than before.<br /><br />  Intuitive File and Folder Organization -&#62;; Organizing the files and folders properly is a very important task because it significantly helps the new developers. It lowers the learning curve for the new fellas.<br /><br /><br />/src<br />    /__tests__<br />      /categoryA<br />        /page1.test.js<br />        /page2.test.js<br />      /categoryB<br />        /page1.test.js<br />        /page2.test.js<br />    /assets<br />    /components<br />      /customElements<br />      /Layout<br />    /features<br />      /redux_slices.js<br />    /pages<br />      /categoryA<br />        /page1.jsx<br />        /page2.jsx<br />      /categoryB<br />        /page1.jsx<br />        /page2.jsx<br />    /routes<br />    /store<br />      /redux_store.js<br />    /styles<br /><br />That’s how we can improve our codebase even more.<br /><br />Then , we have to make sure if our application runs the same on every device , OS , and system specs. For that we can dockerize the react app.<br /><br /><br /><br />Dockerization and Deployment<br /><br />Dockerizing the react app gives us the following benefits:<br /><br /><br />  Consistency: Docker ensures the app runs consistently across different environments.<br />  Dependency Management: Docker encapsulates app dependencies , preventing conflicts.<br />  Easy Deployment: Docker simplifies deployment to various environments.<br />  Scalability: Docker facilitates easy scaling to handle increased traffic.<br />  Versioning and Rollbacks: Docker images can be versioned , enabling controlled updates and rollbacks.<br />  Development and Testing: Docker streamlines development and testing in a consistent environment.<br />  Infrastructure Agnostic: Docker allows running the app on various infrastructures.<br />  Resource Efficiency: Docker containers are lightweight and efficient in resource utilization.<br />  Easy Collaboration: Docker promotes seamless collaboration among developers and teams.<br />  Security: Docker provides isolation , adding an extra layer of security to the app.<br /><br /><br />We can dockerize the react app by adding docker files i.e.<br /><br /><br />  Dockerfile → contains environment and installation instructions for the app.<br /><br /><br />FROM node:18 as builder<br />  WORKDIR /app<br />  COPY package.json .<br />  RUN npm install<br />  COPY . .<br />  RUN npm run build<br />  FROM nginx<br />  EXPOSE 80<br />  COPY --from=builder /app/build /usr/share/nginx/html<br /><br /><br />  docker-compose.yml → contain commands to run our docker container.<br /><br /><br />version: &amp;#39;3&amp;#39;<br />  services:<br />    web:<br />      build:<br />        context: .<br />        dockerfile: Dockerfile<br />      ports:<br />        - &amp;#39;80:80&amp;#39;<br /><br />Now , we have successfully containerized our react application. Finally , we need to deploy it to some cloud services such as AWS.<br /><br /><br />  We can first push our docker image to docker hub<br /><br /><br />docker push iamsmruti/elitmus-frontend<br /><br /><br />  Then we can login to EC2 instance and then pull the docker image<br /><br /><br />docker pull iamsmruti/elitmus-frontend<br /><br /><br />  Finally , we can run the docker image<br /><br /><br />docker run -d -p 5000:5000 iamsmruti/elitmus-frontend<br /><br />That wraps up our frontend application which can now be live. It is fully capable of consuming the APIs from the backend. Now , the business logic is in the backend and doesn’t put much load on the frontend and hence it is performant and reliable.<br /><br />If you have any questions , doubts , you can ping me at smrutiranjanbadatya2@gmail.com.<br /><br />I would definitely get back to you.<br /><br />I Hope this was a helpful and insightful guide for making a better frontend application with all the necessary good practices to maintain sustainability of the project.<br /><br />See Ya 👋🏻 … Peace ✌🏻<br /><br />References<br /><br />  <br />    React Docs - Here<br />  <br />  <br />    Tailwind Docs - Here<br />  <br />  <br />    Redux Toolkit Docs - Here<br />  <br />  <br />    Jest Docs - Here<br />  <br />  <br />    ES Lint Docs - Here<br />  <br />  <br />    Docker Docs - Here<br />  <br /><br />"
		} ,
	
		{
		  "title" : "My Experience as a Summer Intern at eLitmus: Building a Telegram Bot",
		  "category" : "technology",
		  "url" : "/technology/my-experience-as-a-summer-intern-at-elitmus-building-a-telegram-bot/",
		  "date" : "2023-07-19 00:00:00 +0530",
		  "content"	: "As a summer intern at eLitmus , I had the opportunity to work on an exciting project that involved building a Telegram Bot. In today’s digital era , effective communication channels play a crucial role in connecting businesses with their stakeholders. eLitmus , a talent-tech platform , identified the need for a two-way communication channel between the platform and candidates. To achieve this , Telegram bots were chosen as the ideal starting point. This blog post will delve into the Telegram Bot Integration project.<br /><br />How it Began:<br /><br />The project started with the idea of leveraging the Telegram platform as a communication channel between eLitmus and its candidates. The goal was to create a two-way communication channel , enabling candidates to access information , receive updates , and engage in various activities through Telegram bots. This opened up possibilities for automating communication , collecting data , running quizzes , and providing valuable services to candidates.<br /><br />Design:<br /><br />Before diving into development phase , thorough planning and design are crucial. I begin by defining the core functionalities of the Telegram bots. I discovered that creating a bot through Bot Father (Telegram’s official bot) was the standard approach. As I was tasked with implementing the project using Ruby on Rails , I focused on two key aspects: developing the Telegram bot and designing the Admin panel.<br />Designing such an application involves three key aspects: architecture design , database design , and UI/UX design. Let’s dive into each of these parts in more detail:<br /><br /><br />  Architecture : The Telegram bots interact with users through messages and commands. Users can access FAQs , participate in quizzes , and receive responses based on their interactions with the bots. The bots handle user inputs , validate quiz answers , and provide feedback and results accordingly. An intuitive admin panel is developed using Ruby on Rails to facilitate easy management of the bot’s functionalities. The admin panel allows administrators to add , update , and delete FAQs , quizzes , and other content. It also provides insights and analytics related to user engagement and bot usage.<br />  <br />  <br />    Database: The project utilizes a MySQL database to store and manage data related to users , FAQs , quizzes , quiz attempts , analytics , and other relevant information. The database schema is designed to efficiently store and retrieve data , ensuring optimal performance.<br />  <br />  UI/UX:  To ensure a visually appealing and user-friendly Telegram bot interface , I delved into various UI options and explored the best ways to present information and interact with users. This research helped me identify the most effective strategies for creating an engaging and intuitive bot interface. And for the Admin panel , I took the initiative to design the entire interface using Figma. By visualizing the layout , components , and functionalities , I was able to ensure a cohesive and user-friendly experience for administrators managing the bot’s functionalities. Figma provided a powerful toolset for creating wireframes , mock-ups , and interactive prototypes , allowing me to iterate and refine the design before implementation.<br /><br /><br />Development:<br /><br />Before starting this project , I had experience developing mobile applications , and most of them followed the Model-View-Template (MVT) pattern for backend , such as Django. However , for this project , I needed to learn and work with Ruby on Rails , which follows the Model-View-Controller (MVC) architectural pattern. Fortunately , my previous experience with backend development made it easier for me to understand Rails , and within the first two weeks , I was able to develop the basic functionalities of both the FAQ and Quiz bots.<br /><br />Integrating the telegram bot consists of 3 steps:<br /><br /><br />  <br />    Creating a bot using Bot Father ( Official bot of telegram for creating telegram bot) and get the token that was generated by the bot father.<br /><br />    <br />  <br />  <br />    Initalizing the bot in the ruby file and declare a listening function that listens every messsage from the bot.<br />  <br />  <br />    Writing the message specified functions that is called only when a specified message if recieved from the bot.<br />  <br /><br /><br />I have used Ruby on Rails for both front-end and back-end to develope admin panel. For database I have used mysql and for hosting purpose I have used AWS , EC2 to host admin panel using docker and telegram and RDS for database.<br /><br />Using docker to host the bot and admin panel was another part of the development that gave me an idea of how to does docker used by most of the companies , it was my personal goal in the year to learn docker so it got done by this project. And to say using docker wasn’t the difficult part. I had to learn how to write a docker file and docker compose file.<br /><br />Features Developed<br /><br /><br />  User Flow<br /><br /><br />I focused on refining the functionalities and user flow of the bots , particularly in the context of the Telegram channels. The FAQ bot is connected to the Telegram channel , and when a user posts a question in the channel’s comment section , it gets stored in the database. The admin can then view and answer the question , which is sent back to the user personally through Telegram. Additionally , users can access the FAQ bot to view existing FAQs and request the addition of new ones.<br /><br /><br />    <br />      FAQ bot flow<br />    <br />  <br /><br />    <br />      <br />      <br />      <br />      <br />         <br />      <br />    <br />  <br /><br /><br />    <br />      Quiz bot flow<br />    <br />  <br /><br />    <br />      <br />         <br />      <br />      <br />        <br />      <br />    <br />  <br /><br /><br />  <br />    Admin Panel<br /><br />    On the other hand , the admin panel allows the admin to create quizzes and questions. These quizzes are then posted in the Telegram channel , with a button redirecting users to the Quiz bot. Users can access multiple quizzes and attempt them through the bot.<br /><br />    By developing these functionalities , I was able to establish a seamless flow for users , ensuring they can interact with the bots and access relevant information easily. The admin panel provides the necessary tools for managing FAQs , quizzes , and user interactions , allowing for efficient administration and engagement with the users.<br /><br />    In the Admin panel , I implemented the design that I had previously created using Figma. The Admin panel offers various functionalities to enhance the administration and management of the Telegram bots. Here are some key features of the Admin panel:<br /><br />    <br />      <br />        User Management: The Admin panel allows the admin to view active users and access individual user data. This includes information about the user’s activities , quiz attempts , and questions asked through the bot.<br />      <br />      <br />        FAQ Management: The Admin can view and manage the FAQs. They have the ability to add , edit , or remove FAQs as needed. Additionally , the Admin can track the number of reads by users , providing insights into the popularity and relevance of different FAQs.<br />      <br />      <br />        Quiz Management: The Admin can create quizzes and manage them within the Admin panel. They can add questions , set multiple options , and define correct answers. The Admin also has access to the responses of the quizzes , allowing them to analyze individual question analytics and gain insights into user performance. This can also be used to host surveys on telegram channels.<br />      <br />      <br />        Analytics: The Admin panel provides analytics on user activities related to both the FAQ and Quiz bots. The Admin can view data such as the number of attempts per day , week , month , or year , as well as the number of FAQ reads per day , week , month , or year. These analytics help the Admin understand user engagement and make data-driven decisions.<br />      <br />      <br />        Post Management: The Admin can utilize the post section in the Admin panel to create and publish posts in the Telegram channel directly from Telegram and to make it effective I have created two phases of create and publishing the post so that post get reviewed before publishing the post. This feature streamlines the process of sharing content and updates with users in the channel.<br />      <br />    <br /><br />    Admin Panel<br /><br />    <br /><br />    By incorporating these functionalities into the Admin panel , I ensured that the administrative tasks associated with managing the Telegram bots were streamlined and efficient. The panel provides comprehensive control and insights , empowering the admin to effectively manage user interactions , content , and analytics.<br />  <br /><br /><br />Challenges Faced:<br /><br />Working with 3rd party API’s is one of the most challenging task and that is the challenging task of the project using telegram bot API. I could able to use telegram api to minimal amount of data of user , for example I couldn’t able to get users contact details , and I have crossed this challenge by finidng a feature of telegram that is by using permissions to access user details and request user to send the mobile number and location , but I couldn’t able to get location from the web or laptop. The biggest challenge I have faced was setting up and displaying analytics using charts and graphs. Initially , I tried using gems like Chartkick and FusionCharts , but faced issues with rendering the graphs correctly. Despite spending considerable time troubleshooting , the graphs weren’t displaying as expected. Eventually , I opted for Chart.js , which proved to be a more suitable solution for my needs. With Chart.js , I could create visually appealing and interactive charts to showcase the data collected through admin panel. The transition to Chart.js was smooth , and it enabled me to present data insights effectively , providing a valuable user experience.<br /><br />Conclusion:<br /><br />In summary , working on this project presented its fair share of challenges. However , with perseverance and problem-solving skills , I was able to overcome these obstacles and achieve success. I was able to develop the Telegram bots and the Admin panel effectively. I am thrilled to share that my hard work did not go unnoticed , and my project was selected for use by the company. This recognition is truly gratifying , as it demonstrates the value my work brings to the organization and the impact it can have on the company operations. Overall , this project was a rewarding journey that expanded my knowledge and skills in web development.<br />"
		} ,
	
		{
		  "title" : "Resume Parsing: Insights and Steps to Create Your Own Parser",
		  "category" : "technology",
		  "url" : "/technology/resume-parsing-insights-and-steps-to-create-your-own-parser/",
		  "date" : "2023-06-20 00:00:00 +0530",
		  "content"	: "Resume parsing is the automated process of extracting relevant information from resumes or CVs. <br />It analyzes the unstructured text of a resume and extracts specific details like contact information , work experience , education , skills , and achievements. <br />The extracted data is then converted into a structured format , allowing for easy analysis and integration into recruitment systems.<br /><br />Benefits of Resume Parsing<br /><br /><br />  It is a time-saving automation<br />  It increases efficiency in candidate screening<br />  Improves accuracy in data extraction<br />  It standardizes the data extraction and formatting<br /><br /><br />What you’ll learn from this blog:<br /><br /><br />  Resume parsing techniques for different file formats.<br />  Extracting specific details from resumes.<br />  Leveraging NLP techniques for parsing.<br />  Handling multicolumn resumes.<br />  Dockerizing the Application: Simplifying Deployment and Scalability<br />  Hosting it on AWS EC2.<br /><br /><br />Let’s get Started 🎉<br /><br />We’ll utilize Python and its Flask framework to create a resume parsing server.<br /><br />Application Flow Chart:<br /><br /><br /><br />We will be primarily working on 3 categories of file formats:<br /><br />  PDF<br />  DOCX<br />  Images (.png , .jpg , etc.)<br /><br /><br />Data that we will be extracting<br /><br /><br />  Embedded links in PDF<br />  Personal data: <br /> 2.1. Name: First name and last name <br /> 2.2. Email <br /> 2.3. Phone Number <br /> 2.4. Address: City , Country , and Zip code <br /> 2.5. Links: Social and Coding Platform links <br />  Education <br /> 3.1. Institute name <br /> 3.2. Duration: Start date and End date <br /> 3.3. Grade/CGPA <br /> 3.4. Degree <br />  Experience<br /> 4.1. Company name<br /> 4.2. Role<br /> 4.3. Durations: Start date and End date<br /> 4.4. Skills<br />  Certification: <br /> 5.1. Description <br /> 5.2. Duration <br /> 5.3. Skill <br />  Project: <br /> 6.1. Project name <br /> 6.2. Skills <br /> 6.3. Description <br />  Skills<br />  Achievements<br />  Exam scores<br /> 9.1. Exam name<br /> 9.2 Score<br />  All other sections present in resume<br /><br /><br />Date/Duration Extraction<br /><br />To extract dates from text , we will use datefinder module , and regexp to extract years.<br />Then we will combine these two and sort dates to get start and end date for our duration.<br /><br />import re<br />from datetime import date<br />import datefinder<br /><br /><br />def get_date(input_string):<br />    '''Get date from text'''<br />    matches = list(datefinder.find_dates(input_string))<br /><br />    res = []<br />    for i in matches:<br />        date_str = str(i).split(' ')<br />        extracted_date = date_str[0]<br /><br />        res.append(extracted_date)<br />    return res<br /><br /><br />def get_years(txt):<br />    '''Get years from text'''<br />    pattern = r'[0-9]{4}'<br />    lst = re.findall(pattern , txt)<br /><br />    current_date = date.today()<br />    current_year = current_date.year<br />    res = []<br />    for i in lst:<br />        year = int(i)<br />        if 1900 &#60;;= year &#60;;= (current_year + 10):<br />            res.append(i + &quot;-01-01&quot;)<br />    return res<br /><br /><br />def get_duration(input_text):<br />    '''Get duration from text'''<br /><br />    dates = get_date(input_text)<br />    years = get_years(input_text)<br /><br />    for i in years:<br />        dates.append(i)<br />    dates.sort()<br /><br />    duration = {<br />        &quot;start_date&quot;: &quot;&quot; ,<br />        &quot;end_date&quot;: &quot;&quot;<br />    }<br />    if len(dates) &#62;; 1:<br />        duration[&quot;start_date&quot;] = dates[0]<br />        duration[&quot;end_date&quot;] = dates[len(dates) - 1]<br />    return duration<br /><br /><br /><br />Extracting links from PDF:<br /><br />To extract links from the PDF , we will use the python module PDFx.<br /><br />import pdfx<br /><br />def get_urls_from_pdf(file_path):<br />    '''extract urls from pdf file'''<br />    url_list = []<br /><br />    # for invalid file path<br />    if os.path.exists(file_path) is False:<br />        return url_list<br /><br />    pdf = pdfx.PDFx(file_path)<br /><br />    # get urls<br />    pdf_url_dict = pdf.get_references_as_dict()<br /><br />    if &quot;url&quot; not in pdf_url_dict.keys():<br />        return url_list<br /><br />    url_list = pdf_url_dict[&quot;url&quot;]<br /><br />    return url_list<br /><br /><br />PDF to Text<br /><br />import pdfx<br />def get_text_from_pdf(file_path):<br />    '''extract complete text from pdf'''<br /><br />    # for invalid file path<br />    if os.path.exists(file_path) is False:<br />        return &quot;&quot;<br /><br />    pdf = pdfx.PDFx(file_path)<br /><br />    pdf_text = pdf.get_text()<br /><br />    return pdf_text<br /><br /><br /><br />Extracting Personal Details:<br /><br />We will extract text from the PDF and move ahead with further extractions.<br /><br />Name<br /><br />Extracting the name from the text is one of the challenging tasks.<br /><br />For this , we will be using NLP: Named Entity Recognition to extract name from the text.<br /><br />NLP function:<br /><br />def get_name_via_nltk(input_text):<br />    '''extract name from text via nltk functions'''<br />    names = []<br />    for sent in nltk.sent_tokenize(input_text):<br />        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):<br />            if hasattr(chunk , 'label'):<br />                name = ' '.join(c[0] for c in chunk.leaves())<br />                names.append(name)<br />    return names<br /><br /><br />  The text is tokenized into sentences using nltk.sent_tokenize().<br />  Each sentence is further tokenized into words using nltk.word_tokenize().<br />  The part-of-speech tags are assigned to each word using nltk.pos_tag().<br />  The named entities are identified by applying the named entity recognition (NER) using nltk.ne_chunk().<br />  For each identified named entity chunk , if it has a ‘label’ , indicating it is a named entity , the individual words are concatenated to form a name.<br />  The extracted names are appended to the names list.<br /><br /><br />Phone Number<br /><br />To extract the Phone number , we use the following module phonenumbers , we extract users country from text and using that we will extract relevant phone numbers.<br />import geotext<br />from phonenumbers import PhoneNumberMatcher<br /><br />def get_phone(input_text):<br />    '''extract phone number from text'''<br /><br />    phone_numbers = []<br /><br />    countries_dict = geotext.GeoText(input_text).country_mentions<br />    <br />    country_code = &quot;IN&quot;<br />    for i in countries_dict.items():<br />        country_code = i[0]<br />        break<br /><br />    search_result = PhoneNumberMatcher(input_text , country_code)<br /><br />    phone_number_list = []<br />    for i in search_result:<br />        i = str(i).split(' ')<br />        match = i[2:]<br /><br />        phone_number = ''.join(match)<br />        phone_number_list.append(phone_number)<br /><br />    for i in phone_number_list:<br />        if i not in phone_numbers:<br />            phone_numbers.append(i)<br /><br />    return phone_numbers<br /><br /><br />Email<br /><br />To extract the Email , we use the following regexp: [^\s]+@[^\s]+[.][^\s]+<br />def get_email(input_text):<br />    '''extract email from text'''<br />    email_pattern = '[^\s]+@[^\s]+[.][^\s]+'<br /><br />    emails = []<br />    emails = re.findall(email_pattern , input_text)<br /><br />    # pick only unique emails<br />    emails = set(emails)<br />    emails = list(emails)<br /><br />    return emails<br /><br /><br /><br />Address<br /><br />To Extract address , we use the geotext module; we get City , Country , and Zipcode.<br />import geotext<br />def get_address(input_arr):<br />    '''get address information from input array'''<br /><br />    input_text = &quot;  &quot;.join(input_arr)<br /><br />    res = {}<br />    # getting all countries<br />    countries_dict = geotext.GeoText(input_text).country_mentions<br /><br />    res[&quot;country&quot;] = []<br />    for i in countries_dict:<br />        res[&quot;country&quot;].append(i)<br /><br />    # getting all cities<br />    res[&quot;city&quot;] = geotext.GeoText(input_text).cities<br /><br />    # zip code<br />    pattern = &quot;\b([1-9]{1}[0-9]{5}|[1-9]{1}[0-9]{2}\\s[0-9]{3})\b&quot;<br />    res[&quot;zipcode&quot;] = re.findall(pattern , input_text)<br /><br />    return res<br /><br /><br /><br />Links<br /><br />As we already have a URL list from 1st operation , we will match links from a list of our own , this can be saved in any database or hard-coded , and categorize them into social or coding sections.<br /><br />Other Sections<br /><br />There can be many sections in a resume , that we cannot always account for.<br />To extract them , we will create a list of possible section heading and match them against each line from the resume that we have extracted.<br /><br />The code will be as following:<br /><br />from utils import dynamo_db<br /><br />RESUME_SECTIONS = dynamo_db.get_item_db(&quot;RESUME_SECTIONS&quot;)<br /><br /><br />def extract_resume_sections(text):<br />    '''Extract section based on resume heading keywords'''<br />    text_split = [i.strip() for i in text.split('')]<br /><br />    entities = {}<br />    entities[&quot;extra&quot;] = []<br />    key = False<br />    for phrase in text_split:<br />        if len(phrase.split(' ')) &#62;; 10:<br />            if key is not False:<br />                entities[key].append(phrase)<br />            else:<br />                entities[&quot;extra&quot;].append(phrase)<br />            continue<br /><br />        if len(phrase) == 1:<br />            p_key = phrase<br />        else:<br />            p_key = set(phrase.lower().split()) &amp;amp; set(RESUME_SECTIONS)<br /><br />        try:<br />            p_key = list(p_key)[0]<br />        except IndexError:<br />            pass<br /><br />        if p_key in RESUME_SECTIONS and (p_key not in entities.keys()):<br />            entities[p_key] = []<br />            key = p_key<br />        elif key and phrase.strip():<br />            entities[key].append(phrase)<br />        else:<br />            if len(phrase.strip()) &#60;; 1:<br />                continue<br />            entities[&quot;extra&quot;].append(phrase)<br /><br />    return entities<br /><br /><br /><br />Education<br /><br />To extract education , we need to identify a line from our education section that represent the school/institute name , and a line that represents the degree. After which we can search for CGPA or Percentage using regexp.<br />For name recognition , we will make use of a list of keywords that can be present in the name.<br /><br />Code to get school name , similarly we can implement to get degree as well.<br />import re<br />from utils import helper , dynamo_db<br /><br />SCHOOL_KEYWORDS = dynamo_db.get_item_db(&quot;SCHOOL_KEYWORDS&quot;)<br /><br /><br />def get_school_name(input_text):<br />    '''Extract list of school names from text'''<br />    text_split = [i.strip() for i in input_text.split('')]<br /><br />    school_names = []<br /><br />    for phrase in text_split:<br />        p_key = set(phrase.lower().split(' ')) &amp;amp; set(SCHOOL_KEYWORDS)<br /><br />        if (len(p_key) == 0):<br />            continue<br /><br />        school_names.append(phrase)<br />    return school_names<br /><br /><br /><br />Code to extract CGPA/GPA or Percentage grade<br />def get_percentage(txt):<br />    '''Extract percentage from text'''<br />    pattern = r'((\d+)?\d+%)'<br />    lst = re.findall(pattern , txt)<br />    lst = [i[0] for i in lst]<br />    return lst<br /><br /><br />def get_gpa(txt):<br />    '''Extract cgpa or gpa from text in format x.x/x'''<br />    pattern = r'((\d+)?\d+\/\d+)'<br />    lst = re.findall(pattern , txt)<br />    lst = [i[0] for i in lst]<br />    return lst<br /><br /><br />def get_grades(input_text):<br />    '''Extract grades from text'''<br />    input_text = input_text.lower()<br />    # gpa<br />    gpa = get_gpa(input_text)<br /><br />    if (len(gpa) != 0):<br />        return gpa<br /><br />    # percentage<br />    percentage = get_percentage(input_text)<br /><br />    if (len(percentage) != 0):<br />        return percentage<br /><br />    return []<br /><br /><br />Skills<br /><br />In order to extract skills from the text , a master list of commonly used skills can be created and stored in a database , such as AWS DynamoDB. Each skill from the list can be matched against the text to identify relevant skills. By doing so , a comprehensive master skill list can be generated , which can be utilized for more specific skill extraction in subsequent sections.<br /><br /><br />from utils import dynamo_db<br /><br />skills = dynamo_db.get_item_db(&quot;ALL_SKILLS&quot;)<br /><br /><br />def get_skill_tags(input_text):<br />    '''Extract skill tags from text'''<br />    user_skills = []<br />    for skill in skills:<br />        if skill in input_text.lower():<br />            user_skills.append(skill.upper())<br /><br />    return user_skills<br /><br /><br /><br />Experience<br /><br />To extract company names and roles , a similar strategy can be employed as we used for finding school names and degrees. By applying appropriate techniques , such as named entity recognition or pattern matching , we can identify company names and associated job roles from the text. Additionally , for skill extraction , we can match the text against our previously calculated list of skills to identify and extract relevant skills mentioned in the text<br /><br />Achievements and Certifications<br /><br />We can use the section text that we extracted previously and for each line of it , we can search for duration and skills in it.<br /><br /><br />from utils import helper , skill_tags<br /><br /><br />def get_certifications(input_array):<br />    '''Function to extract certificate information'''<br /><br />    res = {<br />        &quot;description&quot;: input_array ,<br />        &quot;details&quot;: []<br />    }<br /><br />    try:<br /><br />        for cert in input_array:<br />            elem_dict = {<br />                &quot;institute_name&quot;: str(cert) ,<br />                &quot;skills&quot;: skill_tags.get_skill_tags(cert) ,<br />                &quot;duration&quot;: helper.get_duration(cert)<br />            }<br />            res[&quot;details&quot;].append(elem_dict)<br /><br />    except Exception as function_exception:<br />        helper.logger.error(function_exception)<br /><br />    return res<br /><br /><br /><br />Projects<br /><br />When it comes to extracting project titles , it can be challenging due to the variations in how individuals choose to title their projects. However , we can make an assumption that project titles are often written in a larger font size compared to the rest of the text. Leveraging this assumption , we can analyze the font sizes of each line in the text and sort them in descending order. By selecting the lines with the largest font sizes from the top , we can identify potential project titles. This approach allows us to further segment the project section and extract additional details such as skills utilized and project durations.<br /><br />Link: How to find the Font Size of every paragraph of PDF file using python code?<br />import fitz<br /><br />def scrape(keyword , filePath):<br />    results = [] # list of tuples that store the information as (text , font size , font name) <br />    pdf = fitz.open(filePath) # filePath is a string that contains the path to the pdf<br />    for page in pdf:<br />        dict = page.get_text(&quot;dict&quot;)<br />        blocks = dict[&quot;blocks&quot;]<br />        for block in blocks:<br />            if &quot;lines&quot; in block.keys():<br />                spans = block['lines']<br />                for span in spans:<br />                    data = span['spans']<br />                    for lines in data:<br />                            results.append((lines['text'] , lines['size'] , lines['font']))<br /><br />    pdf.close()<br />    return results<br /><br /><br />Using this we find our project titles:<br />from utils import helper , skill_tags<br />from difflib import SequenceMatcher<br /><br />def similar(string_a , string_b):<br />    '''Find similarity between two string'''<br />    return SequenceMatcher(None , string_a , string_b).ratio()<br /><br />def extract_project_titles(input_array , text_font_size):<br />    ls = []<br />    for line_tuple in text_font_size:<br />        line = line_tuple[0]<br />        for s in input_array:<br />            if similar(line ,s) &#62;; 0.85:<br />                ls.append([line_tuple[1] , s])<br />    ls.sort(reverse=True)<br /><br />    title_font_size = ls[0][0] if(len(ls) &#62;; 0) else 0<br />    project_title = []<br />    for i in ls:<br />        if i[0] == title_font_size:<br />          project_title.append(i[1])<br />    return project_title<br /><br />def get_projects(input_array , text_font_size):<br />    '''extract project details from text'''<br />    res = {<br />        &quot;description&quot;: input_array ,<br />        &quot;details&quot;: []<br />    }<br />    txt = '  '.join(input_array)<br /><br />    project_titles = helper.extract_titles_via_font_size(<br />        input_array , text_font_size)<br /><br />    project_sections = helper.extract_sections(txt , project_titles)<br /><br />    try:<br />        for i in project_sections.items():<br />            key = i[0]<br />            txt = ''.join(project_sections[key])<br /><br />            elem_dict = {<br />                &quot;project_name&quot;: key ,<br />                &quot;skills&quot;: skill_tags.get_skill_tags(txt) ,<br />                &quot;duration&quot;: helper.get_duration(txt)<br />            }<br /><br />            res[&quot;details&quot;].append(elem_dict)<br />    except Exception as function_exception:<br />        helper.logger.error(function_exception)<br /><br />    return res<br /><br /><br /><br />Handling multicolumn resumes<br /><br />Up until now , we have explored techniques to handle single-column resumes successfully. <br />However , when it comes to two-column or multicolumn resumes , a direct extraction of text may not be sufficient. If we attempt to extract text from a multicolumn PDF using the same method as before , we will encounter challenges such as , the text from different columns will merge together , as our previous approach scans the text from left to right and top to bottom , rather than column-wise.<br /><br />To overcome this issue , let’s delve into how we can solve this problem and effectively handle multicolumn resumes.<br /><br />Drawing textboxes<br /><br />Optical Character Recognition (OCR) comes to the rescue by identifying textboxes and providing their coordinates within the document. By utilizing OCR , we can pinpoint the location of these textboxes , which serve as a starting point for further analysis.<br /><br />To tackle the challenge of multicolumn resumes , a line sweep algorithm is implemented. This algorithm systematically scans along the X-axis and determines how many textboxes intersect each point. By analyzing this distribution , potential column divide lines can be inferred. These lines act as reference markers , indicating the boundaries between columns.<br /><br />Once the column lines are established , the text can be extracted from the identified textboxes in a column-wise manner. Following the order of the column lines , the text can be retrieved and processed accordingly.<br /><br />By leveraging OCR , the line sweep algorithm , and the concept of column lines , we can effectively handle multicolumn resumes and extract the necessary information in an organized and structured manner.<br /><br />Code:<br />import cv2<br />import fitz<br />from fitz import Document , Page , Rect<br />import pytesseract<br />import functools<br /><br />def textbox_recognition(file_path):<br />    '''Extract text_boxes from image'''<br /><br />    img = cv2.imread(file_path , cv2.IMREAD_GRAYSCALE)<br /><br />    ret , thresh1 = cv2.threshold(<br />        img , 0 , 255 , cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)<br /><br />    # kernel<br />    kernel_size = 10<br />    rect_kernel = cv2.getStructuringElement(<br />        cv2.MORPH_RECT , (kernel_size , kernel_size))<br /><br />    # Applying dilation on the threshold image<br />    dilation = cv2.dilate(thresh1 , rect_kernel , iterations=1)<br /><br />    # Finding contours<br />    contours , hierarchy = cv2.findContours(<br />        dilation , cv2.RETR_EXTERNAL , cv2.CHAIN_APPROX_NONE)<br /><br />    segments = []<br />    text_boxes = []<br />    # Looping through the identified contours<br />    for cnt in contours:<br />        x , y , w , h = cv2.boundingRect(cnt)<br />        cv2.rectangle(img , (x , y) , (x + w , y + h) , (0 , 255 , 0) , 2)<br />        segments.append([x , x+w])<br />        text_boxes.append((x , y , w , h))<br /><br />    return (segments , text_boxes)<br /><br /><br />def detect_column_lines(segments):<br />    '''Detect column lines from segments'''<br /><br />    mx = max(i[1] for i in segments)<br /><br />    line_sweep_arr = [0 for _ in range(mx+10)]<br />    for i in segments:<br />        line_sweep_arr[i[0] + 1] += 1<br />        line_sweep_arr[i[1]] -= 1<br /><br />    for i in range(1 , mx+10):<br />        line_sweep_arr[i] += line_sweep_arr[i-1]<br /><br />    line_mean = sum(line_sweep_arr)/len(line_sweep_arr)<br /><br />    potential_points = []<br />    for i in range(1 , mx+10):<br />        if line_sweep_arr[i] &#60;; int(line_mean/2.5):<br />            potential_points.append(i)<br /><br />    line_points = []<br />    for i in potential_points:<br />        if len(line_points) == 0:<br />            line_points.append(i)<br />            continue<br />        prev = line_points[len(line_points) - 1]<br /><br />        if i == prev + 1:<br />            line_points[len(line_points) - 1] = i<br />        else:<br />            line_points.append(i)<br /><br />    return line_points<br /><br /><br />def get_text(img , box_data):<br />    '''Extract text from given box data'''<br />    (x , y , w , h) = box_data<br />    cropped_image = img[y:y+h , x:x+w]<br /><br />    # to show image<br />    txt = pytesseract.image_to_string(cropped_image)<br />    return txt<br /><br /><br />def box_coverage_percentage(x , w , line):<br />    '''Extract coverage area in percentage for box'''<br /><br />    covered_width = line - x<br />    cover_percentage = covered_width / w<br />    return cover_percentage<br /><br /><br />def clean_text(txt):<br />    '''Clean text'''<br />    txt = txt.strip()<br />    txt = txt.replace(&quot;•&quot; , '')<br />    return txt<br /><br /><br />Y_LIMIT = 10<br /><br /><br />def custom_sort(a , b):<br />    '''custom sort logic'''<br />    if a[1] - Y_LIMIT &#60;;= b[1] &#62;;= a[1] + Y_LIMIT:<br />        return -1 if (a[0] &#60;;= b[0]) else 1<br />    return -1 if (a[1] &#60;;= b[1]) else 1<br /><br /><br />def get_boxes_for_line(text_boxes , line , ordered_text_box , prev_line):<br />    '''get boxes with line constraints'''<br />    temp_boxes = [i for i in text_boxes]<br />    temp_boxes.sort(key=functools.cmp_to_key(custom_sort))<br /><br />    res = []<br /><br />    # check if 90% of box is before line<br />    for box in temp_boxes:<br />        if box in ordered_text_box:<br />            continue<br /><br />        (x , y , w , h) = box<br /><br />        if (x &#62;;= prev_line - Y_LIMIT and x &#60;; line and box_coverage_percentage(x , w , line) &#62;;= 0.9):<br />            res.append(box)<br />    res.sort(key=lambda x: x[1])<br />    return res<br /><br /><br />def map_size(x , org , new):<br />    '''map box co-ordinates from image to pdf'''<br />    return (x*new)/org<br /><br /><br />def get_text_from_pdf(box , img_shape , pdf_shape , page):<br />    '''extract text from pdf box'''<br />    (x , y , w , h) = box<br />    (height , width) = img_shape<br />    (W , H) = pdf_shape<br />    x = map_size(x , width , W)<br />    w = map_size(w , width , W)<br />    y = map_size(y , height , H)<br />    h = map_size(h , height , W)<br />    rect = Rect(x , y , x+w , y+h)<br />    text = page.get_textbox(rect)<br />    return text<br /><br /><br />def image_to_text(file_path , pdf_file_path=&quot;&quot;):<br />    '''extract text from image'''<br />    segments , text_boxes = textbox_recognition(file_path)<br />    column_lines = detect_column_lines(segments)<br /><br />    # if single column<br />    if len(column_lines) &#60;; 3:<br />        return &quot;&quot;<br /><br />    # align text boxes by column<br />    # text boxes within columns<br />    ordered_text_box = []<br />    for i in range(len(column_lines)):<br />        prev_line = column_lines[i-1] if ((i-1) &#62;;= 0) else 0<br />        boxes = get_boxes_for_line(<br />            text_boxes , column_lines[i] , ordered_text_box , prev_line)<br />        for b in boxes:<br />            ordered_text_box.append(b)<br /><br />    # boxes that are not in any column<br />    # text boxes not in any column<br />    non_selected_boxes = []<br />    for i in text_boxes:<br />        if i not in ordered_text_box:<br />            non_selected_boxes.append(i)<br /><br />    for i in non_selected_boxes:<br />        y = i[1]<br />        if y &#60;;= ordered_text_box[0][1]:<br />            ordered_text_box.insert(0 , i)<br />        else:<br />            ordered_text_box.append(i)<br /><br />    img = cv2.imread(file_path , cv2.IMREAD_GRAYSCALE)<br />    ret , thresh = cv2.threshold(img , 225 , 255 , 0)<br />    img_shape = img.shape<br /><br />    pdf_shape = (0 , 0)<br />    page = None<br />    if pdf_file_path != &quot;&quot;:<br />        doc = fitz.open(pdf_file_path)<br />        page = doc[0]<br />        pdf_shape = (page.rect.width , page.rect.height)<br /><br />    resume_text = &quot;&quot;<br />    for i in ordered_text_box:<br />        if pdf_file_path != &quot;&quot;:<br />            txt = clean_text(get_text_from_pdf(i , img_shape , pdf_shape , page))<br />        else:<br />            txt = clean_text(get_text(thresh , i))<br />        resume_text += txt + &quot;&quot;<br /><br />    # clean text<br />    txt = resume_text.split(&quot;&quot;)<br /><br />    res = []<br />    for line in txt:<br />        if len(line) == 0:<br />            continue<br />        res.append(line)<br /><br />    resume_text = '  '.join(res)<br />    return resume_text<br /><br /><br /><br />Dockerizing the Application<br /><br />To make deploying the application easy we will be Dockerizing the Application.<br /><br />Dockerfile<br /># syntax=docker/dockerfile:1<br /><br />FROM python:3.9-buster<br /><br />WORKDIR /resume-parser-docker<br /><br />RUN mkdir input_files<br />RUN pip3 install --upgrade pip<br /><br />COPY requirements.txt requirements.txt<br />RUN pip3 install -r requirements.txt<br /><br /># download nltk required<br />RUN python -m nltk.downloader punkt<br />RUN python -m nltk.downloader averaged_perceptron_tagger<br />RUN python -m nltk.downloader maxent_ne_chunker<br />RUN python -m nltk.downloader words<br /><br />RUN apt-get update \<br />  &amp;amp;&amp;amp; apt-get -y install tesseract-ocr<br /><br />RUN apt-get update &amp;amp;&amp;amp; apt-get install ffmpeg libsm6 libxext6  -y<br /><br />COPY . .<br /><br />EXPOSE 5000/tcp<br /><br />CMD [ &quot;python3&quot; , &quot;-u&quot;  , &quot;main.py&quot;]<br /><br /><br />Then run following commands to create image and run it.<br /><br />  Build Image<br />    docker build --tag jhamadhav/resume-parser-docker .<br /><br />  <br />  Run Image at port 5000<br />    docker run -d -p 5000:5000 jhamadhav/resume-parser-docker<br /><br />  <br />  Check images<br />    docker ps<br /><br />  <br />  Stop once done<br />    docker stop jhamadhav/resume-parser-docker<br /><br />  <br /><br /><br />Hosting on AWS<br /><br />Now that we have a docker image of our application.<br /><br />We can publish it to dockerHub:<br />docker push jhamadhav/resume-parser-docker<br /><br /><br />Then login to your EC2 instance and pull the image:<br />docker pull jhamadhav/resume-parser-docker<br /><br /><br />Run the image:<br />docker run -d -p 5000:5000 jhamadhav/resume-parser-docker<br /><br /><br /><br />  🎉🎉🎉 We have a fully functional Resume parser ready.<br /><br /><br />Future Work<br /><br />We can make use of Large Language Models (LLM) , train on datasets and fine tune LLM model to make extraction of below fields more accurate:<br /><br />  School/Institute name<br />  Degree<br />  Company name<br />  Role in a job<br /><br /><br />Conclusion<br /><br /><br />  In conclusion , resume parsing using NLP techniques offers a streamlined approach to extract crucial information from resumes , enhancing the efficiency and accuracy of candidate screening.<br />  By leveraging OCR , named entity recognition , and line sweep algorithms , we can handle various resume formats , including multicolumn layouts.<br />  The power of NLP automates the parsing process , empowering recruiters to efficiently process resumes and make informed hiring decisions.<br />  Embracing resume parsing techniques ensures fair and objective evaluation of applicants , leading to successful recruitment outcomes.<br />  With this skillset , you can revolutionize resume processing and contribute to more efficient hiring practices.<br /><br /><br />If you have any questions , doubts , or just want to say hi , feel free to reach out to me at contact@jhamadhav.com ! I’m always ready to chat about this cool project and help you out. Don’t be shy , drop me a line and let’s geek out together!<br />"
		} ,
	
		{
		  "title" : "Debugging &amp; Fixing mysql deadlock issue",
		  "category" : "technology",
		  "url" : "/technology/debugging-and-fixing-mysql-deadlock-issue/",
		  "date" : "2023-06-12 00:00:00 +0530",
		  "content"	: "Recently , during one of our tests , we encountered a deadlock issue that was reported by Sentry. The deadlock occurred while attempting to insert scores into a table after completing a candidate’s test. We were initially unsure about the cause of this deadlock. Upon investigation , we discovered that it was due to the interplay of various locks in our MySQL database. In this blog post , we will deep dive into the nature of these locks , understand their impact on transactions , and present the solutions we implemented to mitigate deadlock occurrences.<br /><br />Understanding deadlocks<br />To understand the deadlock situation , let’s familiarize ourselves with the different types of locks involved , as defined by the official MySQL documentation:<br /><br />GAP Lock:<br /><br />A gap lock is a lock on a gap between index records , or a lock on the gap before the first or after the last index record. A gap might span a single index value , multiple index values , or even be empty.<br /><br />If id is not indexed or has a nonunique index , the statement does lock the preceding gap.<br /><br />Next Key Lock:<br /><br />A next-key lock is a combination of a record lock on the index record and a gap lock on the gap before the index record. in simple words If one session has a shared or exclusive lock on record R in an index , another session cannot insert a new index record in the gap immediately before R in the index order.<br /><br />Insert Intention Lock:<br /><br />An insert intention lock is a type of gap lock set by INSERT operations prior to row insertion. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap.<br /><br />Problem Scenario<br />In our case , we have two tables , table1 and table2 , with a has_many relationship. All operations are performed on table2 , which has an index on table1 as a foreign key.<br /><br />Transaction A<br /><br />BEGIN;<br />DELETE FROM table2 WHERE table2.table1_id=127;<br />Query OK , 1 row affected (0.00 sec)<br /><br />Resulting data locks<br /><br />mysql&#62;; SELECT INDEX_NAME , LOCK_TYPE ,LOCK_DATA ,LOCK_MODE ,LOCK_STATUS , EVENT_ID FROM performance_schema.data_locks;<br />+-----------------------------------------+-----------+-----------+---------------+-------------+----------+<br />| INDEX_NAME                | LOCK_TYPE | LOCK_DATA | LOCK_MODE     | LOCK_STATUS | EVENT_ID |<br />+-----------------------------------------+-----------+-----------+---------------+-------------+----------+<br />| NULL                      | TABLE     | NULL      | IX            | GRANTED     |      408 |<br />| index_table2_on_table1_id | RECORD    | 127 , 92   | X             | GRANTED     |      408 |<br />| PRIMARY                   | RECORD    | 92        | X ,REC_NOT_GAP | GRANTED     |      408 |<br />| index_table2_on_table1_id | RECORD    | 128 , 93   | X ,GAP         | GRANTED     |      408 |<br />+-----------------------------------------+-----------+-----------+---------------+-------------+----------+<br />4 rows in set (0.00 sec)<br /><br />This query acquires a gap lock on table2 and an insert intention lock on table1_id values 126 and 127.<br /><br />Transaction B<br /><br />BEGIN;<br />INSERT INTO table2(table1_id) VALUES(126);<br />ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction<br /><br />Resulting data locks<br /><br />mysql&#62;; SELECT INDEX_NAME ,LOCK_TYPE ,LOCK_DATA ,LOCK_MODE ,LOCK_STATUS , EVENT_ID FROM performance_schema.data_locks;<br />+-----------------------------------------+-----------+-----------+------------------------+-------------+----------+<br />| INDEX_NAME                  | LOCK_TYPE | LOCK_DATA | LOCK_MODE              | LOCK_STATUS | EVENT_ID |<br />+-----------------------------------------+-----------+-----------+------------------------+-------------+----------+<br />| NULL                        | TABLE     | NULL      | IX                     | GRANTED     |      351 |<br />| index_table2_on_table1_id   | RECORD    | 127 , 92   | X ,GAP ,INSERT_INTENTION | WAITING     |      351 |<br />| NULL                        | TABLE     | NULL      | IX                     | GRANTED     |      408 |<br />| index_table2_on_table1_id   | RECORD    | 127 , 92   | X                      | GRANTED     |      408 |<br />| PRIMARY                     | RECORD    | 92        | X ,REC_NOT_GAP          | GRANTED     |      408 |<br />| index_table2_on_table1_id   | RECORD    | 128 , 93   | X ,GAP                  | GRANTED     |      408 |<br />+-----------------------------------------+-----------+-----------+------------------------+-------------+----------+<br />6 rows in set (0.01 sec)<br /><br />As Transaction A holds the lock on table1_id 126 due to the gap lock , Transaction B waits for the lock. However , it eventually times out , resulting in a lock wait timeout error.<br /><br />To create a deadlock , one must perform a delete query in Transaction B. Then , when attempting to insert a record in Transaction A , a deadlock error is thrown , with Transaction B becoming the victim. This deadlock situation arises due to the conflicts in the next-key lock , preventing Transaction B from inserting the record.<br /><br />In a nutshell<br />Lets understood the above queries in nutshell to create a deadlock.<br /><br />  Transaction A -&#62;; BEGIN;<br />  Transaction A -&#62;; DELETE records on table2 with table1_id=x.<br />  Transaction B -&#62;; BEGIN;<br />  Transaction B -&#62;; DELETE record on table2 with table1_id=y;<br />  Transaction B -&#62;; INSERT a record on table2 and table1_id is x-1.<br />  Transaction A -&#62;; INSERT a record on table2 and table1_id is y-1.<br />  A deadlock occurs , with Transaction A being the victim.<br /><br /><br />Practical example of GAP lock &amp;amp; Next Key Lock.<br />Gap lock is basically on range of values &amp;amp; will be aquired on a range if we try to delete a record which does not exist.<br /><br />table1<br /><br />+----+<br />| id |<br />+----+<br />| 73 |<br />| 74 |<br />| 81 |<br />| 82 |<br />+----+<br /><br />table2<br /><br />+-----+-----------+<br />| id  | table1_id |<br />+-----+-----------+<br />| 1   | 73        | <br />| 2   | 82        |<br />+-----+-----------+<br /><br />Transaction A<br /><br />BEGIN;<br />DELETE from table2 where table1_id=75;<br />Query OK , 0 rows affected (0.00 sec)<br /><br />This transaction will aquire a gap lock on range from 74-80.<br />this means if we try to insert new values in table2(in another session) with table1_id ranging from 74-80 it will wait until delete transaction commits.<br /><br />Other issues<br />In addition to addressing the deadlock issues caused by gap locks , we also encountered problems related to AASM records. We were using the AASM gem , a library that manages state transitions. In our case , this library was responsible for changing the state of the test to “completed” and executing several callback functions. These operations were performed as part of a single transaction , which sometimes resulted in prolonged transaction durations and increased the likelihood of deadlocks.<br /><br />Model dummy code<br /><br />aasm do<br />  state :active , initial: true<br />  state :complete<br />  event :complete , after: [:method1 , :method2 , :method3] do<br />      transitions from: :active , to: :complete<br />  end<br />end<br /><br />When the test is marked as complete and the state changes , all the MySQL-related queries are executed as part of a single transaction.<br /><br />Due to the execution of all these methods within a single transaction , there were instances where the transaction took a considerable amount of time to complete. These prolonged transactions duration increased the risk of deadlocks occurrence and also resulted in issues related to lock wait time.<br /><br />FIX<br /><br /><br />  To fix this we moved the insertion of records as a separate transaction out of the aasm state change.<br />  Optimized transaction size: We optimized the other badly written queries in the transaction.<br />  Reduced transaction duration: Only limited number of queries were part of the state change transaction (to keep the transaction short).<br />  We further optimized the GAP lock by avoiding unnecessary delete queries when the records were not present in the table with the corresponding ID.<br /><br /><br />References<br /><br />  Innodb Gap Lock<br />  Innodb Next Key Lock<br />  Innodb Insert Intention Lock<br />  Gap lock with example medium article<br />  Gap lock article by percona<br /><br />"
		} ,
	
		{
		  "title" : "Website Monitor Using Google App Script",
		  "category" : "technology",
		  "url" : "/technology/website-monitor-using-google-app-script/",
		  "date" : "2022-12-30 00:00:00 +0530",
		  "content"	: "Recently , I was looking for a solution to notify me when a website is down and when it is back up. I found a few solutions , but they all had a learning curve. So I thought of an alternative solution using Google App Script , which I had recently learned about.<br /><br />Requirements<br /><br />  Can run every 5 minutes.<br />  Can send emails when the website is down.<br />  Trustworthy.<br /><br /><br />I wasn’t sure if the first requirement was possible with Google App Script , but the other two were. After reading the documentation , I found that it was possible to create a time-based trigger for a script.<br /><br />Steps to follow:<br /><br />  Create a new Google App Script project.<br />  Create a function to track the website. Here is an example:<br /><br /><br />function myFunction() {<br />   const initialUrls = [<br />     { uri: &amp;#39;https://mock.codes/200&amp;#39; , status: &amp;#39;&amp;#39;} ,<br />     { uri: &amp;#39;https://mock.codes/500&amp;#39; , status: &amp;#39;&amp;#39;}<br />   ];<br /> <br /> const properties = PropertiesService.getScriptProperties();<br /> let urls =  JSON.parse(properties.getProperty(&amp;#39;URL_LIST&amp;#39;)) || initialUrls;<br /> const errorResponseCodes = [500 , 502 , 503 , 504];<br /> const alertEmail = &amp;#39;alertmail@gmail.com&amp;#39;;<br /> <br /> const options = { muteHttpExceptions: true };<br /> <br /> urls.forEach((url) =&#62;; {<br />   let responseCode = UrlFetchApp.fetch(url.uri , options).getResponseCode();<br /> <br />   const isErrorResponse = errorResponseCodes.includes(responseCode);<br />   const wasPreviouslyDown = url.status === &amp;#39;down&amp;#39;;<br /><br />   if (isErrorResponse &amp;amp;&amp;amp; !wasPreviouslyDown) {<br />     // Site is now down for the first time<br />     const subject = `Alert: Your site ${url.uri} is currently down`;<br />     const body = `${url.uri} has encountered an error with status code ${responseCode}`;<br />     MailApp.sendEmail(alertEmail , subject , body);<br />     url.status = &amp;#39;down&amp;#39;;<br />   } else if (!isErrorResponse &amp;amp;&amp;amp; wasPreviouslyDown) {<br />     // Site was previously down , but is now back up<br />     const subject = `Your site ${url.uri} is now back up`;<br />     const body = `${url.uri} has recovered and is now back up`;<br />     MailApp.sendEmail(alertEmail , subject , body);<br />     url.status = &amp;#39;&amp;#39;;<br />   }<br /> });<br /><br /> properties.setProperty(&amp;#39;URL_LIST&amp;#39; , JSON.stringify(urls));<br />}<br /><br /><br />  Go to the “Triggers” menu in the left sidebar of the Google App Script project.<br />  Click the “Add Trigger” button and select the function to run.<br />  Choose the options to run the trigger every 5 minutes and click “Save”<br /><br /><br />Explanation<br /><br />This above code uses the UrlFetchApp service to make HTTP requests to the websites and check their status. it stores the value of each trigger in a variable so that whenver site goes live again it can send email of website backed up.<br /><br /><br />  You can also check the logs for each trigger execution in the “Execution” menu on the left side of the project.<br /><br /><br /><br /><br />Conclusion<br /><br />In conclusion , Google App Script is a useful tool for creating a customized website tracker that can notify the user when a website is down. The process of setting up the tracker is straightforward and the logs can be easily accessed to track the execution of the function. this basic functionality can be enhanced more to record the status in a csv file. also interesting graphs and charts can be made using that data.<br /><br />Additional investigations<br /><br />Upptime is one of the good open-source alternative which can be used to monitor a website. it uses github actions to make sure the website is up and creates a issue if website is down for some reason. it also logs the information about the website speed.<br />"
		} ,
	
		{
		  "title" : "The revamp of a Video Proctoring Solution: A Behind-the-Scenes Look",
		  "category" : "technology",
		  "url" : "/technology/the-revamp-of-a-video-proctoring-solution-a-behind-the-scenes-look/",
		  "date" : "2022-12-27 00:00:00 +0530",
		  "content"	: "The story of how we took a good platform and made it even better<br /><br /><br /><br />Background<br /><br />For the past few months , the number of test takers and clients at eLitmus has increased significantly. Conducting all of these tests remotely poses a significant challenge in terms of preventing cheating. To address this issue , eLitmus has developed an in-house solution using the open-source Kurento media server. While this solution has been effective in terms of recording videos , it is not horizontally scalable.<br /><br />In search of a more effective solution , eLitmus turned to Amazon Kinesis and worked with the AWS team to conduct a proof-of-concept. While this approach allowed for live proctoring , it was not possible to record the exams.<br /><br />How did it get begin?<br /><br />As I was learning about WebRTC and Amazon Kinesis during this time , I had the opportunity to attend a session by a company called 100ms. This company is focused on solving problems related to live conferencing , and I was eager to learn more about their approach.<br /><br />After connecting with the co-founder of 100ms , I received a message from their salesperson to schedule a demo call. During the call , we determined that 100ms could be a potential solution for eLitmus’ scalability problem. However , we needed to weigh the costs of maintaining engineering time and effort to maintain the solution against the opportunity cost of using that time to build a new product , as well as overall server and bandwidth costs.<br /><br />Based on this analysis , we decided to proceed with a proof-of-concept for live remote proctoring. I spent the next week working on the proof-of-concept and was able to complete it successfully. From there , we saw potential synergies between 100ms and eLitmus and decided to make the product(Knights Watch) an open-source platform.<br /><br />Designing &amp;amp; Developing<br /><br />I created a document outlining the requirements for the video proctoring solution , including features such as a proctoring dashboard , candidate tests screen , cheating analysis and verification dashboard , admin dashboard , and auto proctoring. For the first version (v0.1) , we planned to roll out the proctoring dashboard with multiple streams visible to the proctor , storage of the video stream on an s3 server , retrieval of the video stream in the cheating analysis and verification dashboard , and admin configuration.<br /><br />After outlining the requirements for the video proctoring solution , I designed the architecture for the solution , diagrammatically representing how all of the components would be connected. The main components of the app were the 100ms server API , the eLitmus server , and the candidate or proctor’s browser.<br /><br />Next , I created a milestone on Github and listed out the issues that needed to be addressed , including the integration of the proctor dashboard , candidate test screen , algorithm for assigning candidates and proctors to rooms , and storage of videos on the eLitmus prescribed directory structure on an s3 server.<br /><br /><br /><br />I began working on these issues and was able to roll out the v0.1 of the proctoring solution within a few weeks. During this time , our team encountered various challenges and suggested various features to 100ms.<br /><br />Challenges Faced<br /><br />As we worked on storing videos on an AWS s3 server in our prescribed directory structure , we encountered a challenge with the 100ms API. The webhook provided by 100ms was only for the composite recording of the room , not for individual recordings. However , we needed webhooks to notify us of the success of each individual recording. In addition , 100ms had the functionality for only a single webhook per account , but we needed to support multiple environments with multiple applications within a single account. We requested this feature from 100ms.<br /><br />While working on an algorithm to assign candidates and proctors to rooms , I faced the challenge of storing authentication tokens in the user’s browser and in Redis storage in production. I wrote an algorithm to handle the expiration of tokens from both ends and to handle multiple events.<br /><br />As we configured 100ms for various environments including staging , production , and edge , we encountered several issues and suggested various features to 100ms. These included the ability to delete apps and templates from the 100ms dashboard from the front-end , team management options in the dashboard , and handling of access keys and secrets for multiple environments.<br /><br />Testing live remote proctoring solution<br /><br />After completing the first version (v0.1) of the video proctoring solution , we were ready to test it in production. eLitmus was conducting an internal hiring event at the time , and we used the live video proctoring feature for this event with around 400 candidates. The event went smoothly , with minor issues. The proctor was able to hear the voices of the candidates and all of the videos were recorded throughout the session.<br /><br />This success gave us confidence in the solution , and we made some minor tweaks. However , our main concern from the start had been scalability , and we wanted to test the solution at a larger scale. We had an in-person test at IITK with over 600 candidates , and decided to conduct the event with live proctoring. The event went smoothly , but the next day we conducted data analysis and discovered that 14 out of 600+ videos had some data loss or were not recorded.<br /><br />We had a meeting with 100ms to discuss this issue , and after working with their engineering team , we determined that the issue was caused by network connectivity problems. We fixed the issue and the proctoring solution became more stable , with 97% of the videos being recorded.<br /><br />Open-sourcing video proctoring solution<br /><br />After this event , we had discussions with 100ms about pricing and suggested various features , including pricing on the 100ms dashboard itself and the option to opt-in or opt-out of composite recording and browser-based recording.<br /><br />After making the video proctoring solution an open-source project , I focused on documenting the project so that it could be used by others in the community and more developers could contribute to it. I wrote several documents , including a readme file , information on the architecture and prerequisites , installation guidelines , development guidelines , deployment guidelines , a code of conduct , and guidelines for contributing and welcoming new contributors.<br /><br />Conclusion<br /><br />In conclusion , the development of the video proctoring solution at eLitmus was a challenging but rewarding process. By identifying a need to solve the problem of vertical scalability , we were able to explore various solutions and ultimately choose 100ms as a partner to help us build a scalable and effective video proctoring platform. Through the development process , we encountered various challenges and were able to work closely with the 100ms team to find solutions and improve the stability of the platform. We are proud to have made the video proctoring solution an open-source project and to have contributed to the community by documenting the project and welcoming new contributors. We hope that others will find this project useful and will be able to build upon it to create even better solutions in the future.<br />"
		} ,
	
		{
		  "title" : "Fixing Capybara Flaky Tests",
		  "category" : "technology",
		  "url" : "/technology/fixing-capybara-flaky-tests/",
		  "date" : "2022-12-20 00:00:00 +0530",
		  "content"	: "When writing system tests for a user interface , it is common to encounter test cases that fail randomly. One of the common failure can occur when the JavaScript on a page takes time to render , causing issues with the test case.<br /><br />For example , imagine a test case that clicks a button on a page and then checks for the presence of certain content after the click.<br /><br />Demo Code<br /><br />visit submit_page<br />click_on &amp;#39;Submit&amp;#39;<br />assert page.has_content &amp;#39;Some content after clicking on submit&amp;#39;<br /><br />In most cases , this test will run without any issues. However , occasionally the test may fail on the third line with the error “Expected false to be truthy”. This error can occur when the page is visited and the JavaScript on the page takes a few seconds to load. During this time , the submit button may be clicked , but because there is no JavaScript associated with the button yet , the button click does not do anything. As a result , the test is still on the submit page when it tries to assert that the expected content is present , causing the test to fail.<br /><br />Solution<br /><br />One solution to this problem is to increase the wait_time setting in capybara. However , this approach has several limitations. First , the wait_time setting is global and applies to all test cases , so if it is set to a high value , it will increase the overall execution time of the test suite. Additionally , the wait_time setting only waits for a fixed amount of time before moving on with the test , without checking whether the page has finished loading. This means that if the page takes longer to load than the wait_time<br /><br />The other solution is to use the execute_script method provided by Capybara to click the button instead of the click_on method. The execute_script method allows you to execute JavaScript code within the context of the current page. By using this method to click the button , the click action is added to the end of the browser’s call stack. This means that the click action will be executed after any existing JavaScript code on the page has finished running , ensuring that the button is fully initialized and ready to be interacted with before the test tries to click it.<br /><br />To use the execute_script method to click the button , you can use the following code:<br /><br />page.find_button(&amp;#39;Submit&amp;#39;).execute_script(&amp;#39;this.click()&amp;#39;)<br /><br />This way we can ensure that click method will run only after the page javascript is fully loaded.<br /><br />Browser Call Stack<br /><br />|               |<br />          |               |<br />          |   JavaScript  |  &#60;;-- existing code on the page(1)<br />          |_______________|<br />          |               |<br />          |   JavaScript  |  &#60;;-- existing code on the page(2)<br />          |_______________|<br />          |               |<br />          |  click action |  &#60;;-- added by execute_script method(3)<br />          |_______________|<br /><br />"
		} ,
	
		{
		  "title" : "Sidekiq process in production with Systemd and Monit",
		  "category" : "technology",
		  "url" : "/technology/sidekiq-process-in-production-with-systemd-and-monit/",
		  "date" : "2022-06-29 00:00:00 +0530",
		  "content"	: "Recently , we have upgraded our Sidekiq version from 5.2 to 6.5. Before Sidekiq 6.0 we were managing the Sidekiq process directly using Monit. With the release of Sidekiq 6 , the team has removed the daemonization , logfile , and pidfile command line arguments and sidekiqctl binary.  Managing services manually is more error-prone , let our operating system do it for us.<br />We have three options to go with systemd , upstart , and foreman. We decided to go ahead with the systemd.<br /><br />Systemd<br />Systemd is a system and service manager for linux. Systemd tasks are organized as units. Most common units are services(.service) , mount points(.mount) , devices(.device) , sockets(.socket) , or timers(.timer)<br /><br />Systemctl<br />The systemctl command is a utility which is responsible for examining and controlling the systemd system and service manager.<br /><br />Sidekiq<br />Simple , efficient background processing for Ruby.<br /><br />Sidekiq running as Systemd Service<br /><br /><br /><br />To manage Sidekiq we need to create a service file for Sidekiq which can be used to start , stop or restart the Sidekiq process.<br /><br />Sudo nano /lib/systemd/system/sidekiq.service<br /><br /><br /><br /><br />Content in the Sidekiq.service. Sidekiq has provided us with the template for the service file here Sidekiq.service. We modified it according to our use case<br /><br />[Unit]<br />Description=sidekiq<br />After=syslog.target network.target<br /><br />[Service]<br /><br />Type=simple<br /># If your Sidekiq process locks up , systemd&amp;#39;s watchdog will restart it within seconds.<br />#WatchdogSec=10<br /><br />WorkingDirectory=/opt/myapp/current<br /><br />ExecStart=/usr/local/bin/bundle exec sidekiq -C /opt/myapp/shared/config/sidekiq.yml -e production<br />ExecStop=/bin/kill -TSTP $MAINPID<br />ExecStartPost=/bin/sh -c &amp;#39;/bin/echo $MAINPID &#62;; /opt/myapp/shared/pids/sidekiq.pid&amp;#39;<br />ExecStopPost=/bin/sh -c &amp;#39;rm /opt/myapp/shared/pids/sidekiq.pid&amp;#39;<br /><br />User=deploy<br />Group=deploy<br />UMask=0002<br /><br /># Greatly reduce Ruby memory fragmentation and heap usage<br /># https://www.mikeperham.com/2018/04/25/taming-rails-memory-bloat/<br />Environment=MALLOC_ARENA_MAX=2<br /><br /># if we crash , restart<br />RestartSec=10<br />Restart=on-failure<br /><br /># output goes to /var/log/syslog (Ubuntu) or /var/log/messages (CentOS)<br />StandardOutput=syslog<br />StandardError=syslog<br /><br /># This will default to &ldquo;bundler&ldquo; if we don&amp;#39;t specify it<br />SyslogIdentifier=sidekiq<br /><br />[Install]<br />WantedBy=multi-user.target<br /><br /><br /><br /><br />  Our Modified Configurations:<br />  <br />    <br />      As we were using system ruby and using Sidekiq with some custom configurations. To start Sidekiq we used.<br /><br /><br />ExecStart=/usr/local/bin/bundle exec sidekiq -C /opt/myapp/shared/config/sidekiq.yml -e production<br /><br />    <br /><br />    <br />    To stop Sidekiq we need to send a TSTP signal to process all the busy jobs before terminating Sidekiq.<br /><br /><br />ExecStop=/bin/kill -TSTP $MAINPID<br /><br />    <br />    <br />    For Managing with Monit we need the process id , After starting or stopping the service we were maintaining the process id file.<br /><br /><br />ExecStartPost=/bin/sh -c &amp;#39;/bin/echo $MAINPID &#62;; /opt/myapp/shared/pids/sidekiq.pid&amp;#39;<br />ExecStopPost=/bin/sh -c &amp;#39;rm /opt/myapp/shared/pids/sidekiq.pid&amp;#39;<br /><br />    <br />    <br />    As we want to use our app user to run this service.<br /><br /><br />User=deploy<br />Group=deploy<br />UMask=0002<br /><br />    <br /><br />    <br />    And we want to restart only when there is a failure.<br /><br /><br /><br /># if we crash , restart<br />RestartSec=10<br />Restart=on-failure<br /><br />    <br />  <br /><br /><br />Reload the systemctl daemon for the new created service<br /><br />Sudo systemctl daemon-reload<br /><br /> <br /><br /><br /> Now we can start the Sidekiq service:<br /><br /><br />sudo systemctl start|stop|restart sidekiq<br /><br /><br /><br /><br />Monitor Sidekiq process with Monit<br /><br />Now we have systemd to start , stop and restart the Sidekiq process. Now we will look at how to monitor the Sidekiq process with the help of monit.<br /><br />Monit<br /><br />Monit is a utility for managing and monitoring processes , programs , files , directories and filesystems on a Unix system.<br /><br /><br /><br />Modified monitrc<br /><br />check process sidekiq with pidfile &ldquo;/opt/myapp/shared/pids/sidekiq.pid&ldquo;<br />  start program = &ldquo;/bin/bash -l -c  &amp;#39;sudo systemctl start sidekiq&amp;#39; as uid deploy and gid deploy&ldquo;<br />    with timeout 20 seconds<br />  stop program  = &ldquo;/bin/bash -l -c  &amp;#39;sudo systemctl stop sidekiq&amp;#39; as uid deploy and gid deploy&ldquo;<br />    with timeout 20 seconds<br />  if totalmem is greater than 800 MB for 3 cycles then restart<br />  if changed pid then exec &ldquo;/etc/monit/slack_notifier.sh&ldquo;<br />  if cpu is greater than 65% for 2 cycles then exec &ldquo;/etc/monit/slack_notifier.sh&ldquo; else if succeeded then exec &ldquo;/etc/monit/slack_notifier.sh&ldquo;<br /><br /><br /><br />We can check if sidekiq is up and running:<br /><br /><br />sudo monit summary sidekiq<br /><br /><br /><br /><br />Monit will check the Sidekiq process and it will automatically start in case of the unexpected kill of the Sidekiq process.<br /><br />We have successfully completed the Sidekiq process monitoring with the help of Monit and Systemd.<br /><br />"
		} ,
	
		{
		  "title" : "Outliers: The story of Success - Book Review",
		  "category" : "the other side",
		  "url" : "/the-other-side/outliers-the-story-of-success-book-review/",
		  "date" : "2022-05-02 00:00:00 +0530",
		  "content"	: "For the last two months , I have been reading the book “Outliers” by Malcolm Gladwell. “Outliers - The story of success” has two parts: Opportunity and Legacy.<br /><br />“In outliers author survey the ingredients for the success. He wrote about the reason behind the success of great people like Bill Gates , Bill Joy , Joseph Flom , and the musical group Beatles. And how Chris Langan and Oppenheimer ended up with different stories. And how the culture , family , and friends play a role in determining individual success.”<br /><br />Key Factor of Success:<br /><br />1. Opportunity<br /><br />We all have equal opportunities. Some people recognize and take advantage of them. But as per the author , In reality , these people are benefited from hidden and extraordinary opportunities. Culture benefits , the where and in which family and the time we grew up. Values received from our ancestors.<br /><br />2. Environment affect<br /><br />The values of the world we live in and the people around us , have a profound effect on who we are. The place we live and the people we spend time with are critical factors to success. The author explains the Roseto Mystery. Why do the people of Roseto have good health and rare heart attacks? How community benefits play a dominant role in Roseto Mystery.<br /><br />3. Hard work<br /><br />It takes 10000 hours to master anything from beginner to world-class expert in any field. “10000” hours is a lot , so it’s a good advantage for me to start at a young age.<br /><br />The author explains how Bill Joy from age of 16 started with computers. And from that day in university , when introduced to the computer , the place became his life , and he programmed whenever he gets time. He wrote the UNIX and the program for TCP/IP , which allows us to connect to the Internet. In his early days he devoted his 10 ,000 hours with passion and abilities.<br /><br />How Beatles were invited to play in a club where they had to play for a lot of hours , even the whole night. By the time the Beatles reached success , they played almost 12000 times and for more than 10 ,000 hours.<br /><br />4. Legacy often drives our behavior<br /><br />Gladwell points out how values passed from generation to generation. He points out the cultural legacy of Asian countries where rice is the dominant crop. How the success of rice paddy depends on the amount of hard work and diligence; we put in. To have a successful paddy , wake up at dawn and work all day. Which then creates a cultural legacy of hard work.<br />"
		} ,
	
		{
		  "title" : "Creating an npm package from my REACT Component",
		  "category" : "technology",
		  "url" : "/technology/creating-an-npm-package-from-my-react-component/",
		  "date" : "2021-05-25 00:00:00 +0530",
		  "content"	: "So , you have created a useful , customisable , modular component in REACT. Now , you want to share it with everyone by making a package so that anyone can install it ? That is exactly what I had done and now I also wanted to create a npm package and publish it and this is how I did it.<br /><br />Prerequisites<br /><br />Since I was going to create a npm package , I needed to have Node and npm installed on my system.<br /><br />Also I needed a npm account. I didn’t have one so I had to create one before I got started. You can also create one from here.<br /><br />Getting Started<br /><br />First order of business was to select an unique name for my package. I settled on react-rails-pagination as the name for my package.<br /><br />To confirm that no package with the same name existed I had to use the following command.<br /><br />npm search react-rails-pagination<br /><br />You can use<br /><br />npm search &#60;;your-package-name&#62;;<br /><br />And if no existing package is found with the same name , then you are good to go.<br /><br />After I selected a package name , I had to run the following command in my terminal to initialise the package.<br /><br />npx create-react-library react-rails-pagination<br /><br />I was prompted to answer a few questions about my package now<br /><br /><br /><br />After entering all the information , it will automatically setup the project. This process might take a little time.<br /><br />The advantage of using create-react-library is that it will initialise your project to be published along with an example where you can test your package. It will also initialise it as a local git repository which you can simply push to github after adding the URL for your remote repository.<br /><br />After create-react-library finishes , the folder structure looks like this<br /><br /><br /><br />I had to run the following commands in two different terminal tabs to start the development environment<br /><br />cd react-rails-pagination &amp;amp;&amp;amp; npm start<br /><br />cd react-rails-pagination/example &amp;amp;&amp;amp; npm start<br /><br />The first command watches the src/ and recompiles it into the dist/ folder when you make changes.<br /><br />The second command runs the example app that links to your package.<br /><br />Adding my REACT Component<br /><br />Now , I had a look inside the src/ folder in my project. There was an index.js file which held an ExampleComponent that was being used in the example app.<br /><br />To add my own REACT Component , I placed my Pagination.jsx file that held my Pagination component inside the src/ folder. Since , my component required a css file too , I placed my css file index.css inside the same folder as well. I import this index.css file inside my Pagination component.<br /><br />I don’t use a separate css module in my component so I deleted the generated styles.modules.css file inside the src directory.<br /><br />After I had done these changes , my src directory looked something like this<br /><br /><br /><br />Now , I need to make sure that my component is being exported from this package , so that any project that uses my package , will get to use my component as well.<br /><br />For this I have to make some changes to the index.js file.<br /><br />import Pagination from &amp;#39;./Pagination&amp;#39;;<br /><br />export default Pagination;<br /><br />This imports my component into the index.js file and sets it as the default export from the package. I do this because the source file or the entrypoint of my package is the src/index.js file.<br /><br />If you don’t want to use the index.js file or want to create a new entrypoint then open the package.json file in the root of the project and change the value of the source key in that file.<br /><br />This completes the process of adding my component to the package.<br /><br />Checking if my package is working as expected<br /><br />To check if my package is working or not , I have to go to the example/ folder.<br /><br />In that folder , I have to edit the App.js file which imports the ExampleComponent that I modified earlier to use my Pagination component.<br /><br />import React from &amp;#39;react&amp;#39;<br /><br />import Pagination from &amp;#39;react-rails-pagination&amp;#39;<br />import &amp;#39;react-rails-pagination/dist/index.css&amp;#39;<br /><br />const App = () =&#62;; {<br />  let page = 1;<br />  const totalPages = 5;<br /><br />  const handleChangePage = (currentPage) =&#62;; {<br />    page = currentPage;<br />  }<br />  return &#60;;Pagination page={page} pages={totalPages} handleChangePage={handleChangePage} /&#62;;<br />}<br /><br />export default App;<br /><br />These changes allow me to import my package into this example application and check if it is working or not.<br /><br />Now if I open the address that the local development server is running on in my browser , I can see that my component is loaded and functioning now.<br /><br /><br /><br /><br />Publishing my package<br /><br />I need to add a few things to get this package ready for publishing.<br /><br />First I add a .npmignore file to stop a few things from being included in my published package to reduce it’s size. It works the same as a .gitignore file but for npm in this case.<br /><br />The .npmignore looks like this in my project<br /><br />## the src folder<br />src<br />.babelrc<br />rollup.config.js<br />## node modules folder<br />node_modules<br />## git repository related files<br />.git<br />.gitignore<br />CVS<br />.svn<br />.hg<br />.lock-wscript<br />.wafpickle-N<br />.DS_Store<br />npm-debug.log<br />.npmrc<br />#others<br />config.gypi<br />package-lock.json<br /><br />Next I opened the package.json and added a few things in there as well.<br /><br />{<br />  &ldquo;name&ldquo;: &ldquo;react-rails-pagination&ldquo; ,<br />  &ldquo;version&ldquo;: &ldquo;1.0.0&ldquo; ,<br />  &ldquo;description&ldquo;: &ldquo;React Pagination Component for Rails and other MVC Frameworks&ldquo; ,<br />  &ldquo;license&ldquo;: &ldquo;MIT&ldquo; ,<br />  &ldquo;repository&ldquo;: &ldquo;piyushswain/react-rails-pagination&ldquo; ,<br />  &ldquo;main&ldquo;: &ldquo;dist/index.js&ldquo; ,<br />  &ldquo;module&ldquo;: &ldquo;dist/index.modern.js&ldquo; ,<br />  &ldquo;source&ldquo;: &ldquo;src/index.js&ldquo; ,<br />  &ldquo;engines&ldquo;: {<br />    &ldquo;node&ldquo;: &ldquo;&#62;;=10&ldquo;<br />  } ,<br />  &ldquo;keywords&ldquo;: [<br />    &ldquo;react&ldquo; ,<br />    &ldquo;rails&ldquo; ,<br />    &ldquo;mvc&ldquo; ,<br />    &ldquo;react-component&ldquo; ,<br />    &ldquo;pagination&ldquo;<br />  ] ,<br />  &ldquo;author&ldquo;: {<br />    &ldquo;name&ldquo;: &ldquo;Piyush Swain&ldquo; ,<br />    &ldquo;email&ldquo;: &ldquo;piyush.swain3@gmail.com&ldquo;<br />  } ,<br />  &ldquo;homepage&ldquo;: &ldquo;https://github.com/piyushswain/react-rails-pagination&ldquo; ,<br />  .<br />  .<br />  .<br />  .<br />}<br /><br />I updated the author field to add my email.<br /><br />Next , I added the keys homepage and keywords.<br /><br />homepage can be used to add a website link to your project. I used my github repository link for now but I will change it later when I add a demo to this project. If you have a working demo , you can add that link in it’s place.<br /><br />The keywords key can be used to give the npm search directory keywords to attach to your project so that people using the npm search engine can find your project more easily. It takes an array of words as an argument.<br /><br />Finally , I update the README.md file in the root directory to add a description and instructions for anyone using my package. You will have to update your README.md according to your package as well.<br /><br />I review all the changes and then first push my code to my github repository.<br /><br />git remote add origin https://github.com/piyushswain/react-rails-pagination.git # Sets the new remote for the local repo<br />git add .<br />git commit -m &amp;#39;Initial Commit&amp;#39;<br />git push -u origin main  # Pushes the changes to the remote repository<br /><br />Now , my package is ready to be published. I run the following commands to start the process of publishing my package to npm.<br /><br />npm login<br /><br />Login command asks for the username and password of your npm account. Enter those succeessfully and it will log you in to npm. If you have already logged in to npm , then you can skip this step.<br /><br />npm run build<br /><br />This optimizes and creates a production build for your package. I recommend running this everytime before you issue a publish command.<br /><br />npm publish<br /><br />Finally , running this command will upload your package to npm. You can check it in your npm profile where you can find all your uploaded packages.<br /><br />If you wish to publish again after making some changes then open your package.json file and update the version key to publish again. Remember to build your package before publishing as it will create an optimized production build for your package.<br /><br />TIP: If for some reason you cannot get the css to work , then a small hack is to directly update the dist/index.css file as this is the file that is published and used by anyone importing your package<br /><br /><br />  You can find this article on the author’s blog piyushswain.github.io as well.<br /><br /><br />"
		} ,
	
		{
		  "title" : "Migration from Paperclip to ActiveStorage",
		  "category" : "technology",
		  "url" : "/technology/migration-from-paperclip-to-activestorage/",
		  "date" : "2021-05-21 00:00:00 +0530",
		  "content"	: "How we migrated hundreds of thousands of attachments from Paperclip to ActiveStorage without downtime.<br /><br />At eLitmus , recently we migrated thousands of attachment records from Paperclip to Rails-owned ActiveStorage. Paperclip and Active Storage solve similar problems - uploading files to cloud storage like Amazon S3 , Google Cloud Storage , or Microsoft Azure Storage. In our case , we are uploading files to Amazon s3. And then attach those files to Active Records objects. So migrating from one to another is straightforward data-rewriting.<br /><br />Why do we migrate from paperclip to active storage?<br /><br />ActiveStorage was introduced in Rails version 5.2. At the time of migration , we were at Rails version 6.0. So , we were already running behind in keeping things up to date. Active storage is a highly recommended tool for uploading files. For a long , before ActiveStorage , this functionality was provided by outside gems , including Paperclip. With the release of Active storage , Paperclip was already deprecated for some time , and we wanted to move forward with Active Storage knowing it’s not as mature as Paperclip , but it’s owned by the rails’ community behind it. So we were happy with that.<br /><br />How do we migrate from paperclip to active storage?<br /><br />After reading articles on the web and the migration guide provided by the Paperclip process seemed pretty straightforward. We had around 2 Million records belonging to 16 different Active Records. In our case , we need migration that is fast and with no downtime. We had records in millions we cannot afford to wait for days to run migrations. We decided to do it in small steps. One step at a time , migrating all attachments of one Active Record. So a total of 32 Merge Requests were merged in production during this time. For each Active Record , two Merge Requests deployed because we didn’t want to have any unavailable attachments during the whole process , we split it into two steps or Merge Requests.<br /><br />So both steps revolve around the Paperclip and ActiveStorage. Let us refresh our understanding of how paperclip and active storage works. Paperclip works by attaching file data to the model. At the same time , it changes the schema of the model by introducing four columns in the Active Record table. It manages rails validations based on size and presence of file data if required.<br /><br />create_table &ldquo;users&ldquo; , force: :cascade do |t|<br />    t.string &ldquo;image_file_name&ldquo;<br />    t.string &ldquo;image_content_type&ldquo;<br />    t.integer &ldquo;image_file_size&ldquo;<br />    t.datetime &ldquo;image_updated_at&ldquo;<br />  end<br /><br />Here’s how it would go for a User with an image , that is this in Paperclip:<br /><br />class User &#60;; ApplicationRecord<br />    has_attached_file :image<br /><br />    validates_attachment :avatar , presence: true ,<br />      content_type: &ldquo;image/jpeg&ldquo; ,<br />      size: { in: 0..10.kilobytes }<br />  end<br /><br />On another side , we start by installing ActiveStorage. Normally , Rails 6.1 already comes with it , so all we need is run:<br /><br />rails active_storage:install<br /><br />ActiveStorage creates three database tables ActiveStorageBlobs table storing attachment metadata , the ActiveStorageAttachments table , which is a polymorphic table between the blobs table and rails model and the ActiveStorageVariantRecords table tracks the presence of variant in the database. ActiveStorage doesn’t come with validations. we found some outside gems , including active_storage_validations which works for us.<br /><br />create_table :active_storage_blobs do |t|<br />    t.string   :key ,      null: false<br />    t.string   :filename ,     null: false<br />    t.string   :content_type<br />    t.text     :metadata<br />    t.string   :service_name , null: false<br />    t.bigint   :byte_size ,    null: false<br />    t.string   :checksum ,     null: false<br />    t.datetime :created_at ,   null: false<br /><br />    t.index [ :key ] , unique: true<br />  end<br /><br />  create_table :active_storage_attachments do |t|<br />    t.string     :name ,     null: false<br />    t.references :record ,   null: false , polymorphic: true , index: false<br />    t.references :blob ,     null: false<br /><br />    t.datetime :created_at , null: false<br /><br />    t.index [ :record_type , :record_id , :name , :blob_id ] , name: &ldquo;index_active_storage_attachments_uniqueness&ldquo; , unique: true<br />    t.foreign_key :active_storage_blobs , column: :blob_id<br />  end<br /><br />  create_table :active_storage_variant_records do |t|<br />    t.belongs_to :blob , null: false , index: false<br />    t.string :variation_digest , null: false<br /><br />    t.index %i[ blob_id variation_digest ] , name: &ldquo;index_active_storage_variant_records_uniqueness&ldquo; , unique: true<br />    t.foreign_key :active_storage_blobs , column: :blob_id<br />  end<br /><br />Here’s how it would go for a User with an image , that is this in ActiveStorage:<br /><br />class User &#60;; ApplicationRecord<br />    has_one_attached :image<br /><br />    validates :image , attached: true , <br />      content_type: &amp;#39;image/png&amp;#39; ,<br />      size: { in: 0..10.kilobytes }<br />  end<br /><br />Let’s deep dive into the two steps we adopted , Migrated Paperclip data and Adopted ActiveStorage<br /><br />Migrated Paperclip Data<br /><br />In this step , we did the most crucial part of the process , running a rake job to migrate paperclip data to active storage tables. We kept everything from the Paperclip as it is and , we also added support for Active Storage. We were using both functionalities at the same time. During the time , attachments for the model were migrated from Paperclip to ActiveStorage if a user decides to upload any attachments , the user still uses the paperclip implementation , but in the background after the successful commit of all transaction related to Paperclip. We were duplicating the same attachment to active storage by using Active Record Callback after_commit.<br /><br />What does our rake task flow look like?<br /><br />In this step , we created a rake task that copies all the data produced by Paperclip to the new ActiveStorage format.<br /><br /><br />  Firstly , we pushed every column_name matching the Regex containing the file_name into the array. For example , we have a UserSignature model having a column image_file_name.<br />  Secondly , for each instance of the model , created an ActiveStorage record only if ActiveStorage doesn’t contain a record for that instance. The reason for this is that for some reason , we cancel our rake task or it gets crashes , we had a choice to restart it from the place where it left off.<br />  So for each instance , we were first constructing the direct URL of the attachment. Direct URL is the Amazon s3 URL to download the attachment from Amazon s3. We then pass on this direct URL to ActiveStorage::Blob create_and_upload! Method , which first downloads it and re-upload it to the s3 bucket. We then created the associated polymorphic ActiveStorage record.<br /><br /><br />What challenges did we face running rake tasks?<br /><br />At eLitmus , models with CDN bucket configurations have less than 20 thousand records. For models with a limited number of records above approach works well for us. It looks quite straightforward for us. As soon we started migrating the Default bucket , with each model with records greater than 50 ,000 , problems came arising. We started with records in increasing order of their count. For the Default bucket , we started our journey with 56 ,000 records by following the approach mentioned above. It took around more than 4 hours to migrate 56 ,000 in a staging environment. We can’t afford to wait for hours to migrate 56 ,000 attachments. So we had to come up with a different approach and , this is where things become interesting.<br /><br />After all the specs , we found that in the above approach , we have an open URI to download the attachment from Amazon s3 and re-upload it to the s3 bucket in the transaction that prolonged the database connection time. We came up with a different approach by designing our rake task; in such a way that instead of hitting s3 of every record , we decided to just come up with a database migration that copies all of the data generated from the paperclip to the new Active Storage required format. Paperclip adds attachment columns directly to the model’s tables such as image_file_name , image_content_type , image_updated_at , image_file_size. ActiveStorage stores this information in two dedicated tables ActiveStorageBlobs table and ActiveStorageAttachments table.<br /><br />We loop through the records of the model and then through each attachment definition within the model. If the model record doesn’t have an uploaded attachment , skip to the next record. Otherwise , we converted the Paperclip data to ActiveStorage records. We set the values for the new ActiveStorage records based on the data from Paperclip’s field for the ActiveStorageBlobs table.<br /><br />For the records with limited numbers , less than 1 ,00 ,000 approach works well for us. It took only 8 minutes to migrate 96 ,000 records. Our next target was to migrate around 4 ,50 ,000 migrate. We started migrating with the same approach we used for 96 ,00 ,000. But things do not go as straightforward. While migrating 4 ,50 ,000 maximum number of records in our Paperclip data had missing file size. As ActiveStorageBlobs table byte_size is the required field , We had to hit s3 API to fetch file size. It took around 4 hours in staging to migrate. On optimizing the rake task , we came up with another approach instead of reading data from a Paperclip column and then writing them to ActiveStorageBlobs at , same time. We decided to first read all the data from the Paperclip and then write it back to ActiveStorage. Firstly we read all the data from paperclip model columns and made them compatible with ActiveStorage Required format in CSV. Then we write data from CSV to ActiveStorage tables. It took 2 hours for us to migrate 4 ,50 ,000 records in production.<br />With the same approach next , we migrated around 14 ,00 ,000 records and , it took 45 minutes in staging and 18 hours in production.<br /><br />Adopted Active Storage<br /><br />After the job finished , we removed everything related to the paperclip and replaced its usage with active storage.  We updated config files , added Amazon s3 storage definitions to storage.yml , and removed paperclip configuration for attachments related to the model. Updated model , views , and controllers related to Active Record. The red , green , and refactor approach helped us to improve confidence that our code was working as expected.<br /><br />What challenges did we face during migration?<br /><br /><br />  Paperclip provides us several validators to validate our attachments. Out of the box , ActiveStorage doesn’t come with validations. We need to write custom validations in ActiveStorage , to add simple validations for attachments to validate presence , content type , attachment size. After some research , we found some outside gems , including active_storage_validations , provide us validators as Paperclip. As ActiveStorage is evolving day by day , validations are on the to-do list of the rails community. As soon as it is released , we will be ready to get the outside gem replaced.<br />  At eLitmus , we were using two Amazon s3 buckets - default bucket and CDN bucket , to store our attachments. Paperclip provides us functionality to store attachments on different buckets by giving an option bucket name while uploading attachment data. We started migrating from Paperclip to ActiveStorage with our application rails version 6.0. In Rails 6.0 , there was no such tool to categorize the bucket name while uploading an attachment. Almost half of the models in our application are using CDN bucket , and the rest are using default bucket. The Rails community is behind the ActiveStorage in the rails version 6.1 service column was introduced in the ActiveStorageBlobs table for categorizing the bucket name while uploading an attachment. So we migrated the first CDN bucket attachment with rails version 6.0. Then we upgraded our rails version to 6.1 and migrated the other half records to the default bucket.<br />  After the migration of 14 ,00 ,000 records after a week , we encountered a bug in production around 500 , records key were missing from the amazon s3 bucket. After few hours of debugging , we found that between the time ,  1st and 2nd MR’s merge in production. During , this period we kept everything from the paperclip as it is we , also added support for Active Storage. We were using both functionalities at the same time. During the time attachment for the model were migrated from paperclip to active storage , if a user decides to upload any attachments , the user still uses the paperclip implementation , but in the background after the successful commit of all transaction related to paperclip. We were duplicating the same attachment to active storage by using Active Record Callback after_commit. We produce the bug when the user uploads the attachment with the same filename as in our database before the migration process. We accidentally deleted the record’s key from amazon s3. After specs and debugging we , came up with a solution to recover these deleted files from amazon s3. We created a new rake task for recovering the deleted files from s3 by deleting the latest delete markers version for the key from s3. And all files were successfully recovered and working fine now on production.<br />  After three weeks , we encountered another problem in production. Some of our users reported to us with queries that some of them were having problems uploading a resume. After specs and analysis , we figure out that for around one thousand resume records , there were two ActiveStorage attachments for them in ActiveStorage tables. As ActiveStorage works on the principle that for one ActiveRecord object , there will be one ActiveStorage attachment for has_one_attached relationship. During specs , one more problem comes to our front that on our database there were around 3 thousand active storage attachments with missing resume ActiveRecord objects. After deep-diving into the codebase , we figured out that due to our daily cron job , which deletes all inactive users from our database. So for the past three weeks , this job was deleting all the ActiveRecord objects but not ActiveStorage Attachments. On the solution part , we first decided to restrict inactive users to upload the attachments without activating their accounts and updated cron jobs to delete all the ActiveStorage attachments associated with the ActiveRecord object whenever it is deleted. On the other hand , to match the same number of our ActiveRecord objects and ActiveStorage attachments for resumes , we created three rake tasks. The first one to remove all attachments except the latest one from the ActiveStorage tables for an ActiveRecord object with more than one attachment in ActiveStorage tables. The second one , to filter out all the active storage of type resumes which doesn’t have any records for them in the resume table. And saved active storage attachment ID and resume ID in CSV. The third one , that processes CSV generated in the second rake task and deletes all the active storage records associated with them from active storage tables. It took around 15-20 min to run all three rake tasks. As a result of it , both the ActiveRecord and ActiveStorage number matched. Now , it’s running fine on production. We have not received any queries yet.<br /><br /><br />Conclusion<br /><br />ActiveStorage has now been in production for over a week , and it’s been seamless. It provided us everything we needed though they are certainly more things that need to be evolved validations for attachments , supporting directory structure for active storage blob key. Looking Forward to seeing active storage evolve. And this will conclude our journey regarding migration from paperclip to ActiveStorage.<br />"
		} ,
	
		{
		  "title" : "Revamp of our Coding Platform",
		  "category" : "technology",
		  "url" : "/technology/revamp-of-our-coding-platform/",
		  "date" : "2021-05-17 00:00:00 +0530",
		  "content"	: "The story of how we took a good platform and made it even better<br /><br />Before I start telling you this story , I want to just make this clear that this is not filled with technical details of our implementation but rather with the thought process and the journey of redeveloping our coding platform. I will definitely share my learnings and some technical details of this whole endeavour in later blog posts.<br /><br />So , this story starts in October 2019 , with me looking at a web application that I inherited from previous developers at the organisation that I had joined 6 months earlier and thinking to myself that , “Here we have a perfectly functional web application that does it’s job pretty well , but still why does it feel so underwhelming and out of place on the modern web”.<br /><br />Realization<br /><br />What I realized after 2 days of pondering on this topic was that , with the way web applications and their popularity has been growing in our times , The UX had become as important as the function of the web application.<br /><br />If that was not clear , then let me explain further. In broad terms we can breakdown the components of a web application into 2 areas -<br /><br />  Back End or Server Side components<br />  Front End or User Side components<br /><br /><br />The Back End controls the functions of the web application , what it can do and how efficiently it can do that task.<br /><br />While the Front End dictates the interaction between the user and the application.<br /><br />Now , both of these components need to be as good as the other one to ensure that your web application provides a seamless experience. In the case of our coding platform , this was not true , as we had a great Back End implementation but the Front End felt like it was still stuck in the early 2010s.<br /><br /><br /><br /><br /><br />Identify the Issue<br /><br />I knew I wanted to change this platform , but it was important to focus on a few specifics instead of getting bogged down by all the things I wanted to improve. So , I sat down with my colleague Shubham Pandey (Please do checkout his blog and website. He has some amazing stuff on there) and we tried to categorise the problems in the platform under a few broad umbrellas.<br /><br />Experience - We used this category to encompass all the problems that were related to causing an inconvenience to the user who was using our platform.<br />Some of the problems we put under this category were things like the user not being able to see the list of problems while coding , the user’s event time starting before they can see the editor , not being able to see the result and the problem statement at one time and a few more things similar to these problems.<br /><br />Interface - We brought all the issues regarding design , layouts and colours on the platform under this category.<br />Problems like the text being too small in some places , buttons not being of a standard size , the event timer not eye catching feature of the design and again a few more problems similar to these ones.<br /><br />The actual list was a lot longer than mentioned here but , all of them importantly came under these two broad categories.<br /><br />Setting Objectives<br /><br />Now that we had our problem well-defined , we could move on to coming up with a plan of action to solve these problems. To solve these problems we started thinking like a user who had minimal technical background to give us a set of objectives.<br /><br />One of the biggest issues we noticed was the number of clicks that a user had to go through to reach the problem and start coding. On the old platform , a user had to go through the following steps to start their event:<br /><br />=&#62;; Login<br /><br />=&#62;; Find event on dashboard<br /><br />=&#62;; Click on &quot;Load Challenge&quot;<br /><br />=&#62;; Find/Select a problem from the list<br /><br />=&#62;; Click on &quot;Start&quot;<br /><br />=&#62;; Start Coding<br /><br />This was a lot of clicks to start an event on a platform dedicated to hosting coding events and we needed to reduce this as every click meant a complete page reload.<br /><br /><br />  Objective 1: Reduce the amount of clicks a user needs to reach the Coding Test<br /><br /><br /><br />  Objective 2: Minimise Page Reloads<br /><br /><br />Another issue was the dated look and feel of the UI. It did not feel slick or intuitive. This might have been a very good UI by 2012 standards but for 2019 it was not up to the mark.<br /><br /><br />  Objective 3: Modernise the UI<br /><br /><br />We found another issue with the editor we were using on the platform. We used Codemirror on the older platform which although was a good editor , had a few problems that were holding it back. The size of the library was huge , we had to load multiple script tags to access the full set of features , few editing options were missing and some more.<br /><br />P.S : After the recent Codemirror 6 updates some of these problems were solved but at that time there was no confirmation if that would be the case.<br /><br /><br />  Objective 4: Use a featureful coding editor with long term support<br /><br /><br />So , these were the 4 objectives that we set out to achieve in the first version of our new coding platform. Even though this was technically an overhaul of an existing project , we had started calling it “new” so that we start thinking for solutions from scratch instead of just updating a few things and complicating the whole code base and the project even more.<br /><br />Plan of Action and Execution<br /><br />To achieve our 4 objectives , we selected the following libraries and plugins and I will also briefly explain why we opted for these:<br /><br />  REACT<br />  Bootstrap<br />  Monaco Editor<br /><br /><br />To achieve Objectives 1 &amp;amp; 2.<br />We decided that we had to change flow of the user journey on the platform.<br />This was the only way that we could reduce the amount of clicks on the platform and for minimising page reloads , REACT came to our rescue.<br /><br />REACT allowed us to develop , what we call a SPA (Single Page Application) quite easily and without much hassle.<br />I will explain the specific use cases and advantages of a SPA in a future blog post.<br />Also an added benefit was that REACT had a pretty simple integration with our existing application which is a Ruby on Rails based web application. We integrated REACT into our Rails 5.2 application using the webpacker gem.<br />After Rails 6 the webpacker gem now comes as standard with Rails so using REACT as front end for a Rails application has become easier now.<br /><br />Bootstrap is a very popular library that makes developing beautiful UIs very simple with its plethora of classes and functionality that it offers. So , that was a very obvious choice to achieve Objective 3.<br /><br />And lastly Monaco Editor is also a very popular and well-supported coding editor. It is being officially maintained by Microsoft and contains a lot of features that Virtual Studio Code Editor provides on a desktop. That makes it an obvious choice when we were deciding on an editor to use for our platform to achieve Objective 4.<br /><br />Now you can check out the redeveloped platform and see how we executed our plan.<br /><br /><br /><br /><br /><br />Remember , the number of clicks the older platform required to get to the actual coding? That has been reduced to the following now in this new platform:<br /><br />=&#62;; Login<br /><br />=&#62;; Find event on dashboard<br /><br />=&#62;; Click on &quot;Load Challenge&quot;<br /><br />=&#62;; Start Event<br /><br />That’s it. Everything was compressed into a single page to provide a more intuitive and easy to use coding platform that would allow the candidate to focus on coding more than worrying about other things. We tried to make everything else like time , problem list , result etc. available at a glance whenever the candidate needs it.<br /><br />And if you are wondering , we did add a “Dark Mode” also , which has become quite the rage nowadays in modern web design. Notice the sun and moon icons on the right edge of the top bar that denoted the Light and Dark Modes respectively.<br /><br /><br /><br /><br /><br />So , that was the story of how we did a complete overhaul of our coding platform to make it fit for the modern web.<br />It took us about 2 months to complete this project , from coming up with the concept , finalising the technical specifications , development , testing and finally deployment.<br /><br />The process that we followed is what I still use whenever I have to come up with a solution to any problem. That is probably the biggest learning that I took from this project along with learning REACT and developing Single Page Applications that I use quite a lot now.<br /><br /><br />  You can find this article on the author’s blog piyushswain.github.io as well.<br /><br />"
		} ,
	
		{
		  "title" : "Migrating from state_machine to aasm in Rails",
		  "category" : "technology",
		  "url" : "/technology/migrating-from-state-machine-to-aasm-in-rails/",
		  "date" : "2018-06-29 00:00:00 +0530",
		  "content"	: "First things first. State machines are awesome , be it any part of technology you use them in.<br /><br />Recently at work , we passed many pipelines on migrating a very large Rails app from Rails 4 to Rails 5. One of the major parts of this change was shifting from state_machine to aasm for our state transitions. We rely heavily on state machines for how our instances shift states. Much of our tasks associated with the models too are integrated with the after/before actions of our state machines.<br /><br /><br /><br />Need for transition:<br /><br />One and only one reason , state_machine has been dead , and for quite some time. We shifted from Rails 3.2 to Rails 4.2 last year , and since it was a really , really painful migration , we fixed our focus on changed syntax and ActiveJob , found the much famous monkeypatch for Rails 4.2 and stayed happy for the time being with state_machine. Though there is state_machines_activerecord , we wanted to move to a more reliable and tested library , and as we already use acts_as_state_machine or aasm in one of our other projects , we tried and gave it a shot , when we began our Rails 5 voyage , for which of course neither state_machine and its patch worked , nor it was recommended.<br /><br />What changed:<br /><br />As it turned out , the process was not too messy. After a small study of the way both state_machine and aasm handle state transitions , one can easily find an analogy. Here are a few things which usually are a part of a state_machine laden project and how they should be modified to work with aasm<br /><br />1. The gem itself<br /><br />Goes without saying , remove from your Gemfile/gems.rb :<br /><br />gem &amp;#39;state_machine&amp;#39;<br /><br />and add :<br /><br />gem &amp;#39;aasm&amp;#39;<br /><br />2. Get rid of the state_machine monkey-patch if present<br /><br />module StateMachine<br />    module Integrations<br />      module ActiveModel<br />        public :around_validation<br />      end<br />      module ActiveRecord<br />        public :around_save<br />        def define_state_initializer<br />          define_helper :instance , &#60;;&#60;;-end_eval , __FILE__ , __LINE__ + 1<br />            def initialize(*)<br />              super do |*args|<br />                self.class.state_machines.initialize_states self<br />                yield(*args) if block_given?<br />              end<br />            end<br />          end_eval<br />        end<br />      end<br />    end<br />  end<br /><br />Yes , get rid of this if you have it , most probably in one of your config/initializers.<br /><br />3. Transitioning the transitions:<br /><br />This is the major part of the change and yet the easiest to implement. This includes code change in models. Take a look at the documentation over at aasm and start changing the code. Here are a few pointers.<br /><br />add include AASM to your model<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br />  end<br /><br />specify the column name on which you are observing state transitions , for eg. if the column name is state<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br />    aasm.attribute_name :state<br />    ...<br />  end<br /><br />Initiate your state machine block by listing out all your states. The common way is using one line to specify your initial state , and a second line to list all your non-initial states<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br />    aasm.attribute_name :state<br />    aasm do<br />      state :authored , initial: true<br />      state :piloted , :non_active , :active , :removed<br />      ...<br />    end<br />    ...<br />  end<br /><br />Convert your events. All event blocks of the form transition :a =&#62;; :b will be replaced by transitions from: :a , to: :b<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br /><br />    # State machine code<br /><br />    state_machine :state , initial: :authored do<br /><br />      event :pilot do<br />        transition :authored =&#62;; :piloted<br />      end<br /><br />      event :activate do<br />        transition [:piloted , :non_active] =&#62;; :active<br />      end<br /><br />      ..<br />    end<br /><br /><br />   # AASM code<br /><br />    aasm.attribute_name :state<br />    aasm do<br />      state :authored , initial: true<br />      state :piloted , :non_active , :active , :removed<br /><br />      event :pilot do<br />        transitions from: :authored , to: :piloted<br />      end<br /><br />      event :activate do<br />        transitions from: [:piloted , :non_active] , to: :active<br />      end<br /><br />      ...<br />    end<br />    ...<br />  end<br /><br />Callbacks like before_transition and after_transition from state_machine can be converted like this:<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br /><br />    # State machine code<br /><br />    state_machine :state , initial: :authored do<br />      before_transition :authored =&#62;; :piloted , :do =&#62;; :prepare_cockpit<br />      after_transition :authored =&#62;; :piloted , :do =&#62;; :fly_the_plane<br /><br />      event :pilot do<br />        transition :authored =&#62;; :piloted<br />      end<br />      ...<br />    end<br /><br />    # AASM code<br /><br />    aasm.attribute_name :state<br />    aasm do<br />      state :authored , initial: true<br />      state :piloted , :non_active , :active , :removed<br /><br />      event :pilot do<br />        before do<br />          prepare_cockpit<br />        end<br />        transitions from: :authored , to: :piloted , after: :fly_the_plane<br />      end<br /><br />      ...<br />    end<br />    ...<br /><br />    def prepare_cockpit<br />      ...<br />    end<br /><br />    def fly_the_plane<br />      ...<br />    end<br /><br /><br />  end<br /><br />However , in case of callbacks on a part of a transitions defined inside an event , one needs to define the transitions separately<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br /><br />    # State machine code<br /><br />    state_machine :state , initial: :authored do<br />      after_transition :authored =&#62;; :piloted , :do =&#62;; :fly<br /><br />      event :pilot do<br />        transition [:inactive , :authored] =&#62;; :piloted<br />      end<br />      ...<br />    end<br /><br />    # AASM code<br /><br />    aasm.attribute_name :state<br />    aasm do<br />      state :authored , initial: true<br />      state :piloted , :non_active , :active , :removed<br /><br />      event :pilot do<br />        transitions from: :authored , to: :piloted , after: :fly<br />        transitions from: :inactive , to: :piloted<br />      end<br /><br />      ...<br />    end<br />    ...<br /><br />    def fly<br />      ...<br />    end<br /><br />  end<br /><br />if and unless guard blocks on transitions work the same way as in state_machine , and can also be substituted with a guard clause. The guards as well as callbacks can take arguments , lambda as well as Proc , same as the state machine guards<br /><br />class Question &#60;; ActiveRecord::Base<br />    include AASM<br />    ...<br /><br />    # State machine code<br /><br />    state_machine :state , initial: :authored do<br /><br />      event :pilot do<br />        transition :authored =&#62;; :piloted , if: :can_fly?<br />      end<br />      ...<br />    end<br /><br />    # AASM code<br /><br />    aasm.attribute_name :state<br />    aasm do<br />      state :authored , initial: true<br />      state :piloted , :non_active , :active , :removed<br /><br />      event :pilot do<br />        transitions from: :authored , to: :piloted , guard: :can_fly?<br />      end<br /><br />      ...<br />    end<br />    ...<br /><br />    def can_fly?<br />      ...<br />    end<br /><br />  end<br /><br />Yes , that’s it for the models. You can take a detailed look at the docs if you have more complex needs.<br /><br />4. The helpers:<br /><br />One plus point for state_machine  , it has/had a variety of useful helpers for making use of states and events in views and controllers. aasm , though lagging behind a little in this domain , still has a good pool of helpers , both class and instance to make good use of. Here are some pointers.<br /><br /><br />  Question.aasm.states will give you an object list of all states available for the Question model<br />  Question.aasm.events will give you an object list of all events available for the Question model<br />  Question.first.aasm.states will give an object list of all states available for transitioning to for a Question object , in this case the first one.<br />  Question.first.aasm.events will give an object list of all events that can be applied on the current state of the Question object , i.e the first<br />  All of the above helpers will produce an object list that contains name as the name of object , so appending .map(&amp;amp;:name) will give a symbol array of the name of objects , that will come handy in drop-downs. Eg.<br /><br /><br />pry(main)&#62;; Question.last.aasm.events.map(&amp;amp;:name)<br />  =&#62;; [:pilot , :deactivate]<br /><br />Another great point in favor of state_machine is its state_event attribute over the instance. For eg.<br /><br />pry(main)&#62;; question = Question.first<br />  pry(main)&#62;; question.state_event = :deactivate<br />  pry(main)&#62;; question.save<br /><br />The code above will end up saving the question after calling the deactivate event over it. This attribute is highly useful in rails forms where one can easily pass what event to call from , and the transition will happen without extra hassle. Unfortunately , there’s no equivalent attribute cum method in aasm . But one can always write a common ActiveRecord::Base helper for the same.<br /><br />On another note , the not-so-good-looking with_state / with_states scope methods of state_machine can be replaced by the enum equivalent syntax of aasm . For eg.<br /><br />Question.with_state(:active) # state_machine<br /><br />gets replaced by a much cleaner :<br /><br />Question.active<br /><br />So yes , a couple of tweaks here and there , and a good pool of existing test cases which run green , you are done and production ready. This will get you started , but do back yourself up with the aasm docs.<br />"
		} ,
	
		{
		  "title" : "Android Versioning Using Docker &amp; Git Like A Pro",
		  "category" : "technology",
		  "url" : "/technology/android-versioning-using-docker-and-git-like-a-pro/",
		  "date" : "2018-06-10 00:00:00 +0530",
		  "content"	: "Unlike web , android still lacks the ease of version deployments. Specially when you don’t want to use Play Store.<br /><br />Introduction<br /><br />There will be five stages:<br /><br /><br />  Signing application<br />  Versioning of application. For that we gonna use git revision and Major.Minor.Patch naming convention.<br />  Building application using a docker. So that running environment doesn’t change.<br />  Pushing new release to s3 , while maintaining the previous versions.<br />  Pushing new tag to git , with the new version. So , we’ll have tags for each version.<br /><br /><br />Basically , we gonna use docker , git , and some simple hacks to put things in work. In the end , I’ve shared a sample application.<br /><br />Stage 1: Signing Our Application<br /><br />It’s better to start thinking about security right from the big bang.<br />From android studio , you can generate a new keystore , a jks file. Help?<br />Copy the keystore file details in a config.yaml file like below:<br /><br />key_store:<br />  key: /xyz/xyz.jks<br />  alias: key0<br />  store_password: wuhoo<br />  key_password: nibataunga<br /><br />Studio will take care of signing , but to generate signed apk from command line , you’ll need to make some changes in your build.gradle. The credentials we have put in above yaml file will be passed as command line args to gradle(Build stage[2]).<br /><br />android {<br />    ...<br />    signingConfigs {<br />        release {<br />            if (project.hasProperty(&amp;#39;APP_RELEASE_STORE_FILE&amp;#39;)) {<br />                storeFile file(&ldquo;$APP_RELEASE_STORE_FILE&ldquo;)<br />                storePassword &ldquo;$APP_RELEASE_STORE_PASSWORD&ldquo;<br />                keyAlias &ldquo;$APP_RELEASE_KEY_ALIAS&ldquo;<br />                keyPassword &ldquo;$APP_RELEASE_KEY_PASSWORD&ldquo;<br />            }<br />        }<br />    }<br />    buildTypes {<br />        release {<br />          ...<br />          if (project.hasProperty(&amp;#39;APP_RELEASE_STORE_FILE&amp;#39;)) {<br />              signingConfig signingConfigs.release<br />          }<br />        }<br />    }<br />}<br /><br />Stage 2: Release Versioning , Digging Git<br /><br />I’am here using the semantic versioning.<br /><br />Major.Minor.GitRevision.Patch<br /><br />Let’s dig into GitRevision<br />It counts the number of commits from git , so you’ll get incremental values everytime you release a new version. GitRevision will make versioning easy and consistent.<br /><br />We’ll put the below code in build.gradle[app]<br /><br />def getGitRevision = { -&#62;;<br />    try {<br />        def stdout = new ByteArrayOutputStream()<br />        exec {<br />            standardOutput = stdout<br />            commandLine &amp;#39;git&amp;#39; , &amp;#39;rev-list&amp;#39; , &amp;#39;--first-parent&amp;#39; , &amp;#39;--count&amp;#39; , &amp;#39;master&amp;#39;<br />        }<br />        logger.info(&ldquo;Building revision #&ldquo;+stdout)<br />        return stdout.toString(&ldquo;ASCII&ldquo;).trim().toInteger()<br />    }<br />    catch (Exception e) {<br />        e.printStackTrace();<br />        return 0;<br />    }<br />}<br /><br />And in build.gradle[app]<br /><br />defaultConfig {<br />        ...<br />        versionCode = 10000000*majorVersion+10000*minorVersion + 10*revision<br />        versionName = &amp;#39;v&amp;#39; + majorVersion + &amp;#39;.&amp;#39; + minorVersion + &amp;#39;.&amp;#39; + revision + patch<br />    }<br /><br />Docker Image , Savage<br /><br />We first need to build a docker image with minimum libraries and dependencies required.<br /><br />FROM openjdk:8<br />RUN apt-get update<br />RUN cd /opt/<br />RUN wget -nc https://dl.google.com/android/repository/sdk-tools-linux-4333796.zip<br />ENV ANDROID_HOME /opt/android-sdk-linux<br />RUN mkdir -p ${ANDROID_HOME}<br />RUN unzip -n -d ${ANDROID_HOME} sdk-tools-linux-4333796.zip<br />ENV PATH ${PATH}:${ANDROID_HOME}/tools:${ANDROID_HOME}/tools/bin:${ANDROID_HOME}/platform-tools<br />RUN yes | sdkmanager --licenses<br />RUN yes | sdkmanager \<br />      &ldquo;platform-tools&ldquo; \<br />      &ldquo;build-tools;27.0.3&ldquo; \<br />      &ldquo;platforms;android-27&ldquo;<br /><br />RUN apt-get -y install ruby<br />RUN gem install trollop<br /><br />Trollop will be helpful in compiling scripts , spicing the boring command line args.<br /><br />We are using openjdk as base image for java environment and installed our sdk with version 27. You can change that accordingly.<br /><br />Building the image:<br /><br />docker build -t ${docker_image} -f ./scripts/Dockerfile .<br /><br />Or you can directly pull my latest base image.<br /><br />docker pull mukarramali98/androidbase<br /><br />Docker container on the way<br /><br />To automate the process , let’s dig into a small script:<br /><br />#!/usr/bin/env bash<br />set -xeuo pipefail<br /><br />app_name=xyz<br />container_name=androidcontainer<br /><br />if [ ! &ldquo;$(docker ps -q -f name=${container_name})&ldquo; ]; then<br />    if [ &ldquo;$(docker ps -aq -f status=exited -f name=${container_name})&ldquo; ]; then<br />        # cleanup<br />        docker rm $container_name<br />    fi<br />    # run your container<br />    docker run -v ${PWD}:/${app_name}/ --name ${container_name} -w /${app_name} -d -i -t mukarramali98/androidbase<br />fi<br /><br />docker exec ${container_name} ruby /${app_name}/scripts/compile.rb -k /${app_name}/config.yaml<br /><br />Here we first check if the container already exists. Then create accordingly.<br />While creating the container , we mount our current project directory. So next time we run this container , our updated project will already be there in the container.<br /><br />Stage 3: Running container , Build Stage<br /><br />We run the container , with our compile script. Pass the signing config file we created earlier.<br /><br />config = YAML.load_file(key_config_file)<br /><br />key_store = config[&amp;#39;key_store&amp;#39;]<br />output_file = &amp;#39;app/build/outputs/apk/release/app-release.apk&amp;#39;<br />`rm #{output_file}` if File.exists?output_file<br /><br />puts `#{File.dirname(__FILE__)}/../gradlew assembleRelease --stacktrace \<br />    -PAPP_RELEASE_STORE_FILE=#{key_store[&amp;#39;key&amp;#39;]} \<br />    -PAPP_RELEASE_KEY_ALIAS=#{key_store[&amp;#39;alias&amp;#39;]} \<br />    -PAPP_RELEASE_STORE_PASSWORD=&amp;#39;#{key_store[&amp;#39;store_password&amp;#39;]}&amp;#39; \<br />    -PAPP_RELEASE_KEY_PASSWORD=&amp;#39;#{key_store[&amp;#39;key_password&amp;#39;]}&amp;#39;`<br /><br />Stage 4: Pushing to S3<br /><br />So , now we have build a signed apk from a docker container. It’s time to push them.<br />Connect with your s3 bucket and generate $HOME/.s3cfg file , and pass it to ruby script below:<br /><br />if File.file?(s3_config)<br />  # Push the generate apk file with the app and version name<br />  `s3cmd put app/build/outputs/apk/release/app-release.apk s3://#{bucket}/#{app_name}-#{version_name}.apk -m application/vnd.android.package-archive -f -P -c #{s3_config}`<br />  # application/vnd.android.package-archive is an apk file format descriptor<br /><br />  # Replace the previous production file<br />  `s3cmd put app/build/outputs/apk/release/app-release.apk s3://#{bucket}/#{app_name}.apk -m application/vnd.android.package-archive -f -P -c #{s3_config}`<br /><br />  # To keep the track of latest release<br />  `echo #{version_code}&#62;; latest_version.txt`<br />  `s3cmd put latest_version.txt s3://#{bucket}/latest_version.txt -f -P -c #{s3_config}`<br />  `rm latest_version.txt`<br />  puts &ldquo;Successfully released new app version.&ldquo;<br />end<br /><br />application/vnd.android.package-archive is the apk file type descriptor.<br /><br />Stage 5: Finally , Git Tagging The New Release Version , #hashtag<br /><br />def push_new_tag version_name<br />  `git tag #{version_name}`<br />  `git push origin #{version_name}`<br />  puts &ldquo;New tag pushed to repo.&ldquo;<br />end<br /><br />Demo Application<br />"
		} ,
	
		{
		  "title" : "How Tough is it to Score Well in Board Exams?",
		  "category" : "career",
		  "url" : "/career/how-tough-is-it-to-score-well-in-board-exams/",
		  "date" : "2018-02-22 00:00:00 +0530",
		  "content"	: "I completed my entire schooling (Classes I through XII) at one of Kolkata’s favoured catholic schools. In those days , discipline and academic excellence were the primary parameters that mattered and my school checked both these boxes rather well.<br /><br /><br />Trouble started brewing once I graduated to Class XI and started thinking about higher education , specifically opportunities at the national level. That’s when I truly realized the impact of the education board. In my case , the impact was limited to a couple of aspects , viz. a) subjects / topics not covered in the Bengal board syllabus , and b) the frugality in awarding marks.<br /><br />Fortunately , some extra tuitions covered up for the former , while the latter did not come into play at all in any of the options I signed up for , or in the higher education option I finally opted for.<br /><br />Times have changed….<br /><br />For a few years , till 2016 , 40% weightage was accorded to an applicant’s Class XII board marks in calculating her All India Rank in the JEE (entrance tests for admission to India’s flagship IITs , and a few other engineering schools) exams. However , since 2017 , the rules were changed to treat the Class XII marks as a qualifying criterion: a minimum of 75% marks , or a rank in the top 20th percentile in the board.<br /><br />The 75% cut-off may appear inconsequential to folks intimately familiar with the CBSE or ISCE boards , but not all students find it amusing. The JEE implementation committee publishes the 80th percentile cut-off marks for every higher secondary educational board in the country to level the playing field. Finally , we have access to data that clearly shows the disparity in awarding marks across boards in India.<br /><br />According to data for the 2016 Class XII exams , the 5 most liberal boards are (percentages indicate the 80th percentile cut-off score):<br /><br />  Telengana Board of Secondary Education (95%)<br />  Andhra Pradesh Board of Intermediate Education (94%)<br />  Council for the Indian School Certificate Examinations (88.6%)<br />  Banasthali Vidyapeeth , Rajasthan (87.4%)<br />  Tamil Nadu Board of Higher Secondary Education (87.2)<br /><br /><br />While the 4 most frugal boards are:<br /><br />  Tripura Board of Secondary Education (59.8%)<br />  Jharkhand Academic Council (60.6%)<br />  Meghalaya Board of Secondary Education (61.6%)<br />  Odisha Council of Higher Secondary Education (62%)<br />  Bihar Intermediate Education Council (63%)<br /><br /><br />What this essentially means is that a student scoring 95% in the Telengana board exams is academically comparable to a student scoring 60.6% in the Jharkhand board exams , despite a whopping 34.6% gap is scores!<br /><br />The data clearly shows how a single mark-based cut-off or a mark-based weightage criterion can result in gross injustice to students from boards that are frugal in awarding marks! Thankfully , the JEE implementation committee , in its infinite wisdom , has taken steps to normalize this inherent disparity.<br /><br />Time will tell whether the practice of allotting an explicit or implicit weightage to board exam performance will become the norm , not only in JEE but in other national level entrance tests as well. But for now , this is definitely something for parents to consider while looking for a school for their children.<br /><br />But what about employment? It is common practice among potential employers to set mark-based cut-offs for board exams (while hiring entry-level talent) , among others. And in almost all cases , the cut-off is a single number applicable across the board (pun intended!).<br /><br />Let’s say company X sets a Class XII marks cut-off at 75%. Referring to the 10 boards listed above , company X will end up considering a population far larger than the top quintile from to 5 most generous states , and a population far smaller than the top quintile in the 5 most frugal states. The playing field is not so level anymore…..<br />"
		} ,
	
		{
		  "title" : "It's the Attitude, Stupid!",
		  "category" : "career",
		  "url" : "/career/its-the-attitude-stupid/",
		  "date" : "2017-12-19 00:00:00 +0530",
		  "content"	: "Congratulations on your first job! So you cleared the selection process; cracked some tests maybe , shone through a group discussion possibly , and impressed and charmed your way through the interviews. Well done!<br /><br /><br />You’ve been assessed and found suitable for the job on offer. No more evaluations , no more assessments. Right?<br /><br />Wrong! Let’s get one thing straight. This is just the beginning of your evaluation. What you’ve achieved (and by no means is it trivial) is to convince your future employer that you have potential. However , from day one in your first job , you will be continuously assessed on what you deliver.<br /><br />For whatever it’s worth , taking the liberty to share a cheat sheet that may help in ensuring that promise does translate to delivery in the first couple of years in your career. And be warned that it’s all in the attitude…<br /><br />Cheat 1<br />Sumadhur was undoubtedly brilliant. Armed with a degree from India’s best engineering school , he was given complex tasks in line with his academic record and promise.<br /><br />However , very soon , his manager realised that he was not able to complete most of his tasks. The manager tried to find probable reasons behind such repeated failures. Soon , he realised that Sumadhur was not open to data , insights or feedback that conflicted with his own assumptions and beliefs.<br /><br />Tell yourself at least once each day: “I do not know anything. I am here to learn and apply.”<br /><br />Cheat 2<br />Abhinav had completed just 6 months in his first job , but had been upset for a while. His friends earned twice as much as him , despite working fewer hours than he did. They seemed to be having a rollicking time! Abhinav started looking around and got a 50% higher offer. He took the offer up with glee. Not only was the pay better , but the work was less demanding and had relaxed deadlines. Life could not be better…<br /><br />Two years down the line , Abhinav started looking for yet another change , and yet another quantum leap in compensation. However , he found , to his dismay , that he was way out of depth and no company was willing to make him an offer.<br /><br />Convince yourself that reward follows performance , and not the other way around.<br /><br />Cheat 3<br />Kanu was very sharp. However , four years of hostel life had turned his biological clock upside down. When he did make it to office during normal working hours , he excelled in his tasks. Unfortunately , on most days , he simply could not.<br /><br />Within six months , he was asked to leave.<br /><br />Change your college habits to the extent required , so as to ensure that you are available (and awake!) when your work and / or team needs you to be.<br /><br />Cheat 4<br />Manish was a star in college. He excelled in academics and was popular among students as well. Out of sheer habit , he continued behaving in a brash manner with his peers in the workplace.<br /><br />He was counselled by his manager , but was unable to mend his ways in time and was asked to leave.<br /><br />Accept the fact that you need to interact with people across age groups and cultures. You are not expected to like or respect one and all (after all , respect is earned!). However , you are expected to treat one and all with respect.<br /><br />"
		} ,
	
		{
		  "title" : "Fishing in Troubled Waters",
		  "category" : "the other side",
		  "url" : "/the-other-side/fishing-in-troubled-waters/",
		  "date" : "2017-12-19 00:00:00 +0530",
		  "content"	: "Back in my final year in college , I applied for a job at an FMCG major hiring for Techno-managerial roles. I cleared the eligibility criteria and the group discussion. The short and not-so-sweet interview went something like this:<br /><br /><br />Q: Did you apply to any core companies?<br /><br />A: Yes , I applied to X and Y.<br /><br />Q: What happened?<br /><br />A: I did not clear the written test.<br /><br />Q: What were the tests about?<br /><br />A: They were technical tests.<br /><br />Q: Why do you think you did not clear?<br /><br />A: Majority of questions were from courses taught in my second year. Since I had not brushed up on concepts , I could not answer many questions.<br /><br />Q: We expect our hires to know their domain. We cannot train them. It’s clear that you do not know your domain. Thank you for your time.<br /><br />A: (Thinking)… wow what just happened???<br /><br />I was asked the same set questions in a subsequent interview and , needless to say , I did not repeat all my answers (though I did not lie). The irony of it all is that , not only was I made an offer , I accepted the offer , spent 8 years in that organization and did fairly well in a core technical role!<br /><br />Since I moved over to the other side , I have kept revisiting to my own experience in that FMCG interview. My key takeaways are:<br /><br /><br />  <br />    Aptitude or natural ability matters orders more than knowledge in a particular domain , especially in entry level roles.<br />  <br />  <br />    In today’s dynamic landscape , it is almost guaranteed that specific skills will become obsolete with time. Therefore , the relevance of employees will persist only if they have the ability to learn new skills. Once again , natural ability and mindset will come to the fore.<br />  <br />  <br />    There was no Google in those days. Nor had anyone imagined Facebook , LinkedIn or Twitter. Today , any half-serious candidate can dig up a bunch of information on a company’s recruitment process , right down to the kind of questions asked in tests and interviews , in no time whatsoever. Generic questions will lead to rehearsed “correct” answers , which have no correlation with the actual aptitude , mindset , skill or aspirations of the candidate.<br />  <br /><br /><br />I find it amusing that many organizations still follow the same two-decade old formulae to hire people. To be sure , it may very well work for certain organizations. However , it is worth taking a dispassionate look at whether your process indeed works like you expect it to. Are you convinced that you are not ending up hiring the “wrong” folks , or worse , missing out on the “right” ones?<br />"
		} ,
	
		{
		  "title" : "Hiring may be Rocket Science, but its Tenets are Basic",
		  "category" : "the other side",
		  "url" : "/the-other-side/hiring-may-be-rocket-science-but-its-tenets-are-basic/",
		  "date" : "2017-10-24 00:00:00 +0530",
		  "content"	: "Hiring is as old as employment itself. We are talking about a time period of a few thousand years – not a fact to be trifled with. The methods , tools and approaches may have changed and evolved with time , but I believe that it’s safe to say that the underlying principles would not have changed much.<br /><br /><br />Recall instances where you were evaluating a potential candidate - and it does not matter whether you were looking for someone to help out at home , or someone to drive your vehicle , or someone to join your team at work. Consciously or subconsciously you would be ticking checkboxes against these basic principles… Not convinced? Okay , allow me to lay them out for you – I’ll stick my neck out on this one.<br /><br />Tenet 1<br /><br />Notwithstanding his ability with the bat , fitness levels and mental strength , would MS Dhoni have been successful as an opener in Test Cricket? Most likely not!<br /><br />What are you assessing?<br />Does she have an aptitude for this job? In other words , does she have a natural ability to perform this job?<br /><br />Tenet 2<br /><br />An ex-colleague of mine , a product of India’s best t-school , was undoubtedly brilliant. However , he consistently failed to meet his goals. Reason: either lost interest in the last mile , or buckled under deadline pressures. Sounds familiar?<br /><br />What are you assessing?<br />Does she have the right mindset? How often have we seen – and heart wrenchingly so – that aptitude alone cannot and does not guarantee success on the job?<br /><br />Tenet 3<br /><br />Ever wondered why a majority of software developers are engineers?<br /><br />What are you assessing?<br />Does she have the necessary skills? Skills could be knowledge , experience or even academic qualification? Even if you have a top class training facility at your disposal , your fresh hires will need a minimum set of pre-existing skills that are critical to succeed.<br /><br />Tenet 4<br /><br />A high performing and reliable colleague decided to put in his papers because he wanted to migrate to the US.<br /><br />What are you assessing?<br />Do her aspirations align with what you can offer her? And this is not just about compensation and benefits. But more importantly , a match between career aspirations and available growth paths in your organization.<br /><br />Would love to hear your views , especially if you disagree or have a contrarian opinion. Keep them coming…<br />"
		} ,
	
		{
		  "title" : "Money, get away, Get a good job with more pay and you're OK?",
		  "category" : "career",
		  "url" : "/career/money-get-away-get-a-good-job-with-more-pay/",
		  "date" : "2017-10-23 00:00:00 +0530",
		  "content"	: "Many commentators , most of them far wiser than me , have coined brilliant terms to refer to the generation gap that exists in the Indian demographic today. Since the workplace is but a small subset of the overall demographic , this generation gap is equally alive and kicking there as well.<br /><br /><br />In my humble view , the workforce in any organization can be divided into two generations:<br /><br />Those who completed schooling before the turn of the millennium , and<br />Those who did so in the present century<br /><br /><br />I’d like to see the former as the RKM (Roti Kapda Makaan) generation , whereas the latter is more of the YOLO/FOMO (You Only Live Once/Fear Of Missing Out) kind. I neither desire , nor possess the ability , to perform a cost-benefit analysis of either approach towards personal finance. However , running the risk of generalization , this is probably how each generation judges the other:<br /><br />RKM (judging YOLO/FOMO): Where are your savings? How can you blow up your entire salary within the first fortnight? Let me show you how to manage your finances.<br /><br />YOLO+FOMO (judging RKM): Papa don’t preach! Get a life!<br /><br />Irrespective of whether money is funny or not for you or whether you desire to write a suicide note on a hundred-dollar bill , you cannot escape the thrill (or chill) of earning your first few pay-checks! And this is arguably one of the primary transformations you’ll experience as you transition from being a student to a professional in your chosen place of work. But as Uncle Ben advised Peter Parker (aka Spiderman) , with great power comes great responsibility…<br /><br />What’s fantastic is , that the power is yours to wield and whether you want to do so responsibly or not , is entirely your choice!<br /><br />"
		} ,
	
		{
		  "title" : "To the Goth Kids",
		  "category" : "career",
		  "url" : "/career/to-the-goth-kids/",
		  "date" : "2017-07-28 00:00:00 +0530",
		  "content"	: "<br /><br />It seems like only recently , startups would hire folks to do just about everything , all at once , and then somehow , we’ve arrived at a ‘Product Manager of Paytm Experience’. How tunnel vision syndrome is eroding the startup spirit.<br /><br /><br />Recently , a startup valued at around half a billion dollars laid off 10 of its Product Managers , leaving them with another 30. Just last year , the same startup hired Product guys at 20–30% premium from the market at average CTCs well over ₹20L. Even more intriguing , some of these guys were techie turned MBAs with less than four years’ experience. Having always been on the sales side of things , where one had to justify a minimum of 4x one’s salary to the company , I was fascinated by this lot. What did they do that was worth that much money?<br /><br />One product guy I spoke to said he managed Paytm experience; which meant he had to ensure there were no drop-offs when the user chose to pay through Paytm. He also said he was mandated to prioritise payment through Paytm. And there were similar folks for each of the other alternate payment processes. “But , is Paytm the most competitive payment solution?” , I asked. He didn’t care , really. All that mattered was ‘mukesh23’ got through paying on the platform without , gods forbid , choosing to click on the refresh button. Even better if he came through one of the exclusive Paytm promotions that he had brokered with his counterpart from the other side.<br /><br />Evidently , the metric for success — # completed transactions , is not entirely off-base. A good product guy could save the company millions , potentially. But , on closer observation it seems as though his chutzpah might also cost the company millions , if not more. In the above case , there are several problems with how the roles are structured. What if Paytm wasn’t the best payment option on the platform? What if (plain conjecture , here) it were costlier , for instance? What if these users exited the native platform at rates higher than average?<br /><br />Let’s leave those seemingly troubling questions aside for a moment. What does one do to improve a third party payment experience? Mainly , vary size and placement of the button , apparently. Turns out , there isn’t much you can do. But , that doesn’t mean you can’t do nothing. So , if you notice a needless “improvement” in your experience , know that some Product Manager’s review is forthcoming. Then , is it a surprise , really , that when things take a turn for the worse , these lot are first in the line of fire? I was surprised , though , to understand that a lot of them were entirely at peace with the transitional nature of their employment. 30% elsewhere , then.<br /><br />This is not restricted to Product folks alone , although it does seem like in recent years it has become a “get rich scheme” of sorts for some techies with an acute propensity to bullshit their way through things. I see Marketing Managers who can’t / won’t write a line of copy or tweak keywords on their website. I see Designers who can’t / won’t code simple HTML or work on user personas and flow maps. I see Engineers who can’t / won’t test their code or learn how to write coherent software requirement specifications. This is manifestly due to the over-specialisation of roles and warped organisation structures in these startups. Ergo , the bleeding disinterest.<br /><br />It wasn’t always like this. It used to be that startups hired people to do just about everything , all at once.<br /><br />Fresh out of college , I was hired as a ‘Management Trainee’ , which I came to realise was code for will do whatever the hell it takes to move the needle. In the first year alone I did Sales , Product , Operations , and Marketing. I wasn’t alone; it seemed like everybody did everything. I remember our VP-Technology managing client delivery for a new initiative , and doing a damn good job at that. I remember our Operations Manager writing bizarre VBA Macros for Excel that saved hours of effort. And it seemed everybody everywhere else , too , were running an arm and a leg short of the work that was on their plates. It was synonymous with startups.<br /><br />It made tremendous business sense , too. First , it was easier to find (and afford) people at the median levels of overlapping skill sets than at the top 1% of their specialisations. Second , people always had a macro view of the company’s goals and everybody , more or less , aligned their personal work accordingly. Third , it had unexpected , yet , massive pay-offs for our chosen specialisations: techies wrote better code because they understood business and sales guys were more effective because they knew the real implications of that code. It wasn’t easy , but those years probably had the greatest impact in my life. It schooled my thoughts and perspectives.<br /><br />When people ask me what changed over the years , I give them the following analogy: a startup , back in the day , was like a goth band. It attracted the misfits , the weirdos , and those of us who just happened to stumble into the mosh pit. There was a ton of work to do and very little real money to be made. You belonged to somewhat of a cult and expected unreasonable things of yourself and your brethren. What we lacked in resources we made up for with ingenuity and perseverance. Over the years , however , the goth band got a makeover. We couldn’t have been more thrilled at that time. It seemed like the World was finally giving us our due. We didn’t have to explain to mothers , uncles and landlords , what we did for a living. We could finally afford EMIs.<br /><br />But , gradually , the goth kids turned cool. The makeup , now , seemed superficial and the cult constantly disowned its own — for how much wisdom can be gained from tweaking button sizes over years? The law of diminishing marginal returns applied: increasing number of new members to the cult caused the marginal product of others to be smaller than the marginal product of the previous members at this point. And that’s how we got ‘Product Manager of Paytm Experience’.<br /><br />But , what of the goth kids? They do lurk around. You won’t find them in conferences or hackathons or by the vending machines mooching off’ the free stuff. They’re likely in an intimate corner , head immersed in the laptop , being productive and maybe checking on Twitter once a while. If you’re ever in the position of hiring for your startup , my suggestion is for you to find and hire the goth kid. He’ll remind you why you started up in the first place. And also , he won’t ask you about your company’s pet policy.<br /><br /><br /><br />  This article was originally published  on Medium. We are republishing it here with the permission of the author<br /><br /><br /><br />"
		} ,
	
		{
		  "title" : "Setting up OAuth2 callbacks in Rails with HTTPS offloading on load balancers",
		  "category" : "technology",
		  "url" : "/technology/setting-up-elb-plus-nginx-with-https-offloading/",
		  "date" : "2017-03-08 00:00:00 +0530",
		  "content"	: "“HTTPS everywhere” is not a luxury anymore. It is a necessity. Thankfully , obtaining an SSL certificate has become easier too , with initiatives such as Let’s Encrypt , GeoTrust , Positive SSL , StartSSL. Even cloud based services such as Cloudflare and Amazon AWS provide free SSL certificates to their customers.<br /><br />####Here is setting some context to help the reader appreciate the discussion:<br />We host our rails applications on Amazon AWS. We generally use three different environments - development , staging and production. Development environment is generally local to a developer while staging and production are hosted on the cloud. There is a minor difference in the way we configure our staging and production environments. Our staging environment typically contains a single machine instance hosting our application. This single instance is exposed to internet directly (has a public IP). On the other hand , our production environment typically contains a cluster of instances for the sake of horizontal scaling. These instances typically do not have a public IP and hence not exposed to internet directly. We put this cluster behind an internet-facing Elastic Load Balancer (ELB).<br /><br />We use chef-solo to manage our cloud infrastructure as well as to deploy code to various environments.<br /><br />#####The Problem Statement:<br />For the sake of this discussion , we shall limit ourselves to configuring SSL certificates obtained from the two free providers , namely Let’s Encrypt and Amazon AWS.<br /><br />Using Let’s Encrypt in a clustered setup is tricky , since you need to make one of the instances stateful , in the sense , one instance needs to be given the responsibility of obtaining and renewing SSL certificate from Let’s Encrypt. All other instances need to copy this certificate every time its renewed. This requirement unnecessarily complicates the setup and also takes away some amount of flexibility. Also , Let’s Encrypt does not issue wildcard certificates and the validity of a certificate is just 90 days<br /><br />The certificates provisioned from the other provider , Amazon AWS , can only be installed on an ELB. Hence is best suited for our clustered setup , namely production. An added advantage is that Amazon can issue wildcard certificates. We could always add an ELB to our staging environment (even though we will never have more than one instance) , but that costs extra money for no reason.<br /><br />This leaves us with these options<br /><br /><br />  <br />    Environment<br />    Best Option<br />  <br />  <br />    Staging<br />    Let's Encrypt<br />  <br />  <br />    Production<br />    Amazon AWS<br />  <br /><br /><br />We went ahead with this choice. Using chef to manage our setup came handy.<br /><br />We first configured our Staging environment and everything worked as expected.<br /><br />However , the same application , in production environment , started throwing CSRF detected Error whenever an OAuth2 callback happened. This was really strange. Our application integrated with two different OAuth providers , and the problem was consistent with both these providers.<br /><br />#####What’s the issue?<br /><br />The only difference between our Staging and Production setups was the ELB.<br /><br />In production , we offloaded HTTPS at the ELB. Plain HTTP request would hit the NGINX web server , which in turn would reverse-proxy it to unicorn and rails.<br /><br />CSRF detected was clearly an error emitting from the rails application. Not from NGINX , and not from the ELB.<br /><br />A closer look would reveal that the rails application had no way to know if the callback was made on a http:// URL or a https:// URL , because it sees only HTTP (due to offloading).  Was this the reason rails was unhappy?<br /><br />OAuth2 , by design , does not accept plain HTTP callbacks (unless it is to localhost).<br /><br />####How do we move forward?<br /><br />#####PoC to prove the theory<br /><br />Just to confirm what we think is the cause , we enabled HTTPS on NGINX (like we did in our staging environment). This was in addition to HTTPS on the Load balancer. We reconfigured the Load Balancer to NOT offload HTTPS but forward the request as-is to NGINX.<br /><br />What do we have now? The CSRF detected errors are gone. Application behaves just like it should.<br /><br />This confirmed our theory.<br /><br />But the question now is , how do we achieve our desired configuration of offloading HTTPS at the ELB ? Is it just not possible ?<br /><br />The Solution<br /><br />We have been using X-Forwarded-For header while reverse proxying to unicorn so that our rails application knows the client IP address (rather than the IP address of the Load Balancer). We need this for logging and tracking.<br /><br />Could there be something on similar lines to tell the rails application that the request was not on HTTP but on HTTPS?<br /><br />Sure there is. We had to set a header in our reverse proxy configuration:<br /><br />X-Forwarded-Proto  to  https<br /><br /><br />For NGINX , we do it like this:<br /><br />proxy_set_header X-Forwarded-Proto https;<br /><br />Voila , Rails is happy and things are back to normal!<br /><br />Details:<br /><br />Csrf detected!<br /><br />Rails bothers about SSL only at two places , <br /><br /><br />  At environment config , force_ssl.<br /><br />  At external included Gem like Omniauth. <br /><br /><br /><br />In Rails environment config.<br /><br />config.force_ssl = true<br /><br />This does the trick , but doesn’t seem like a good idea to enable this option in Rails because , we offload https at NGINX. For Rails , request came in http , so it does a permanent redirect to https , which ends in a infinite loop.<br /><br />Our stack trace gave a clue that error might be inside omniauth gem.<br /><br /><br />    actionpack-4.2.7.1/lib/abstract_controller/base.rb:132 → process<br />    actionview-4.2.7.1/lib/action_view/rendering.rb:30 → process<br />    actionpack-4.2.7.1/lib/action_controller/metal.rb:196 → dispatch<br />    actionpack-4.2.7.1/lib/action_controller/metal/rack_delegation.rb:13 → dispatch<br />    actionpack-4.2.7.1/lib/action_controller/metal.rb:237 → block in action<br />    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:74 → dispatch<br />    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:43 → serve<br />    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:43 → block in serve<br />    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:30 → each<br />    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:30 → serve<br />    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:817 → call<br />    omniauth-1.3.1/lib/omniauth/strategy.rb:186 → call!<br />    omniauth-1.3.1/lib/omniauth/strategy.rb:164 → call<br /><br /><br />As we dug inside the Gem and found out that Omniauth looks at these headers<br /><br />lib/omniauth/strategy.rb#L493-L499<br /><br />def ssl?<br />  request.env[&amp;#39;HTTPS&amp;#39;] == &amp;#39;on&amp;#39; ||<br />  request.env[&amp;#39;HTTP_X_FORWARDED_SSL&amp;#39;] == &amp;#39;on&amp;#39; ||<br />  request.env[&amp;#39;HTTP_X_FORWARDED_SCHEME&amp;#39;] == &amp;#39;https&amp;#39; ||<br />  (request.env[&amp;#39;HTTP_X_FORWARDED_PROTO&amp;#39;] &amp;amp;&amp;amp; request.env[&amp;#39;HTTP_X_FORWARDED_PROTO&amp;#39;].split(&amp;#39; ,&amp;#39;)[0] == &amp;#39;https&amp;#39;) ||<br />  request.env[&amp;#39;rack.url_scheme&amp;#39;] == &amp;#39;https&amp;#39;<br />end<br /><br />This is where we found that setting up X_FORWARDED_PROTO to https should fix our problems.<br /><br />Initially , this X_FORWARDED_PROTO was set to $scheme. Which will be http for production as https is offloaded at ELB.<br /><br />Now , by setting X_FORWARDED_PROTO to https , we are making sure that redirects are happening on https.<br /><br />"
		} ,
	
		{
		  "title" : "Custom Capacity Buffers In Go",
		  "category" : "technology",
		  "url" : "/technology/custom-capacity-buffers-in-go/",
		  "date" : "2015-03-31 00:00:00 +0530",
		  "content"	: "At elitmus we use ruby to create most of our tools and most of our applications  are also written in ruby. Recently I started exploring ways to build our tools especially the backend tools in languages other than ruby which have much lesser memory footprint and better efficiency. One such cases was to create a sandboxed environment for running  untrusted code on our servers. After evaluating multiple languages , I decided to use golang because of it’s excellent library support coupled with the fact the docker(a sandboxed env) was also written in go.<br /><br />One of the many challenges we faced while creating our sandbox was  redirection of the standard output of untrusted code , as this simple code below will fill up the disk if redirected to file or use all system resources if redirected to a buffer.<br /><br />while(true) printf(“I am the green monster”);<br /><br />So the problem is ,how to limit the size of a file or buffer? , I  started with   buffers  as they are more easy to implement.  I assumed that the Write method of the Buffer struct which writes to the buffer , will panic with ErrTooLarge error if buffer size is above it’s capacity , which i hoped to catch using recover builtin function.<br /><br />This is the code snippet below.<br /><br />defer func() {<br />      if r := recover(); r != nil {<br />         fmt.Println(&ldquo;Should catch if anyone panics&ldquo;)<br />      }<br />   }()<br />  a := bytes.NewBuffer(make([]byte , 0 , 2))<br />  for {<br />    _ ,err := a.Write([]byte(&ldquo;create boom&ldquo;))<br />    if err != nil {<br />      fmt.Println(err.Error())<br />       return<br />    }<br /><br />  }<br /><br />On running this code , my system was frozen and crashed a little later. This is not what i expected , On further investigation by looking to source code and reading the bytes package documentation again , i found out that Write method in the bytes package is growing the capacity of the  buffer if the buffer capacity is not enough , which in turn is increasing the amount of memory and resources used by the system.<br /><br />After some googling and with good help from the go community(thanks to dave cheney) , i decided  to create wrapper around the buffer struct and implement my own io.Writer interface by implementing Write method for the wrapper which writes to the buffer.<br /><br />My custom wrapper’s will take capacity as parameter when initializing and the Write method will do the required action if there is a buffer overflow , instead of increasing the capacity like the Write method from bytes package. This is done by monitoring the size of the buffer before writing to the buffer.<br /><br />This is code snippet of my custom wrapper.<br /><br />type MyBuffer struct {<br />    cap   int<br />    mybuf *bytes.Buffer<br />}<br /><br />func (b *MyBuffer) Write(p []byte) (n int , err error) {<br />    if len(p)+b.mybuf.Len() &#62;; b.cap {<br />        fmt.Printf(b.mybuf.String())<br />        panic(&ldquo;Buffer Overflow&ldquo;)<br />    } else {<br />        b.mybuf.Write(p)<br />    }<br />    return len(p) , nil<br />}<br /><br />func NewBuffer(buf []byte , cap int) *MyBuffer {<br />    return &amp;amp;MyBuffer{mybuf: bytes.NewBuffer(buf) , cap: cap}<br />}<br /><br />func main() {<br /><br />    defer func() {<br />        if r := recover(); r != nil {<br />            fmt.Println(&ldquo;recover in yes&ldquo;)<br />        }<br />    }()<br /><br />    a := NewBuffer(make([]byte , 0 , 100) , 200)<br />    for {<br />        _ , err := a.Write([]byte(&ldquo;Check for Buffer Overflow&ldquo;))<br />        if err != nil {<br />            fmt.Println(err.Error())<br />            return<br />        }<br />    }<br />}<br /><br />On running this code , it worked as expected , hopefully will be deployed in production.<br />The same goes for files as well.<br /><br />Note: useful links , on docker ,on golang bytes package<br /><br />"
		} ,
	
		{
		  "title" : "Making Airtel 3G dongle work on Mac OS 10.10 Yosemite",
		  "category" : "technology",
		  "url" : "/technology/making-airtel-3g-dongle-work-on-mac-os-10-dot-10-yosemite/",
		  "date" : "2014-12-03 00:00:00 +0530",
		  "content"	: "If you use Airtel 3G Dongle (Mine is Huawei E173) on your Mac , and are having issue using the dongel after upgrading to Yosemite , airtel is of little help. They asked me to downgrade the OS to Mavericks!<br /><br />The reason why the dialer software provided by airtel does not work is , that they internally use Apple USB Modem. According to this FAQ on apple support site , your Operating system should be running in 32 bit mode for the modem to work. Yosemite however , is 64 bit.<br /><br />Anyway , I could find multiple ways to overcome the problem. Here I am writing about the most simple one<br /><br />###Step 1: <br />Click on this link to download the new compitable driver from Huawei website Mac-V200R003B015D11SP00C983(for Mac10.10).rar<br /><br /><br /><br />###Step 2: <br />Open the archive , you will find two files<br /><br />1. Mobile Partner install user guide.docx<br />2. Mobile Partner.zip<br /><br /><br />The word document has detailed instructions with screenshots , on how to install.<br /><br />###Step 3: <br />Open the zip file Mobile Partner.zip , you will find Mobile Partner.app. Double click on this file to install the app<br /><br />###Step 4: <br />Once installed , start the app and go to Tools -&#62;; Options<br /><br /><br /><br />###Step 5: <br />In the Options window , choose “Profile Management” from the left side menu<br /><br /><br /><br />###Step 6: <br />Click on “New” button to create a new profile. Give it a name , such as “airtel 3g”. Also , make sure the “Access Number” is set to *99#. Click “Save” , then “Ok”.<br /><br /><br /><br />###Step 7: <br />Insert your Dongel into an USB port. You should see “Mobile Partner” application starting automatically. Choose the profile you created in Step 6 (“airtel 3g”) and “Connect”.<br /><br /><br /><br />That’s it.<br /><br />"
		} ,
	
		{
		  "title" : "IT Career - Pitfalls to avoid",
		  "category" : "career",
		  "url" : "/career/it-career-pitfalls-to-avoid/",
		  "date" : "2014-09-22 00:00:00 +0530",
		  "content"	: "It is very humbling when a youngster walks up to us and says “Thanks for helping me get my first job”. While we are delighted at one end , we are worried at the other.  Why? Because , most of the time a fledgling mind does not see the disaster ahead! Yes , we mean disaster – 75% of IT professionals of 2011-2014 batches will be unemployed 20 years from now.  And this is assuming IT industry does well !! Looks unlikely? Read on to know more.<br /><br />Indian IT industry has employed around 7 ,50 ,000 professionals from the four batches (2011 , 2012 , 2013 , 2014). An estimated 1 ,50 ,000 of these will leave the Indian IT industry to pursue higher studies and never come back to work for the same industry. That leaves us with 6 ,00 ,000 professionals who will be in the industry for long. The question is: How long? Being highly paid with around 20 years of experience , in the year 2030 , companies would want them to take larger responsibilities and oversee at least 100 professionals under them. Summing it up , these 6 lakh professionals should have 6 crore professionals below them. Assuming IT industry grows at 10% per annum for 20 years (caution - it may already be slowing down) , the whole industry will be just 1.2 crore strong. That means at most 1.2 lakh senior professionals will be needed. What would happen to the rest 4.8 lakh professionals? They would , of course , be unemployed.<br /><br />Difficult to digest? In 1995 , there were approximately 11 ,000 software professionals across all levels. Nearly 50% of them are now citizens of another country or earned enough money from the exponentially growing market (nascent market then) growing market to retire , appropriately called VIP (vested in peace). Another at most 15 ,500 professionals of the same era migrated into IT industry from other industries (like SAP consultants , Supply Chain , Financial professionals). So that is a conservative 21 ,000 senior professionals in the whole of Indian IT industry. Many of these who lose a job today struggle to find another suitable profile (20 Yrs of experience) and this is when growth rates in this period have been over 25%. You can see it happening for the current 40+ year old professionals!!<br /><br />Now you have a lingering doubt – could there be something wrong in the projections? Yes !! But it is unfortunately on the negative side. What if the industry grows slower than 10% (may be another bad patch of no growth for 2-4 years). What if automation makes many more jobs redundant (now in IT itself , think about it!)? Last but not the least , another country taking away jobs from India (like China did in manufacturing)?<br /><br />A tell-a-tale from not very long ago is the textile mills of Bombay. They were teeming with activity and nothing could go wrong for them in 1970s and early 1980s. It could only get better as population was growing and people’s ability to spend was increasing. These very mills today are malls!! It can be argued to be a ‘crowding out’ phenomenon , surely not applicable to sunrise IT industry. Or maybe it is visible only in hindsight !!!<br /><br />What are we doing at eLitmus to help the cause?<br /><br /><br />  <br />    We are pushing companies not to lower the entry barriers. We have found that immediately after a slow down year , quality and quantity of candidates improve. Quantity ok , but how quality? You call it competition , you call it lowered demand or call it buyers market. So if companies can adopt the quality principals in this period , why not in growth phase as well. It will help students also.<br />We strongly believe a youngster`s ability to adapt and evolve is much higher than an older person. So push them today rather than tomorrow. If you remember your grandparents had the fitness and ability to walk kilometres at their old age (not spoilt by automobiles when they were a child)<br />  <br />  <br />    Educating students that the easy path out , though rosy for short term , will destroy them. We want them to go that extra mile. That explains our rigorous question paper which tests fundamentals and concepts. We want them to earn their job rather than get it. In the process their ability goes up. Few students who wrote pH test in the initial years of eLitmus have founded their own start-ups.<br />  <br />  <br />    Ensuring start-ups and companies with great work environment do not struggle for lack of talent. Most of these engagements are loss making. We survive thanks to the fact that most of our colleagues at eLitmus are passionate about what they do and work at a fraction of their market salary!!<br />  <br /><br /><br />We are committed to “making India competitive” and we hope we have challenged the young reader of this blog to go the extra mile. As Steve Jobs once quoted the Whole Earth catalogue “Stay hungry , Stay foolish!”<br /><br />"
		} ,
	
		{
		  "title" : "How we host our blog on GitHub pages and yet serve it from our own Sub-URL",
		  "category" : "technology",
		  "url" : "/technology/how-we-host-our-blog-on-github-pages-and-yet-serve-it-from-our-own-sub-url/",
		  "date" : "2014-08-18 00:00:00 +0530",
		  "content"	: "There are umpteen number of blog posts telling you how to host your static site on GitHub Pages for free. They also tell you how to serve such a site from your own domain name.<br /><br />As you may have guessed , this blog is also hosted on GH Pages. Don’t believe me? try visiting this URL https://shireeshj.github.io/blog/<br /><br />It is easy to map a github.io url such as this , to a subdomain. For example , it is easy to map the url to https://blog.elitmus.com/blog/.  All you need to do is check-in a file named CNAME into the root folder of your git repo that contains your static site.<br /><br />What if you want your static site to be served from domain apex? That is easy too.  GitHub pages help explains this in a simple manner.<br /><br />However , If your domain apex is already taken , say by your other website , you have a problem.  To host your static site on a domain apex (or a sub-url of domain apex) the domain apex should be available exclusively for use by github pages.<br /><br />We had to overcome this very problem , since our business website is already hosted on elitmus.com (and www.elitmus.com).  Given that we are not in great love with subdomains. We had to find a workaround. And here is what we did:<br /><br />Since we use nginx to server our business website , all we had to do was to write a simple traffic-cop rule. What this rule did was , to parse the request url to see if it starts with /blog/. If yes , then the request is reverse proxied to GitHub Pages. If no , then it is served from local disk.<br /><br />The relevant lines from the config file are here<br /><br /><br/>location /blog/ {<br /><br/>    proxy_pass       http://shireeshj.github.io/;<br /><br/>    proxy_redirect off;<br /><br/>    proxy_set_header Host &#60;;shireeshj.github.io&#62;;;<br /><br/>    proxy_set_header X-Host &#60;;shireeshj.github.io&#62;;;;<br /><br/>    proxy_set_header X-Real-IP $remote_addr;<br /><br/>    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;<br /><br/>  }<br /><br /><br />This is how we get free hosting for our blog , yet serve it from our official URL. If GitHub pages ever stops us from reverse proxying , we shall simply spin our own webserver to run this static site and reverse proxy to that web server.<br /><br />"
		} ,
	
		{
		  "title" : "Using Monit to get email alert on unauthorized login",
		  "category" : "technology",
		  "url" : "/technology/using-monit-to-get-email-alert-on-unauthorized-login/",
		  "date" : "2014-06-04 00:00:00 +0530",
		  "content"	: "For a long time , we had our own custom written perl script to alert us whenever someone logged into our production servers from an ip address we do not recognize (not whitelisted). The script looked somewhat like this…<br /><br />#!/usr/bin/perl<br /># script file: alert_on_login.pl<br />#<br />my $login_str = &ldquo;Accepted publickey&ldquo;<br />my $whitelist_ip = &ldquo;122.123.123.111&ldquo;<br /><br />sub sendEmail<br />{<br />        my ($to , $from , $subject , $message) = @_;<br />        my $sendmail = &amp;#39;/usr/lib/sendmail&amp;#39;;<br />        open(MAIL , &ldquo;|$sendmail -oi -t&ldquo;);<br />        print MAIL &ldquo;From: $from&ldquo;<br />        print MAIL &ldquo;To: $to&ldquo;<br />        print MAIL &ldquo;Subject: $subject&ldquo;<br />        print MAIL &ldquo;$message&ldquo;<br />        close(MAIL);<br />}<br /><br />while (&#60;;&#62;;) {<br />        if (grep(/$login_str/ , $_) &amp;amp;&amp;amp; !grep(/$whitelist_ip/ , $_)) {<br />                print $_;<br />                chomp $_;<br />                @arr = split(&amp;#39; &amp;#39; , $_);<br />                sendEmail(&amp;#39;recepient1@elitmus.com , recepient2@elitmus.com&amp;#39; ,<br />                          &amp;#39;monit@elitmus.com&amp;#39; ,<br />                          &amp;#39;Server login from &amp;#39; . $arr[10] ,<br />                          $_);<br />        }<br />}<br /><br />All we needed to do was to run this script in the background as a daemon , and it would send us an email alert whenever someone logged in successfully. As root user start the script like this:<br /><br />  # (perl alert_on_login.pl /var/log/auth.log &amp;amp;)<br /><br /><br />Ever since we started using monit for the usual purpose (monitoring processes) , we have also entrusted monit to do the job of the above perl script. Monit makes this super simple…<br /><br />Monit is a popular opensource process monitoring tool. It is used mostly for monitoring health of any linux process and take necessary action if any of the set parameters are breached. Monit can restart a process if the process failed for some reason. Monit can also notify you of incidents and actions taken.<br /><br />See this to learn more about monit’s alert capabilities.<br /><br />Monit’s global configuration file is usually /etc/monit/monitrc. Here is what monit needs to be told about how to send email alerts:<br /><br />...<br /># This is our SMTP server settings. The complete syntax is<br /># SET MAILSERVER &#60;;hostname [PORT] [USERNAME] [PASSWORD] [using SSLAUTO|SSLV2|SSLV3|TLSV11|TLSV12] [CERTMD5 checksum]&#62;; , ...<br />#          [with TIMEOUT X SECONDS]<br />#          [using HOSTNAME hostname]<br />#<br /># But for our purpose , localhost is good enough<br />SET mailserver localhost<br /><br /># This is the email template for alert messages<br />SET mail-format {<br />  from: monit@elitmus.com<br />  subject: $SERVICE $EVENT at $DATE<br />  message: Monit $ACTION $SERVICE at $DATE on $HOST: $DESCRIPTION.<br />           Yours sincerely ,<br />           monit<br />}<br /><br /># Alerts can be triggered for various reasons. Successful ssh login is just one of those reasons.<br /># Since this is a global configuration , we can tell monit to not send alerts for certain events<br />#  We also specify the email address of the recepient who will receive the alerts<br /><br />set alert recepient1@elitmus.com NOT ON { action , instance , pid , ppid , nonexist }<br />...<br /><br />And then we add this config file ssh_logins.conf specific to sshd related stuff:<br /><br />check file ssh_logins with path /var/log/auth.log<br />  ignore match &ldquo;/etc/monit/whitelist_ips.regex&ldquo;<br />  if match &ldquo;Accepted publickey&ldquo; then alert<br /><br />Notice how we tell monit to ignore logins from known ip addresses. We can now store all whitelist ip addresses in a separate file /etc/monit/whitelist_ips.regex , one address per line.<br /><br />Note: We have disabled password based login and hence do not monitor for passworded logins. If you use passworded login , you should change &quot;Accepted publickey&quot; to &quot;Accepted password&quot;<br /><br />Happy monitoring!<br /><br />"
		} ,
	
		{
		  "title" : "Gotcha's while syntactically translating AES encryption logic from PHP to Ruby",
		  "category" : "technology",
		  "url" : "/technology/gotchas-while-syntactically-translating-aes-encryption-logic-from-php-to-ruby/",
		  "date" : "2014-05-25 00:00:00 +0530",
		  "content"	: "Our Payment Gateway service provider recently launched a new platform with some nice-to-have features. We wanted those features and so we decided to migrate. Being one of the earliest adopters of the new platform , there was no integration kit available. We had to build it ourselves. Not a problem. Since we are a Ruby On Rails shop , we built our own Ruby integration kit. All went well and we pushed it to production.<br /><br />A month or two later , we got an email from our gateway provider seeking our help with writing the encryption and decryption logic for the Ruby integration kit they were developing. We were a little surprised , because we noticed they had already published integration kits for PHP , Python , JAVA etc. How difficult can it be to translate that to Ruby?<br /><br />Turns out , syntactic transalation of code from one programming language to another does not always work. A slightly more deeper knowledge helps. We could almost guess where they were getting stuck.<br /><br />Before we get to the story , some backgroung on the encryption algo will add clarity.<br /><br />For secure communication between our server and the gateway , the prescribed cipher was AES , specifically symmetric-key block cipher with a 128 bit secret key in CBC mode. Since OpenSSL already implements this algo and is avaliable on almost all platforms , most programming languages just bundle a wrapper for OpenSSL.<br /><br />So if its the same OpenSSL that the wrappers call , why couldn’t the gateway service provider translate their own PHP code to Ruby?<br /><br />Here is why:<br /><br />AES works by breaking the plain text (the text to be encrypted) into blocks of 128 bits (or 16 bytes). In CBC mode , each block is XORed with the key to get cipher text of that block. The cipher text of the previous block is used for encrypting the next block… so on and so forth , until all the blocks are encrypted.<br /><br />Note that the length of the cipher text will be exactly same as that of the plain text.<br /><br />The problem occures with the last block. If the length of the plain text is not a multiple of 128. the last block will be shorter than 128 bits. Since the algo can work only on blocks of 128 bits , It is a common practice to pad the last block so that it becomes equal to 128 bits in lenght. This padding is subsequently discarded after decryption.<br /><br />Note: The actual algo is more complicated than this. We have deliberately left out details that are not relevent for this post.<br /><br />This is the encryption method in the PHP integration kit published by the gateway service provider<br /><br />function encrypt($plainText ,$key)<br />{<br />  $secretKey = hextobin(md5($key));<br />  $initVector = &ldquo;...&ldquo;<br />  $openMode = mcrypt_module_open(MCRYPT_RIJNDAEL_128 , &amp;#39;&amp;#39; ,&amp;#39;cbc&amp;#39; , &amp;#39;&amp;#39;);<br />  $blockSize = mcrypt_get_block_size(MCRYPT_RIJNDAEL_128 , &amp;#39;cbc&amp;#39;);<br /><br />  $plainPad = pkcs5_pad($plainText , $blockSize);  //  &#60;;---- Padding<br /><br />  if (mcrypt_generic_init($openMode , $secretKey , $initVector) != -1) <br />  {<br />    $encryptedText = mcrypt_generic($openMode , $plainPad);<br />    mcrypt_generic_deinit($openMode);      <br />  } <br />  return bin2hex($encryptedText);<br />}<br /><br />// Padding method<br />function pkcs5_pad ($plainText , $blockSize)<br />{<br />  // padding logic here<br />}<br /><br />And here is the same implemented in Ruby<br /><br />def self.encrypt(plain_text , key)<br />    secret_key     = Digest::MD5.digest(key)<br />    cipher         = OpenSSL::Cipher::AES.new(128 , :CBC)<br />    cipher.encrypt<br />    cipher.key     = secret_key<br />    cipher.iv      = INIT_VECTOR<br />    encrypted_text = cipher.update(plain_text) + cipher.final<br />    return (encrypted_text.unpack(&ldquo;H*&ldquo;)).first<br />end<br /><br />Notice any difference?<br /><br />It turns out that , unlike in Python , PHP and few other languages , Ruby wrapper for OpenSSL automatically takes care of padding (default behaviour). This is clearly mentioned in the documentation. For some reason , techies at our gateway service provider overlooked this and hit a dead-end.<br /><br />By the they , they were gracious enough to acknowledge our contribution in their Ruby Integration Kit (accessible only to their subscribers)<br /><br />But We have open sourced our code here ‘cca_crypto’. We have plans of make this into a complete package - with view generators etc. , and publish this as a rubygem. We shall gladly accept any pull request!<br /><br />"
		} ,
	
		{
		  "title" : "Setting Up Amazon RDS as a Slave to a self-managed MySQL server",
		  "category" : "technology",
		  "url" : "/technology/setting-up-amazon-rds-as-a-slave-to-a-self-managed-mysql-server/",
		  "date" : "2014-05-21 00:00:00 +0530",
		  "content"	: "Last week , we migrated our MySQL database server , which was running on an EC2 instance , to RDS. We hoped the migration process would be smooth.<br /><br />As always , migrating a large database has its challenges. Business folks expect the minimum possible downtime.<br /><br />The plan was simple.<br /><br /><br />  Launch an RDS instance<br />  Load a full dump into it<br />  Configure it to act as a slave of the self-managed server (current master)<br />  On the D-day , pull the website down and promote the RDS instance to take over as the new master<br /><br /><br />We soon discovered that RDS comes with curtailed root permissions. There are several commands that are disallowed. Some of these include “CHANGE MASTER TO….”<br /><br />What do we do now?<br /><br />One option was to carry out the migration in one go , while the website was offline. This meant the downtime would have been several hours , instead of minutes. Obviously , not an acceptable option at all.<br /><br />Some R&amp;amp;D was all it took to discover how to proceed with the original approach.<br /><br />RDS comes with a bunch of stored procedures , which help you configure it as a slave. There is almost a one-to-one mapping of these stored procedures with the commands that are disallowed.<br /><br /><br />MySQL CommandCorrosponding Stored Proc<br />CHANGE MASTER TOmysql.rds_set_external_master<br />START SLAVEmysql.rds_start_replication<br />STOP SLAVEmysql.rds_stop_replication<br />RESET MASTERmysql.rds_reset_external_master <br /><br /><br />So , Using these stored procedures , you can now configure your RDS instance as a slave to your self-managed MySQL server<br /><br />After loading a full dump to RDS , Call the stored procedure mysql.rds_set_external_master like this<br /><br />CALL mysql.rds_set_external_master ('servername' , port , 'user' , 'password' , 'binlog-file' , binlog-offset , 0);<br /><br /><br />Then<br /><br />CALL mysql.rds_start_replication;<br /><br /><br />This will make RDS a slave of your self managed mysql server. You can run “SHOW SLAVE STATUS” to see its working.<br /><br />When it is time to promote RDS to master. You call these stored procedures<br /><br />CALL mysql.rds_stop_replication;<br /><br />CALL mysql.rds_reset_external_master;<br /><br /><br />That’s it. Now point your applications to the RDS instance and take your site live.<br /><br />Note:<br /><br />For your RDS to work as a slave , it needs permissions to connect to port 3306 of your current master. Make sure you open this port for the RDS instance.<br /><br />You can run the following command to find out the ip address of your rds instance<br /><br />ping -c rdsname.cpesx66wwe7y.ap-southeast-1.rds.amazonaws.com<br /><br />"
		} ,
	
		{
		  "title" : "Beware of creating $HOME/.ssh folder by hand, when SELinux is turned on",
		  "category" : "technology",
		  "url" : "/technology/beware-of-creating-ssh-folder-by-hand-when-selinux-is-turned-on/",
		  "date" : "2012-07-22 00:00:00 +0530",
		  "content"	: "I was experimenting with chef to manage our Linux boxes. As a standard practice , our application user deployer is homed in /applications/deployer rather than the usual /home/deployer.<br /><br />To enable password less login , I appended my public key to ~/.ssh/authorized_keys<br /><br /> ssh-copy-id -i ~/.ssh/id_rsa deployer@remote.server<br /><br /><br />The first time I run this command , I will be prompted for a password to install my key. After this , I can run the below command to login without a password:<br /><br />ssh -i ~/.ssh/id_rsa deployer@remote.server<br /><br /><br />However , that did not work as expected.<br /><br />For some reason , sshd was unable to read the authorized_keys file. I checked all the usual things.. all looked fine. Everything seem to work just fine when SELinux was running in permissive mode on the remote server , but not when it was in enforcing mode.<br /><br />Discovered that if .ssh folder was created by hand (or even the folder containing .ssh folder) , we need to do few additional things.<br /><br />Step 1:<br /><br />Open this file /etc/selinux/targeted/contexts/files/file_contexts.homedirs and append the following line to the bottom<br /><br /> /applications/deployer/[^/]*/ssh(/.*)?     system_u:object_r:ssh_home_t:s0<br /><br /><br />Note: remember to adjust the path as per your needs.<br /><br />Step 2: run the following command<br /><br />restorecon -R -v /applications/deployer/.ssh<br /><br /><br />Again , remember to adjust the path as per your needs.<br /><br />Now you are all set!<br /><br /> ssh -i ~/.ssh/id_rsa deployer@remote.server<br /><br /><br />should log you in without asking for a password!<br />"
		} ,
	
		{
		  "title" : "Importance of Date field in an email's Header",
		  "category" : "technology",
		  "url" : "/technology/importance-of-date-field-in-an-emails-header/",
		  "date" : "2012-04-04 00:00:00 +0530",
		  "content"	: "So far , we paid little attention to email delivery issues. We knew delivering to rediffmail is a pain. So we discouraged our users from using rediffmail. Apart from that we had FCrDNS and SPF configured and working fine. We had also configured DKIM. And then a month ago , we also added DMARC in monitor mode.<br /><br />We were happy! Until…<br /><br />Recently , we started getting loads of phishing emails from what appeared to originate from our own domain name [not our servers].<br /><br />It told us two things.<br /><br /><br />  eLitmus.com was growing in popularity<br />  We cannot ignore email delivery issue any longer<br /><br /><br />We ran our email through Spam Assassin checks and were surprised to see that we got a score of 6. Anything above 5 is BAD. It’s a straight spam! But we knew we were not spamming. These were transactional emails triggered by our website on certain events , such as New registration , or Forgot Password.<br /><br />It was almost by accident , we noticed that the timezone in the Date header of the email was appearing as +0580. Indian Standard Time (IST) is 5 hours and 30 minutes ahead of UTC. So this value should have been +0530 , not +0580. Apparently , that is good enough reason for Spam Assassin to treat our mails as spam.<br /><br />Tracing backwards , we discovered a bug in our application code and fixed it. It was a single line fix.<br /><br />With this change , Spam Assassin was happy to give us a score of zero.<br /><br />That is just one part of one header. There are ten others which have to be configured correctly.<br /><br />Here is an article with good insights in to how gmail calculates sender reputation. Its a little dated , but still relevent. Sender reputation in a large webmail service (PDF)<br /><br />By the way , here is a nice and free JSon API to check your email’s reputation.<br /><br />"
		} 
	
]