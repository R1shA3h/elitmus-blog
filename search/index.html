<!-- 	this file came from cloud cannon's course
		link ti git repo: https://github.com/CloudCannon/bakery-store-jekyll-template/tree/lunrjs

		this file displays the results of search query
		this file is only for testing purposes
		delete this file after development and testing is finished
-->

<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
<title>eLitmus Blog</title>
<meta name="description" content="We learn new things everyday, and we document our learnings here...">



<!-- Twitter Cards -->
  <meta name="twitter:card" content="summary">    
<meta name="twitter:title" content="eLitmus Blog">
<meta name="twitter:description" content="">
<meta name="twitter:site" content="@elitmus">

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    

    




<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:image" content="/blog/images/blog_logo_taglinedorange.png"/>
<meta property="og:type" content="article">
<meta property="og:title" content="eLitmus Blog">
<meta property="og:description" content="">
<meta property="og:url" content="/blog/search/">

<meta property="og:site_name" content="eLitmus Blog">







<link rel="canonical" href="https://www.elitmus.com/blog/search/">
<link href="/blog/feed.xml" type="application/atom+xml" rel="alternate" title="eLitmus Blog Feed">


	<link rel="author" href="https://plus.google.com/+eLitmus?rel=author">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Icons -->
<!-- apple touch icons -->
<link rel="apple-touch-icon" sizes="57x57" href="/blog/images/apple-touch-icon-57x57.png">
<link rel="apple-touch-icon" sizes="114x114" href="/blog/images/apple-touch-icon-114x114.png">
<link rel="apple-touch-icon" sizes="72x72" href="/blog/images/apple-touch-icon-72x72.png">
<link rel="apple-touch-icon" sizes="144x144" href="/blog/images/apple-touch-icon-144x144.png">
<link rel="apple-touch-icon" sizes="60x60" href="/blog/images/apple-touch-icon-60x60.png">
<link rel="apple-touch-icon" sizes="120x120" href="/blog/images/apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" sizes="76x76" href="/blog/images/apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" sizes="152x152" href="/blog/images/apple-touch-icon-152x152.png">
<!-- Favicons default 16x16 -->
<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="/blog/favicon.ico">

<!-- Favicons others -->
<link rel="icon" type="image/png" href="/blog/images/favicon-196x196.png" sizes="196x196">
<link rel="icon" type="image/png" href="/blog/images/favicon-160x160.png" sizes="160x160">
<link rel="icon" type="image/png" href="/blog/images/favicon-96x96.png" sizes="96x96">
<link rel="icon" type="image/png" href="/blog/images/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/blog/images/favicon-32x32.png" sizes="32x32">

<!-- Icons for Windows 8 Tiles -->
<meta name="msapplication-TileColor" content="#ffffff"/>
<meta name="msapplication-TileImage" content="/blog/images/mstile-144x144.png">

<!-- For all browsers -->


<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:600,300,300italic|The+Girl+Next+Door" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Source+Code+Pro:300' rel='stylesheet' type='text/css'>

<meta http-equiv="cleartype" content="on">

<!-- bootstrap 4 -->
<link rel="stylesheet" type="text/css" href="/blog/assets/css/bootstrap.min.css">

<!-- below css file is used for navbar in individual blogposts -->
<link rel="stylesheet" type="text/css" href="/blog/assets/css/main.min.css">

<!-- <link rel="stylesheet" href="//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css"> -->

<!-- Optional theme -->
<link rel="stylesheet" type="text/css" href="/blog/assets/css/bootstrap-theme.min.css">
<meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Font Awesome -->
<link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="/blog/assets/css/font-elitmuslogo.css" />

<link rel="stylesheet" href="/blog/assets/css/customize.css">
<link rel="stylesheet" href="/blog/assets/css/forkit.css">
		<meta charset="utf-8">
		<title></title>
		<link rel="stylesheet" href="/blog/assets/css/style.css">
		<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,700" media="all">
	</head>
	<body id="page">
		<header class="headroom">

  <nav class="navbar navbar-expand-md navbar-dark bg-black fixed-top" role="navigation">

      <a class="navbar-brand" href="/blog/">
        <img src="https://cdn0.elitmus.net/assets/elitmus-only-logo-6efc8a74032179afdb89829a093e05bd05d627dd4354bbd1ff7f4eb5db8a0af8.svg" class="img-logo">
        <span><div class="blog-label">blog</div></span>
      </a>

      <!-- the toggle button -->
      <button type="button" class="navbar-toggler" data-toggle="collapse" data-target=".navbar-collapse">
        <span class="navbar-toggler-icon"></span>
        <span class="sr-only">Toggle navigation</span><!-- only for screen readers -->
      </button>

      <div class="collapse navbar-collapse">
        <ul class="nav navbar-nav mr-auto">

          <!-- first item = dropdown search -->
          <li class="nav-item">
            <div class="dropdown search-box-margin">

            
              <div class="form-inline">
                 

                <input data-toggle="dropdown" placeholder="Search..." id="search-box" class="form-control form-control-sm search-input dropdown-toggle" aria-haspopup="true" aria-expanded="false">


                <div class="dropdown-menu" id="search-dropdown">
                  <a role='presentation' class='dropdown-item search-results'> Type more.... </a>
                </div>

              </div>



            <!-- the below line will pass "search word" to search.html -->

            <!--
            <form class="form-inline" action="/blog/search" method="get">
               


              <label for="search_box">Search</label>
              <input data-toggle="dropdown" type="text" name="query" placeholder="Search..." id="search-box" class="form-control form-control-sm search-input dropdown-toggle" aria-haspopup="true" aria-expanded="false">

              <div class="dropdown-menu" id="search-dropdown">
                  <a role='presentation' class='dropdown-item search-results'> Type more.... </a>
              </div>

              <input type="submit" value="search">

            </form>
            -->

            <!-- end of form -->


            </div>
          </li>
        </ul>

        

        <!-- every link on the right side -->
        <ul class="nav navbar-nav navbar-right nav-icon-size-reduce">
          
          <li class="nav-item">
             
              <a href="/blog/" class="nav-link">
                <i class="fa fa-home"></i> Home
              </a>
            
          </li>
          
          <li class="nav-item">
             
              <a href="/blog/posts/" class="nav-link">
                <i class="fa fa-file-text-o"></i> Posts
              </a>
            
          </li>
          
          <li class="nav-item">
             
              <a href="//www.elitmus.com/" class="nav-link">
                <i class="el el-elitmuslogo"></i> eLitmus
              </a>
            
          </li>
          

          <!-- feed is the last link -->
          <li class="nav-item">
            <a href="/blog/feed.xml" class="nav-link" title="Atom/RSS feed">
              <i class="fa fa-rss"></i> Feed
            </a>
          </li>
        </ul>



      </div> <!-- /.navbar-collapse -->

  </nav><!--[if lt IE 9]><div class="upgrade"><strong><a href="https://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
</header>
<!-- 

  <div class="search-wrapper">
    <div class="search-form">
      <input type="text" class="search-field" placeholder="Search...">
      <i class="icon-remove-sign icon-2x"></i>
      <ul class="search-results post-list"></ul> -->
<!-- /.search-results -->
<!-- </div> -->
<!-- /.search-form -->
<!-- </div> -->
<!-- ./search-wrapper -->
<!--  -->
		<div class="content">

			<div class="search">
				<form action="#" method="get">
  <label for="search-box">Search</label>
  <input type="text" id="search-box" name="query">
  <input type="submit" value="search">
</form>

<ul id="search-results"></ul>

<script>
  window.store = {
    
      "technology-puma-from-daemonization-to-process-control-with-systemctl-and-monit": {
        "title": "Puma: From Daemonization to Process Control with Systemctl and Monit",
        "author": "nikhil",
        "category": "technology",
        "content": "Puma is a popular Ruby web server that is known for its speed and scalability. It has undergone significant changes in recent versions(starting 5.0.0). One of the most notable alterations is the removal of the daemonization feature. But what does it mean?Daemonization, in the context of web servers, is a process that allows a program to run in the background as a system service. In older versions, Puma made it simple for users to daemonize their processes with a straightforward configuration snippet:#config/puma.rbdaemonizeHowever, in recent versions, attempting to use the daemonize code will result in an error, as this functionality has been removed from the  codebase.Why daemonization should not be part of gem?Incorporating daemonization directly within a gem can lead to undesirable consequences: as explained by Mike Perham in a Blog Post. Here are some key points that should be considered -  Complexity: Adding daemonization features to a gem can make its code more complex and challenging.  Maintenance: The responsibility of maintaining daemonization, automatic restart, and similar core features becomes an additional burden.  Efficiency: System processes are better equipped to manage tasks like daemonization. Delegating this function to the system ensures more efficient and reliable execution, rather than embedding it within the gem.As a result of these considerations, Puma decided to remove the daemonization feature from the gem.This decision led us to make some changes in our setup to ensure the smooth running of our applications.Using SystemdWe had previously implemented daemonization for Sidekiq, which was a process similar to Puma’s needs. Although there were some minor adjustments required for Puma. Here are steps to achieve daemnization through systemctl:  Remove daemonization from config/puma.rb file      Create a file in /lib/systemd/system/puma.service. Below is sample systemd service configuration example, modify it according to your needs.          [Unit]      Description=Puma HTTP Server      After=network.target      [Service]      Type=notify      User=username      WorkingDirectory=/dir/path      ExecStart=/bin/pumactl start -F /path/puma_config --environment env      ExecStartPost=/bin/sh -c &#39;/bin/echo $MAINPID &gt; /usr/myapp/shared/pids/puma.pid&#39;      ExecStop=/bin/kill -TSTP $MAINPID      RestartSec=10      Restart=on-failure      [Install]      WantedBy=multi-user.target            Two prominent Puma restart strategies are Phased and Hot restarts. Phased restarts are slower but ensure that all workers finish their existing requests before restarting the server, while Hot restarts are faster but come with increased latency during the restart.     To initiate Puma with a phased restart, you can pass the phased-restart option. This choice offers flexibility to adapt Puma's behavior according to specific needs. More about puma restarts Here.        Monit configurations    Monit is a utility for managing and monitoring processes, programs, files, directories and filesystems on a Unix system Monit Docs.     Updated monitrc file        check process puma with pidfile &quot;/usr/myapp/shared/pids/puma.pid&quot;      start program = &quot;/bin/bash -l -c &#39;sudo systemctl start puma&#39;&quot; with timeout 20 seconds      stop program = &quot;/bin/bash -l -c &#39;sudo systemctl stop puma&#39;&quot; with timeout 20 seconds      if totalmem is greater than 800 MB for 3 cycles then restart      if cpu is greater than 65% for 2 cycles then exec &quot;/etc/monit/slack_notifier.sh&quot; else if succeeded then exec &quot;/etc/monit/slack_notifier.sh&quot;            To check if puma is running correctly follow the commands.        ps aux | grep puma    sudo monit summary      Exploring Other AlternativesAs alternative to this we considered using puma-daemon gem, which essentially replicated the removed code and maintained it in a separate gem. However, after careful consideration, we chose not to adopt this alternative for the following reasons:  Violation of system standards.  Additional gem and maintainence burden.SummaryWhile the removal of daemonization from Puma may require some adjustments, it aligns with the best practices of modern web server management Managing processes at the system level, using tools like systemd and Monit, is considered a more efficient and maintainable approach. Daemonizing processes within application code is discouraged, as it’s a task that falls under the system level. Ultimately, the shift towards system-level process management ensures the stability and efficiency of web applications.",
        "url": "/technology/puma-from-daemonization-to-process-control-with-systemctl-and-monit/"
      }
      ,
    
      "technology-demystifying-rails-7-system-tests-configuring-ci-pipeline": {
        "title": "Demystifying Rails 7 System Tests: Configuring CI Pipeline",
        "author": "manish",
        "category": "technology",
        "content": "In Rails 5.1 and later versions, system tests were introduced as a new type of test to simulate a user interacting with a web application. These tests use a headless browser, typically powered by Capybara and a WebDriver, to mimic a user’s actions like clicking buttons, filling forms, and navigating through the application.Why do we need System Tests?  System tests let you test applications in the browser. Because system tests use a real browser experience, you can test all of your JavaScript easily from your test suite.  Typically used for:    Acceptance testing: verify that the app has implemented a specific featureSmoke testing: verify that the app is functional on a fundamental level and doesn't have code issues.Characterization testing: is a type of software testing that involves examining and documenting the behavior of an existing system or application without making any modifications to its code  How we can run System Test?  System Test interacts with your app via an actual browser to run them.  From a technical perspective, system tests aren’t necessarily required to interact with a real browser; they can be set up to utilize the rack test backend, which emulates HTTP requests and processes the HTML responses. While system tests based on rack_test run faster and more dependable than front-end tests involving an actual browser, they have notable limitations in mimicking a genuine user experience as they are incapable of executing JavaScript.The Anatomy of a System Test?  Minitest    Minitest is a small and incredibly fast unit testing framework.It provides the base classes for test cases.  For Rails System Tests, Rails provides an ApplicationSystemTestCase base class which is in turn based on  ActionDispatch::SystemTestCase:    require &quot;test_helper&quot;  class ApplicationSystemTestCase &lt; ActionDispatch::SystemTestCase    driven_by :selenium, using: :chrome, screen_size: [1400, 1400]  end    In ActionDispatch::SystemTestCase we require the capybara/minitest library.  It provides basics assertions like assert_equal, assert_nil, assert_same, assert_raises, assert_includes.  A runner to run the tests and report on their success and failure.        Capybara      Capybara starts your app in a separate process before running the tests. This ensures that the tests are run against the correct version of your app.  Capybara provides a high-level API that makes it easy to write tests in a natural way. For example, you can write a test that says \"click the button\" instead of having to write code to find the button and click it.  Here is an example of a test written with Capybara's DSL (Domain Specific Language):    visit(&#39;/login&#39;)  fill_in(&#39;email&#39;, with: &#39;user@example.com&#39;)  fill_in(&#39;password&#39;, with: &#39;password&#39;)  click_button(&#39;Login&#39;)        Selenium-Webdriver    Capybara uses the Selenium Webdriver library to interact with real browsers. Selenium WebDriver is a cross-platform library that provides a way to control web browsers from code. Capybara uses Selenium WebDriver to translate its high-level DSL (Domain Specific Language) into low-level commands that the browser can understand.    require &quot;selenium-webdriver&quot;  driver = Selenium::WebDriver.for :firefox  driver.navigate.to &quot;http://google.com&quot;  element = driver.find_element(name: &#39;q&#39;)  element.send_keys &quot;Hello WebDriver!&quot;  element.submit  puts driver.title  Driver.quit    You can see how it’s a bit lower-level than the Capybara example further up. The selenium-webdriver library translates these calls into WebDriver Protocol, which it speaks to a webdriver executable.        Webdriver Protocol    The Selenium WebDriver library translates its calls into the WebDriver Protocol. The WebDriver Protocol is a HTTP-based wire protocol that is used to communicate between the Selenium WebDriver library and the web browser.In order to start a chrome browser window and navigate to google.com. We need to startup geckodriver.We send it a “new session” command with a HTTP post request    curl -X POST &#39;http://127.0.0.1:9515/session&#39; -d &#39;{&quot;capabilities&quot;:{&quot;firstMatch&quot;:[{&quot;browserName&quot;:&quot;firefox&quot;}]}}&#39;    This return a session id along with data    { ... &quot;sessionId&quot;:&quot;f1776ba558e28309299dc5f62864e977&quot; ... }    Then we make another post request with a session id. And url in data parameters    curl -X POST &#39;http://127.0.0.1:9515/session/f1776ba558e28309299dc5f62864e977/url&#39; -d &#39;{&quot;url&quot;: &quot;https://google.com&quot;}&#39;    Webdriver    Webdriver is a tool that speaks “Webdriver protocol” and controls the browser.Every major browser there is an associated webdriver tool. Chrome has chromedriver. Firefox has a geckodriver. MS Edge has edgedriver. Safari has safaridriver.WebDriver tools act as servers: when you execute them, they start a persistent process that listens for HTTP requests until it is terminated.    Webdrivers gem    Before selenium-webdriver 4.11, webdrivers gem automatically determines which WebDriver executable needs to be downloaded for your platform and selected browser, downloads it, and arranges for that executable to be used by selenium-webdriver.From version 4.11, they have incorporated the functionality in selenium-webdriver gem using selenium-manager.  Running Rails 7 System Tests with Docker and Gitlab Runner on Arm64 and Amd64 linux machinesStep 1: Prepare the Rails 7 application for testing  Run the command below to generate a very basic Ruby on Rails 7 app:rails new minitest-rails-app  Go ahead and open up the project in your favourite editor and proceed to the Gemfile, specifically to the test block:  group :test do    # Use system testing [https://guides.rubyonrails.org/testing.html#system-testing]    gem &quot;capybara&quot;    gem &quot;selenium-webdriver&quot;    gem &quot;webdrivers&quot;  end    Next, let’s do a quick scaffold generation to have something to work with:  rails generate scaffold Blog title:string body:text    Usually, generating a scaffold will automatically generate the application_system_test_case.rb and everything you need for the system tests  application_system_test_case.rb (default)     require &quot;test_helper&quot;    class ApplicationSystemTestCase &lt; ActionDispatch::SystemTestCase    driven_by :selenium, using: :chrome, screen_size: [1400, 1400]  end    Run the database commands  rails db:setup  rails db:migrate    Running a Basic System For the First Time  rails test:system  Step 2: Exclude the gem webdrivers from the list of dependencies  Before selenium-webdriver 4.11, webdrivers gem automatically download webdriver executable.  From version 4.11, they have incorporated the functionality in selenium-webdriver gem using selenium-manager.  We can comment out the webdrivers line from Gemfile.  After change, Gemfile looks like this  group :test do  # Use system testing [https://guides.rubyonrails.org/testing.html#system-testing]  gem &quot;capybara&quot;  gem &quot;selenium-webdriver&quot;, &quot;~&gt; 4.11&quot;  #gem &quot;webdrivers&quot;  end  Step 3: Point the Selenium-webdriver to use the firefox browser  As chrome has not released binary compatible with linux/arm64 machine. So the test failed on the arm64 linux machine. I tried multiple approaches to make it work with headless_chrome, but didn’t work and commend the issue in details in this  issue tracker  We need to change the browser to the firefox.  #application_system_test_case.rb (change driver to Firefox)   require &quot;test_helper&quot;    class ApplicationSystemTestCase &lt; ActionDispatch::SystemTestCase    driven_by :selenium, using: :firefox, screen_size: [1400, 1400]  end  Step 4: Prepare the docker image  Create Dockerfile  FROM ruby:3.1.2-slim-buster  RUN apt-get update  RUN apt-get -y install gnupg curl wget xvfb unzip  ENV NODE_VERSION 19  RUN curl -fsSL https://deb.nodesource.com/setup_${NODE_VERSION}.x | bash -  &amp;&amp; \\  apt-get install --yes nodejs &amp;&amp; \\  apt-get install --yes libxss1 libappindicator1 libindicator7 python2  RUN apt-get update &amp;&amp; \\  apt-get install --yes software-properties-common build-essential libssl-dev sqlite3 libsqlite3-dev pkg-config ca-certificates firefox-esr  RUN apt-get install -y git-all  RUN npm install yarn -g  ADD . /data        This Dockerfile sets up an image with Ruby 3.1.2 and Node.js 19 installed. It installs system dependencies like Git, Yarn, various libraries for sqlite and Firefox.        Build Docker image    docker buildx build -t dockermanishelitmus/systemtest-rails-app:latest1.0 . --platform linux/amd64,linux/arm64 --push    Command is building a Docker image using the buildx extension, targeting two different platforms (Intel/AMD 64-bit and ARM 64-bit), tagging the image as latest1.0, and pushing the resulting image to a container registry.Step 5: Prepare the gitlab-runner  In the project root directory create a file .gitlab-ci.yml with contentimage: &quot;dockermanishelitmus/systemtest-rails-app:latest1.0&quot;services: - redis:latestvariables: RAILS_ENV: &quot;test&quot;cache: paths:   - vendor/ruby   - node_modules/before_script: - gem install bundler  --no-document - bundle config set force_ruby_platform true - bundle install - bin/rake db:drop - bin/rake db:setup - bin/rake db:migratestages: - testsSystemTests: stage: tests script:   - yarn install   - bin/rake assets:precompile   - bin/rails test:system artifacts:   when: on_failure   name: &quot;$CI_JOB_NAME-$CI_COMMIT_REF_NAME&quot;   paths:     - coverage/   expire_in: 1 day  Finally run your test suitegitlab-runner exec docker SystemTests  Output  $ bin/rails test:system  Running 4 tests in a single process (parallelization threshold is 50)  Run options: --seed 13031  # Running:  Capybara starting Puma...  * Version 5.6.7 , codename: Birdie&#39;s Version  * Min threads: 0, max threads: 4  * Listening on http://127.0.0.1:33385  ....  Finished in 7.865541s, 0.5085 runs/s, 0.5085 assertions/s.  4 runs, 4 assertions, 0 failures, 0 errors, 0 skips  Saving cache for successful job  Creating cache SystemTests/main...  WARNING: vendor/ruby: no matching files. Ensure that the artifact path is relative to the working directory  node_modules/: found 2 matching files and directories  No URL provided, cache will not be uploaded to shared cache server. Cache will be stored only locally.  Created cache  Job succeededConclusionNow we have a setup that enables us to run system tests in both arm64 and amd64 linux machines with minimal customizations we may want to add. A few tips and tricks should help to get your first system tests up and running in CI pipeline.",
        "url": "/technology/demystifying-rails-7-system-tests-configuring-ci-pipeline/"
      }
      ,
    
      "technology-building-a-frontend-scoring-engine-automating-frontend-evaluation": {
        "title": "Building a Frontend Scoring Engine: Automating Frontend Evaluation",
        "author": "bhushan",
        "category": "technology",
        "content": "The frontend scoring engine is a powerful tool designed to assess the frontend skills of candidates based on code quality, responsiveness, and functionality. It aims to streamline the evaluation process for frontend development by automating the assessment of code quality, best practices, and functionality.What you’ll learn from this blogIn this blog, we will dive into the technical aspects of building a frontend scoring engine.  The need for frontend scoring engine in today’s technology landscape.  The technical requirements gathering and Research phase involved.  Generation of Test script for Test automation using Puppeteer.  Dockerizing the Application.  Features and Process of building the application.Need for the Frontend Scoring EngineIn today’s technology-driven world, the demand for skilled frontend developers is at an all-time high. With the rapid evolution of web applications and user interfaces, companies are constantly seeking talented individuals who can create visually appealing, intuitive, and responsive frontend experiences. However, evaluating frontend development skills can be a complex and time-consuming task. This is where a frontend scoring engine comes into play Automating the Evaluation Process, Measurement of Code Quality and Ensuring Mobile Responsiveness. By allowing users to input HTML, CSS and JavaScript code, and generating scores based on predefined test cases, the scoring engine provides a comprehensive evaluation of candidates’ frontend skills.Research WorkBefore starting the implementation of the frontend scoring engine project, extensive research was conducted to understand the need for such a system, evaluate existing systems, explore testing tools, and plan the evaluation process. This research phase played a crucial role in shaping the project and ensuring its successful execution. Let’s take a brief look on highlight and the key areas of research conducted during the project’s inception.  Evaluating Existing Systems :To gain insights into the existing solutions available in the market, a comprehensive evaluation of similar systems was conducted. Various frontend scoring engines, online code editors were explored to understand their features, functionalities, strengths, and weaknesses. This evaluation provided valuable insights that influenced the design decisions and feature set of the new scoring engine. Some similar existing systems:          Codier.io      Frontend Mentor      CSS Battle      Algoexpert.io Frontend        Testing Tools and Technologies :During our research, we explored various testing tools and technologies to find the perfect fit for executing test cases, assessing code quality, and evaluating frontend functionalities. The evaluation revolved around factors like capabilities, ease of use, and compatibility with our project requirements. Tools such as Selenium, Cypress, Jest, csslint, eslint were taken into consideration.Read more about the tools:          Selenium      Cypress      Jest        Puppeteer :Puppeteer was chosen over Selenium primarily due to its compatibility with Docker and its ability to control headless Chrome or Chromium instances. Docker provides an efficient and scalable environment for running tests, and Puppeteer seamlessly integrates with Docker containers. Additionally, Puppeteer offers a more modern and concise API, making it easier to write test scripts and perform browser automation tasks.          Puppeteer vs Selenium      Puppeteer Docs        Docker Integration :We explored the benefits of Docker, a widely-used containerization platform, and discovered how it could greatly enhance our project. Docker allows us to create lightweight, portable, and isolated containers, which provide a consistent and reproducible environment. Leveraging Docker, we encapsulated and ran our scoring engine, testing tools, and other dependencies, ensuring seamless integration and efficient execution. We pulled various Docker images from Docker Hub, enabling us to set up the required tools effortlessly.          csslint      eslint      jest        Real-Time Code Editor :To provide a user-friendly and real-time code editing experience, we started searching for frontend code editors and existing projects available on GitHub. Various code editor projects were evaluated, and their source code were studied to understand the implementation details. This research helped in selecting the most suitable code editor framework and implementing it within our frontend scoring engine.           Codepen      Fronteditor      CodeG            Problem Statement and Test Case Creation :The goal was to design problem statements that accurately reflect real-world frontend development challenges and create test cases that thoroughly evaluate candidates’ code. Puppeteer test scripts were written to simulate user interactions, perform assertions, and capture screenshots for image comparison using the PixelMatch JavaScript library.    Cloud Deployment and Infrastructure :For our final Deployment and integration Amazon Web Services (AWS) was choosen. The research covered various AWS services, including EC2 instances for hosting the scoring engine, S3 for storage, and other relevant services for infrastructure setup. The deployment process, security considerations, and scaling options were thoroughly explored to ensure a robust and scalable deployment architecture.Test Script GenerationIn the frontend scoring engine, we ensure evaluation of user-submitted HTML, CSS, and JavaScript code by subjecting it to comprehensive testing against predefined test cases. These tests are designed to assess the code quality, functionality, and adherence to best practices, providing a total assessment of candidates’ frontend development skills. By conducting these thorough evaluations, we can accurately determine the proficiency of developers in creating efficient and reliable frontend solutions. Throughout this section, you’ll get an overview of the various types of tests performed, explaining their significance in evaluating code quality and functionality.      Heading/Element TestingThis test focuses on ensuring the presence and correctness of specific HTML elements within the user’s code. Test cases are designed to check if required headings, such as h1, h2, p or specific elements identified by ID or class, are present. The purpose of this test is to assess the structure and semantic correctness of the user’s HTML code.        CSS Properties TestingThis test aims to verify the correct usage of CSS properties in the user’s code. It includes checking for the presence of essential CSS properties, such as margin, padding, font-size, or specific properties required for a particular problem statement. This test ensures that the user’s code adheres to the defined CSS requirements and best practices.        Form Validation TestingForm validation testing focuses on assessing the user’s code for proper form validation techniques. Test cases can include checking for required fields, validating email formats, enforcing password complexity, or implementing custom validation logic. This test ensures that the user’s code handles form validation correctly and provides appropriate error messages.        Function TestingThis test evaluates the functionality and correctness of JavaScript functions implemented by the user. Test cases are designed to cover different scenarios and edge cases to ensure that the functions perform as expected. This test assesses the user’s ability to write functional and efficient JavaScript code.        API TestingAPI testing involves verifying the integration of API calls in the user’s code. Test cases may include checking if an API request is made, handling the API response correctly, and displaying the data from the API on the page. This test ensures that the user’s code effectively interacts with external APIs.        Button TestingButton testing focuses on evaluating the behavior and interactivity of buttons implemented by the user. Test cases may include checking if a button triggers a specific action, updates the UI, or performs a navigation action. This test ensures the proper functionality of user-defined buttons.        Redirection TestingThis test aims to assess the behavior of navigation and redirection implemented by the user’s code. Test cases may include checking if clicking a link or a button redirects the user to the correct page or if the page refreshes as intended. This test ensures that the user’s code correctly handles navigation and redirection scenarios.  Dockerizing the Puppeteer with Chrome Browser SupportDockerfile:# Use the node:slim base imageFROM node:slim# Set an environment variable to skip Puppeteer Chromium download during installationENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD trueRUN apt-get update &amp;&amp; apt-get install gnupg wget -y &amp;&amp; \\ wget --quiet --output-document=- https://dl-ssl.google.com/linux/linux_signing_key.pub | gpg --dearmor &gt; /etc/apt/trusted.gpg.d/google-archive.gpg &amp;&amp; \\ sh -c &#39;echo &quot;deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt;&gt; /etc/apt/sources.list.d/google.list&#39; &amp;&amp; \\ apt-get update &amp;&amp; \\ apt-get install google-chrome-stable -y --no-install-recommends &amp;&amp; \\ rm -rf /var/lib/apt/lists/\\*# Set the working directory inside the containerWORKDIR /usr/src/app# Copy the package.json file to the working directoryCOPY package.json ./# Install project dependencies using npmRUN npm install# Expose port 3000 to allow access to the app outside the containerEXPOSE 3000# Run the app using the &quot;npm test&quot; command when the container startsCMD [&quot;npm&quot;, &quot;test&quot;]Build command:docker build -t bhushan21z/puppchrome .Publish it to Docker Hub:docker push bhushan21z/puppchrome:tagnamePull commnd:docker pull bhushan21z/puppchromeRun command:docker run -it --rm -v $(pwd)/files:usr/src/app/files puppeteerchromeFeatures and ArchitectureScoring Engine:  Inputs: The scoring engine takes HTML, CSS, and JavaScript files created by users on the client side, as well as the test cases file generated on the backend.  Code Quality Assessment: The engine assesses code quality using ESLint CSSlint and similar tools.  Scoring: The engine generates a score based on code quality, along with the results of the test cases executed on the client-side code.  Modular Architecture: The scoring engine is a separate entity, independent of the frontend and backend code.  Technology Stack: Python Flask framework is used to implement the scoring engine.  Working: Flask runs various Docker run commands to execute test script.Backend:  MySql Database: Schema Created with various tables such as users, questions, testcases and submissions.  Node JS: Express framework is used to implement Rest APIs.  User auth: Contains user register and login APIs.  Questions: Questions create/get APIs.  Test Cases: Testcases create/get APIs and joining it with Questions table with question id as foreign key.  Scoring Engine: POST request to get user data and sending it to scoring engine and returning scoring engine response to frontend.  Submissions: User Submissions create/get APIs and joing it with users table and questions table.Frontend (Admin Side):  Problem Creation: Admins can create problem statements, describing the problem to be solved.  Problem Settings: Problems can include various settings such as score weightage, best practices to check, and mobile responsiveness evaluation.  Test Cases: Admins can add multiple test cases related to each problem statement.  Test Case Visibility: Some test case outputs will be visible to users, while others will be hidden, showing only whether the score passed or failed.  User-Friendly Test Case Creation: Adding test cases are straightforward, even for users with limited programming knowledge.Frontend (Client Side):  Problem List: Users can view a list of problems on their screen.  Code Editor: Users can write HTML, CSS, and JavaScript code for each problem, similar to the CodePen editor.  Code Compilation: Users can compile their code and generate the output.  Score Display: Users can view the scores generated by the scoring engine based on the performed test cases.Tools &amp; TechnologiesFrontend:  ReactJS is used develop the frontend of the scoring engine.Backend:  Node.js is employed for building the backend of the scoring engine.  MySQL is used as the database management system.Scoring Engine  Puppeteer is used for implementing testcases and browser testing.  Docker containers are utilized for testing code quality and running test cases.  Flask is used to make scoring engine server which takes data and interacts with docker.ConclusionBy implementing a frontend scoring engine, we can automate frontend development evaluation, resulting in a streamlined and efficient assessment process. This blog has explored the goals, research, features, technical requirements, and tools and technologies involved in developing a frontend scoring engine. The automation of code assessment, real-time editing, and integration of testing tools have resulted in an efficient and comprehensive evaluation platform. The challenges we faced during development have strengthened our understanding of frontend development and inspired innovative solutions. As we move forward, we remain committed to enhancing the scoring engine to meet the evolving needs of the tech industry. If you have any questions, doubts or suggestions feel free to reach out to me on LinkedIn",
        "url": "/technology/building-a-frontend-scoring-engine-automating-frontend-evaluation/"
      }
      ,
    
      "technology-revamping-elitmus-dot-com-stand-alone-front-end-module": {
        "title": "Revamping eLitmus.com | Stand-Alone Front-end Module",
        "author": "smruti",
        "category": "technology",
        "content": "The current elitmus.com is a web application built with Ruby on Rails Framework, and the views are sent directly from the backend server whenever requested. This was quite good before, but in present scenario of internet and web technologies, these seem to lack some very basic requirements. And Hence, an upgradation is required.Formally, current elitmus.com has a monolithic structure i.e. the front-end and the back-end are tightly coupled together. As a result of this, it is not possible to divide the project’s logic and team for front-end and back-end. Only Full Stack Developers having knowledge of both the domains are required in order to work in this project. This somehow limits the people who are more expertised in one of the domains.Also, the present elitmus.com is not using the latest web technologies available. This greatly impacts the user experience.So, What’s the solution for this ?Well, we can separate the front-end and back-end. This will solve all the problems faced by the developers who work or tends to work in this project. This solves some of the major issues faced today by developers.Now, we can have a distributed system, with the views ( front-end ) in one place and the Models and Controllers in the other. The Front-end we plan to build can be built using the latest and efficient web technologies currently available. This helps to improve the User Experience as well.What Benefits ?  Developer Experience          Team Separation → We can have Dedicated teams for front-end and back-end, each expertised in their own domains      Logic Separation → We can separate the Logic of course for the frontend and backend      Easy to Manage      Easy to Scale        User Experience          Latest Web Tech like React can be used to Build Views      Improved Speed      Improved Performance      Consistency in design      How do we do it?Well, now that we know what we have to do. We are halfway there already ( Just Kidding ). Let’s discuss some of the things we can use to make the front-end efficient and reliable.  React JS          Its component architecture , helps us building a consistent design across the site.      It’s fast and performant.        Tailwind CSS          This is a light-weight CSS framework which is highly reliable and easy to use.      This has a good community, which can help to borrow UI components rather than making it from scratch.        Redux Toolkit          Redux Toolkit is a light version of Redux, which extracts away a lot of boilerplate codes and provides us easy to use APIs to manage state.        Jest          Jest is the most popular library for writing tests in a react application. Infact, Create-React-App provides support for this out of the box when we initiate a new react project.      So, that’s all the core technologies we can use to build an efficient and reliable front-end. But, here is the catch: we can even improve more by following certain practices, which will be fruitful in the long run.What else can we Improve ?Following are some of the best practices that we can use to further improve the frontend application.  ES Lint          Enforcing a code style guide is important to maintain the source code of the application. It helps to maintain consistency across the application.      More Particularly, we can use the AirBNB Style Guide. This is the most popular style guide for React Application.      We can add rules as per our need and requirements in the .eslintrc.js        Nested Routes          This is one of the features of react. We can nest the routes under other routes to maintain a route intuition.        &lt;Route path=&quot;/jobs&quot; element={&lt;JobsAndInterviews /&gt;}&gt;    &lt;Route index element={&lt;AllJobs /&gt;} /&gt;    &lt;Route path=&quot;my_jobs&quot; element={&lt;MyJobs /&gt;}&gt;      &lt;Route index element={&lt;ActiveJobs /&gt;} /&gt;      &lt;Route path=&quot;active&quot; element={&lt;ActiveJobs /&gt;} /&gt;      &lt;Route path=&quot;inactive&quot; element={&lt;InActiveJobs /&gt;} /&gt;      &lt;Route path=&quot;interviews&quot; element={&lt;Interviews /&gt;} /&gt;    &lt;/Route&gt;    &lt;Route path=&quot;all_jobs&quot; element={&lt;AllJobs /&gt;} /&gt;  &lt;/Route&gt;Like in this example snippet, we have a parent route for job and under that my_jobs and inside that we have active, inactive, interviews.      /jobs/my_jobs/active → this route path is really gives a lot of information of the pages.        Dynamic Routes          This is another feature of React Itself. This allows us to only load the pages that are requested by the user and not all.      Just imagine, our site has hundreds of pages. When the user wants to visit the homepage, we are trying to send him all the hundred pages. This doesn’t make any sense right ?        // Jobs Page Routes  export const JobsAndInterviews = lazy(() =&gt; import(&#39;../pages/Jobs&#39;));  export const AllJobs = lazy(() =&gt; import(&#39;../pages/Jobs/AllJobs&#39;));  export const ApplyJob = lazy(() =&gt; import(&#39;../pages/Jobs/ApplyJob&#39;));  export const JobDetails = lazy(() =&gt; import(&#39;../pages/Jobs/JobDetails&#39;));  export const MyJobs = lazy(() =&gt; import(&#39;../pages/Jobs/MyJobs&#39;));  export const ActiveJobs = lazy(() =&gt; import(&#39;../pages/Jobs/MyJobs/Active&#39;));  export const InActiveJobs = lazy(() =&gt; import(&#39;../pages/Jobs/MyJobs/Inactive&#39;));  export const Interviews = lazy(() =&gt; import(&#39;../pages/Jobs/MyJobs/Interviews&#39;));This above snippet shows how to import the components dynamically. But for this thing to work, we need to wrap the Routes in a Suspense Component which takes fallback.  The component given inside the fallback is rendered in between the dynamic loads. So, we can put our page loader here. Below is the snippet showing how to do it.  import jobsRoutes from &#39;./routes/jobsRoutes&#39;;  const App = () =&gt; (  &lt;Router&gt;    &lt;Provider store={store}&gt;      &lt;Layout&gt;        &lt;Suspense fallback={() =&gt; &lt;Loader /&gt;}&gt;          &lt;Routes&gt;            {jobsRoutes}          &lt;/Routes&gt;        &lt;/Suspense&gt;      &lt;/Layout&gt;    &lt;/Provider&gt;  &lt;/Router&gt;  );Now, this makes the website immensely faster than before.  Intuitive File and Folder Organization -&gt; Organizing the files and folders properly is a very important task because it significantly helps the new developers. It lowers the learning curve for the new fellas.  /src    /__tests__      /categoryA        /page1.test.js        /page2.test.js      /categoryB        /page1.test.js        /page2.test.js    /assets    /components      /customElements      /Layout    /features      /redux_slices.js    /pages      /categoryA        /page1.jsx        /page2.jsx      /categoryB        /page1.jsx        /page2.jsx    /routes    /store      /redux_store.js    /stylesThat’s how we can improve our codebase even more.Then, we have to make sure if our application runs the same on every device, OS, and system specs. For that we can dockerize the react app.Dockerization and DeploymentDockerizing the react app gives us the following benefits:  Consistency: Docker ensures the app runs consistently across different environments.  Dependency Management: Docker encapsulates app dependencies, preventing conflicts.  Easy Deployment: Docker simplifies deployment to various environments.  Scalability: Docker facilitates easy scaling to handle increased traffic.  Versioning and Rollbacks: Docker images can be versioned, enabling controlled updates and rollbacks.  Development and Testing: Docker streamlines development and testing in a consistent environment.  Infrastructure Agnostic: Docker allows running the app on various infrastructures.  Resource Efficiency: Docker containers are lightweight and efficient in resource utilization.  Easy Collaboration: Docker promotes seamless collaboration among developers and teams.  Security: Docker provides isolation, adding an extra layer of security to the app.We can dockerize the react app by adding docker files i.e.  Dockerfile → contains environment and installation instructions for the app.  FROM node:18 as builder  WORKDIR /app  COPY package.json .  RUN npm install  COPY . .  RUN npm run build  FROM nginx  EXPOSE 80  COPY --from=builder /app/build /usr/share/nginx/html  docker-compose.yml → contain commands to run our docker container.  version: &#39;3&#39;  services:    web:      build:        context: .        dockerfile: Dockerfile      ports:        - &#39;80:80&#39;Now, we have successfully containerized our react application. Finally, we need to deploy it to some cloud services such as AWS.  We can first push our docker image to docker hub  docker push iamsmruti/elitmus-frontend  Then we can login to EC2 instance and then pull the docker image  docker pull iamsmruti/elitmus-frontend  Finally, we can run the docker image  docker run -d -p 5000:5000 iamsmruti/elitmus-frontendThat wraps up our frontend application which can now be live. It is fully capable of consuming the APIs from the backend. Now, the business logic is in the backend and doesn’t put much load on the frontend and hence it is performant and reliable.If you have any questions, doubts, you can ping me at smrutiranjanbadatya2@gmail.com.I would definitely get back to you.I Hope this was a helpful and insightful guide for making a better frontend application with all the necessary good practices to maintain sustainability of the project.See Ya 👋🏻 … Peace ✌🏻References      React Docs - Here        Tailwind Docs - Here        Redux Toolkit Docs - Here        Jest Docs - Here        ES Lint Docs - Here        Docker Docs - Here  ",
        "url": "/technology/revamping-elitmus-dot-com-stand-alone-front-end-module/"
      }
      ,
    
      "technology-my-experience-as-a-summer-intern-at-elitmus-building-a-telegram-bot": {
        "title": "My Experience as a Summer Intern at eLitmus: Building a Telegram Bot",
        "author": "Mahesh",
        "category": "technology",
        "content": "As a summer intern at eLitmus, I had the opportunity to work on an exciting project that involved building a Telegram Bot. In today’s digital era, effective communication channels play a crucial role in connecting businesses with their stakeholders. eLitmus, a talent-tech platform, identified the need for a two-way communication channel between the platform and candidates. To achieve this, Telegram bots were chosen as the ideal starting point. This blog post will delve into the Telegram Bot Integration project.How it Began:The project started with the idea of leveraging the Telegram platform as a communication channel between eLitmus and its candidates. The goal was to create a two-way communication channel, enabling candidates to access information, receive updates, and engage in various activities through Telegram bots. This opened up possibilities for automating communication, collecting data, running quizzes, and providing valuable services to candidates.Design:Before diving into development phase, thorough planning and design are crucial. I begin by defining the core functionalities of the Telegram bots. I discovered that creating a bot through Bot Father (Telegram’s official bot) was the standard approach. As I was tasked with implementing the project using Ruby on Rails, I focused on two key aspects: developing the Telegram bot and designing the Admin panel.Designing such an application involves three key aspects: architecture design, database design, and UI/UX design. Let’s dive into each of these parts in more detail:  Architecture : The Telegram bots interact with users through messages and commands. Users can access FAQs, participate in quizzes, and receive responses based on their interactions with the bots. The bots handle user inputs, validate quiz answers, and provide feedback and results accordingly. An intuitive admin panel is developed using Ruby on Rails to facilitate easy management of the bot’s functionalities. The admin panel allows administrators to add, update, and delete FAQs, quizzes, and other content. It also provides insights and analytics related to user engagement and bot usage.        Database: The project utilizes a MySQL database to store and manage data related to users, FAQs, quizzes, quiz attempts, analytics, and other relevant information. The database schema is designed to efficiently store and retrieve data, ensuring optimal performance.    UI/UX:  To ensure a visually appealing and user-friendly Telegram bot interface, I delved into various UI options and explored the best ways to present information and interact with users. This research helped me identify the most effective strategies for creating an engaging and intuitive bot interface. And for the Admin panel, I took the initiative to design the entire interface using Figma. By visualizing the layout, components, and functionalities, I was able to ensure a cohesive and user-friendly experience for administrators managing the bot’s functionalities. Figma provided a powerful toolset for creating wireframes, mock-ups, and interactive prototypes, allowing me to iterate and refine the design before implementation.Development:Before starting this project, I had experience developing mobile applications, and most of them followed the Model-View-Template (MVT) pattern for backend, such as Django. However, for this project, I needed to learn and work with Ruby on Rails, which follows the Model-View-Controller (MVC) architectural pattern. Fortunately, my previous experience with backend development made it easier for me to understand Rails, and within the first two weeks, I was able to develop the basic functionalities of both the FAQ and Quiz bots.Integrating the telegram bot consists of 3 steps:      Creating a bot using Bot Father ( Official bot of telegram for creating telegram bot) and get the token that was generated by the bot father.            Initalizing the bot in the ruby file and declare a listening function that listens every messsage from the bot.        Writing the message specified functions that is called only when a specified message if recieved from the bot.  I have used Ruby on Rails for both front-end and back-end to develope admin panel. For database I have used mysql and for hosting purpose I have used AWS, EC2 to host admin panel using docker and telegram and RDS for database.Using docker to host the bot and admin panel was another part of the development that gave me an idea of how to does docker used by most of the companies, it was my personal goal in the year to learn docker so it got done by this project. And to say using docker wasn’t the difficult part. I had to learn how to write a docker file and docker compose file.Features Developed  User FlowI focused on refining the functionalities and user flow of the bots, particularly in the context of the Telegram channels. The FAQ bot is connected to the Telegram channel, and when a user posts a question in the channel’s comment section, it gets stored in the database. The admin can then view and answer the question, which is sent back to the user personally through Telegram. Additionally, users can access the FAQ bot to view existing FAQs and request the addition of new ones.          FAQ bot flow                                                                 Quiz bot flow                                                               Admin Panel    On the other hand, the admin panel allows the admin to create quizzes and questions. These quizzes are then posted in the Telegram channel, with a button redirecting users to the Quiz bot. Users can access multiple quizzes and attempt them through the bot.    By developing these functionalities, I was able to establish a seamless flow for users, ensuring they can interact with the bots and access relevant information easily. The admin panel provides the necessary tools for managing FAQs, quizzes, and user interactions, allowing for efficient administration and engagement with the users.    In the Admin panel, I implemented the design that I had previously created using Figma. The Admin panel offers various functionalities to enhance the administration and management of the Telegram bots. Here are some key features of the Admin panel:                  User Management: The Admin panel allows the admin to view active users and access individual user data. This includes information about the user’s activities, quiz attempts, and questions asked through the bot.                    FAQ Management: The Admin can view and manage the FAQs. They have the ability to add, edit, or remove FAQs as needed. Additionally, the Admin can track the number of reads by users, providing insights into the popularity and relevance of different FAQs.                    Quiz Management: The Admin can create quizzes and manage them within the Admin panel. They can add questions, set multiple options, and define correct answers. The Admin also has access to the responses of the quizzes, allowing them to analyze individual question analytics and gain insights into user performance. This can also be used to host surveys on telegram channels.                    Analytics: The Admin panel provides analytics on user activities related to both the FAQ and Quiz bots. The Admin can view data such as the number of attempts per day, week, month, or year, as well as the number of FAQ reads per day, week, month, or year. These analytics help the Admin understand user engagement and make data-driven decisions.                    Post Management: The Admin can utilize the post section in the Admin panel to create and publish posts in the Telegram channel directly from Telegram and to make it effective I have created two phases of create and publishing the post so that post get reviewed before publishing the post. This feature streamlines the process of sharing content and updates with users in the channel.              Admin Panel        By incorporating these functionalities into the Admin panel, I ensured that the administrative tasks associated with managing the Telegram bots were streamlined and efficient. The panel provides comprehensive control and insights, empowering the admin to effectively manage user interactions, content, and analytics.  Challenges Faced:Working with 3rd party API’s is one of the most challenging task and that is the challenging task of the project using telegram bot API. I could able to use telegram api to minimal amount of data of user, for example I couldn’t able to get users contact details, and I have crossed this challenge by finidng a feature of telegram that is by using permissions to access user details and request user to send the mobile number and location, but I couldn’t able to get location from the web or laptop. The biggest challenge I have faced was setting up and displaying analytics using charts and graphs. Initially, I tried using gems like Chartkick and FusionCharts, but faced issues with rendering the graphs correctly. Despite spending considerable time troubleshooting, the graphs weren’t displaying as expected. Eventually, I opted for Chart.js, which proved to be a more suitable solution for my needs. With Chart.js, I could create visually appealing and interactive charts to showcase the data collected through admin panel. The transition to Chart.js was smooth, and it enabled me to present data insights effectively, providing a valuable user experience.Conclusion:In summary, working on this project presented its fair share of challenges. However, with perseverance and problem-solving skills, I was able to overcome these obstacles and achieve success. I was able to develop the Telegram bots and the Admin panel effectively. I am thrilled to share that my hard work did not go unnoticed, and my project was selected for use by the company. This recognition is truly gratifying, as it demonstrates the value my work brings to the organization and the impact it can have on the company operations. Overall, this project was a rewarding journey that expanded my knowledge and skills in web development.",
        "url": "/technology/my-experience-as-a-summer-intern-at-elitmus-building-a-telegram-bot/"
      }
      ,
    
      "technology-resume-parsing-insights-and-steps-to-create-your-own-parser": {
        "title": "Resume Parsing: Insights and Steps to Create Your Own Parser",
        "author": "madhav",
        "category": "technology",
        "content": "Resume parsing is the automated process of extracting relevant information from resumes or CVs. It analyzes the unstructured text of a resume and extracts specific details like contact information, work experience, education, skills, and achievements. The extracted data is then converted into a structured format, allowing for easy analysis and integration into recruitment systems.Benefits of Resume Parsing  It is a time-saving automation  It increases efficiency in candidate screening  Improves accuracy in data extraction  It standardizes the data extraction and formattingWhat you’ll learn from this blog:  Resume parsing techniques for different file formats.  Extracting specific details from resumes.  Leveraging NLP techniques for parsing.  Handling multicolumn resumes.  Dockerizing the Application: Simplifying Deployment and Scalability  Hosting it on AWS EC2.Let’s get Started 🎉We’ll utilize Python and its Flask framework to create a resume parsing server.Application Flow Chart:We will be primarily working on 3 categories of file formats:  PDF  DOCX  Images (.png, .jpg, etc.)Data that we will be extracting  Embedded links in PDF  Personal data:  2.1. Name: First name and last name  2.2. Email  2.3. Phone Number  2.4. Address: City, Country, and Zip code  2.5. Links: Social and Coding Platform links   Education  3.1. Institute name  3.2. Duration: Start date and End date  3.3. Grade/CGPA  3.4. Degree   Experience 4.1. Company name 4.2. Role 4.3. Durations: Start date and End date 4.4. Skills  Certification:  5.1. Description  5.2. Duration  5.3. Skill   Project:  6.1. Project name  6.2. Skills  6.3. Description   Skills  Achievements  Exam scores 9.1. Exam name 9.2 Score  All other sections present in resumeDate/Duration ExtractionTo extract dates from text, we will use datefinder module, and regexp to extract years.Then we will combine these two and sort dates to get start and end date for our duration.import refrom datetime import dateimport datefinderdef get_date(input_string):    '''Get date from text'''    matches = list(datefinder.find_dates(input_string))    res = []    for i in matches:        date_str = str(i).split(' ')        extracted_date = date_str[0]        res.append(extracted_date)    return resdef get_years(txt):    '''Get years from text'''    pattern = r'[0-9]{4}'    lst = re.findall(pattern, txt)    current_date = date.today()    current_year = current_date.year    res = []    for i in lst:        year = int(i)        if 1900 &lt;= year &lt;= (current_year + 10):            res.append(i + \"-01-01\")    return resdef get_duration(input_text):    '''Get duration from text'''    dates = get_date(input_text)    years = get_years(input_text)    for i in years:        dates.append(i)    dates.sort()    duration = {        \"start_date\": \"\",        \"end_date\": \"\"    }    if len(dates) &gt; 1:        duration[\"start_date\"] = dates[0]        duration[\"end_date\"] = dates[len(dates) - 1]    return durationExtracting links from PDF:To extract links from the PDF, we will use the python module PDFx.import pdfxdef get_urls_from_pdf(file_path):    '''extract urls from pdf file'''    url_list = []    # for invalid file path    if os.path.exists(file_path) is False:        return url_list    pdf = pdfx.PDFx(file_path)    # get urls    pdf_url_dict = pdf.get_references_as_dict()    if \"url\" not in pdf_url_dict.keys():        return url_list    url_list = pdf_url_dict[\"url\"]    return url_listPDF to Textimport pdfxdef get_text_from_pdf(file_path):    '''extract complete text from pdf'''    # for invalid file path    if os.path.exists(file_path) is False:        return \"\"    pdf = pdfx.PDFx(file_path)    pdf_text = pdf.get_text()    return pdf_textExtracting Personal Details:We will extract text from the PDF and move ahead with further extractions.NameExtracting the name from the text is one of the challenging tasks.For this, we will be using NLP: Named Entity Recognition to extract name from the text.NLP function:def get_name_via_nltk(input_text):    '''extract name from text via nltk functions'''    names = []    for sent in nltk.sent_tokenize(input_text):        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):            if hasattr(chunk, 'label'):                name = ' '.join(c[0] for c in chunk.leaves())                names.append(name)    return names  The text is tokenized into sentences using nltk.sent_tokenize().  Each sentence is further tokenized into words using nltk.word_tokenize().  The part-of-speech tags are assigned to each word using nltk.pos_tag().  The named entities are identified by applying the named entity recognition (NER) using nltk.ne_chunk().  For each identified named entity chunk, if it has a ‘label’, indicating it is a named entity, the individual words are concatenated to form a name.  The extracted names are appended to the names list.Phone NumberTo extract the Phone number, we use the following module phonenumbers, we extract users country from text and using that we will extract relevant phone numbers.import geotextfrom phonenumbers import PhoneNumberMatcherdef get_phone(input_text):    '''extract phone number from text'''    phone_numbers = []    countries_dict = geotext.GeoText(input_text).country_mentions        country_code = \"IN\"    for i in countries_dict.items():        country_code = i[0]        break    search_result = PhoneNumberMatcher(input_text, country_code)    phone_number_list = []    for i in search_result:        i = str(i).split(' ')        match = i[2:]        phone_number = ''.join(match)        phone_number_list.append(phone_number)    for i in phone_number_list:        if i not in phone_numbers:            phone_numbers.append(i)    return phone_numbersEmailTo extract the Email, we use the following regexp: [^\\s]+@[^\\s]+[.][^\\s]+def get_email(input_text):    '''extract email from text'''    email_pattern = '[^\\s]+@[^\\s]+[.][^\\s]+'    emails = []    emails = re.findall(email_pattern, input_text)    # pick only unique emails    emails = set(emails)    emails = list(emails)    return emailsAddressTo Extract address, we use the geotext module; we get City, Country, and Zipcode.import geotextdef get_address(input_arr):    '''get address information from input array'''    input_text = \" \\n \".join(input_arr)    res = {}    # getting all countries    countries_dict = geotext.GeoText(input_text).country_mentions    res[\"country\"] = []    for i in countries_dict:        res[\"country\"].append(i)    # getting all cities    res[\"city\"] = geotext.GeoText(input_text).cities    # zip code    pattern = \"\\b([1-9]{1}[0-9]{5}|[1-9]{1}[0-9]{2}\\\\s[0-9]{3})\\b\"    res[\"zipcode\"] = re.findall(pattern, input_text)    return resLinksAs we already have a URL list from 1st operation, we will match links from a list of our own, this can be saved in any database or hard-coded, and categorize them into social or coding sections.Other SectionsThere can be many sections in a resume, that we cannot always account for.To extract them, we will create a list of possible section heading and match them against each line from the resume that we have extracted.The code will be as following:from utils import dynamo_dbRESUME_SECTIONS = dynamo_db.get_item_db(\"RESUME_SECTIONS\")def extract_resume_sections(text):    '''Extract section based on resume heading keywords'''    text_split = [i.strip() for i in text.split('\\n')]    entities = {}    entities[\"extra\"] = []    key = False    for phrase in text_split:        if len(phrase.split(' ')) &gt; 10:            if key is not False:                entities[key].append(phrase)            else:                entities[\"extra\"].append(phrase)            continue        if len(phrase) == 1:            p_key = phrase        else:            p_key = set(phrase.lower().split()) &amp; set(RESUME_SECTIONS)        try:            p_key = list(p_key)[0]        except IndexError:            pass        if p_key in RESUME_SECTIONS and (p_key not in entities.keys()):            entities[p_key] = []            key = p_key        elif key and phrase.strip():            entities[key].append(phrase)        else:            if len(phrase.strip()) &lt; 1:                continue            entities[\"extra\"].append(phrase)    return entitiesEducationTo extract education, we need to identify a line from our education section that represent the school/institute name, and a line that represents the degree. After which we can search for CGPA or Percentage using regexp.For name recognition, we will make use of a list of keywords that can be present in the name.Code to get school name, similarly we can implement to get degree as well.import refrom utils import helper, dynamo_dbSCHOOL_KEYWORDS = dynamo_db.get_item_db(\"SCHOOL_KEYWORDS\")def get_school_name(input_text):    '''Extract list of school names from text'''    text_split = [i.strip() for i in input_text.split('\\n')]    school_names = []    for phrase in text_split:        p_key = set(phrase.lower().split(' ')) &amp; set(SCHOOL_KEYWORDS)        if (len(p_key) == 0):            continue        school_names.append(phrase)    return school_namesCode to extract CGPA/GPA or Percentage gradedef get_percentage(txt):    '''Extract percentage from text'''    pattern = r'((\\d+\\.)?\\d+%)'    lst = re.findall(pattern, txt)    lst = [i[0] for i in lst]    return lstdef get_gpa(txt):    '''Extract cgpa or gpa from text in format x.x/x'''    pattern = r'((\\d+\\.)?\\d+\\/\\d+)'    lst = re.findall(pattern, txt)    lst = [i[0] for i in lst]    return lstdef get_grades(input_text):    '''Extract grades from text'''    input_text = input_text.lower()    # gpa    gpa = get_gpa(input_text)    if (len(gpa) != 0):        return gpa    # percentage    percentage = get_percentage(input_text)    if (len(percentage) != 0):        return percentage    return []SkillsIn order to extract skills from the text, a master list of commonly used skills can be created and stored in a database, such as AWS DynamoDB. Each skill from the list can be matched against the text to identify relevant skills. By doing so, a comprehensive master skill list can be generated, which can be utilized for more specific skill extraction in subsequent sections.from utils import dynamo_dbskills = dynamo_db.get_item_db(\"ALL_SKILLS\")def get_skill_tags(input_text):    '''Extract skill tags from text'''    user_skills = []    for skill in skills:        if skill in input_text.lower():            user_skills.append(skill.upper())    return user_skillsExperienceTo extract company names and roles, a similar strategy can be employed as we used for finding school names and degrees. By applying appropriate techniques, such as named entity recognition or pattern matching, we can identify company names and associated job roles from the text. Additionally, for skill extraction, we can match the text against our previously calculated list of skills to identify and extract relevant skills mentioned in the textAchievements and CertificationsWe can use the section text that we extracted previously and for each line of it, we can search for duration and skills in it.from utils import helper, skill_tagsdef get_certifications(input_array):    '''Function to extract certificate information'''    res = {        \"description\": input_array,        \"details\": []    }    try:        for cert in input_array:            elem_dict = {                \"institute_name\": str(cert),                \"skills\": skill_tags.get_skill_tags(cert),                \"duration\": helper.get_duration(cert)            }            res[\"details\"].append(elem_dict)    except Exception as function_exception:        helper.logger.error(function_exception)    return resProjectsWhen it comes to extracting project titles, it can be challenging due to the variations in how individuals choose to title their projects. However, we can make an assumption that project titles are often written in a larger font size compared to the rest of the text. Leveraging this assumption, we can analyze the font sizes of each line in the text and sort them in descending order. By selecting the lines with the largest font sizes from the top, we can identify potential project titles. This approach allows us to further segment the project section and extract additional details such as skills utilized and project durations.Link: How to find the Font Size of every paragraph of PDF file using python code?import fitzdef scrape(keyword, filePath):    results = [] # list of tuples that store the information as (text, font size, font name)     pdf = fitz.open(filePath) # filePath is a string that contains the path to the pdf    for page in pdf:        dict = page.get_text(\"dict\")        blocks = dict[\"blocks\"]        for block in blocks:            if \"lines\" in block.keys():                spans = block['lines']                for span in spans:                    data = span['spans']                    for lines in data:                            results.append((lines['text'], lines['size'], lines['font']))    pdf.close()    return resultsUsing this we find our project titles:from utils import helper, skill_tagsfrom difflib import SequenceMatcherdef similar(string_a, string_b):    '''Find similarity between two string'''    return SequenceMatcher(None, string_a, string_b).ratio()def extract_project_titles(input_array, text_font_size):    ls = []    for line_tuple in text_font_size:        line = line_tuple[0]        for s in input_array:            if similar(line,s) &gt; 0.85:                ls.append([line_tuple[1], s])    ls.sort(reverse=True)    title_font_size = ls[0][0] if(len(ls) &gt; 0) else 0    project_title = []    for i in ls:        if i[0] == title_font_size:          project_title.append(i[1])    return project_titledef get_projects(input_array, text_font_size):    '''extract project details from text'''    res = {        \"description\": input_array,        \"details\": []    }    txt = ' \\n '.join(input_array)    project_titles = helper.extract_titles_via_font_size(        input_array, text_font_size)    project_sections = helper.extract_sections(txt, project_titles)    try:        for i in project_sections.items():            key = i[0]            txt = '\\n'.join(project_sections[key])            elem_dict = {                \"project_name\": key,                \"skills\": skill_tags.get_skill_tags(txt),                \"duration\": helper.get_duration(txt)            }            res[\"details\"].append(elem_dict)    except Exception as function_exception:        helper.logger.error(function_exception)    return resHandling multicolumn resumesUp until now, we have explored techniques to handle single-column resumes successfully. However, when it comes to two-column or multicolumn resumes, a direct extraction of text may not be sufficient. If we attempt to extract text from a multicolumn PDF using the same method as before, we will encounter challenges such as, the text from different columns will merge together, as our previous approach scans the text from left to right and top to bottom, rather than column-wise.To overcome this issue, let’s delve into how we can solve this problem and effectively handle multicolumn resumes.Drawing textboxesOptical Character Recognition (OCR) comes to the rescue by identifying textboxes and providing their coordinates within the document. By utilizing OCR, we can pinpoint the location of these textboxes, which serve as a starting point for further analysis.To tackle the challenge of multicolumn resumes, a line sweep algorithm is implemented. This algorithm systematically scans along the X-axis and determines how many textboxes intersect each point. By analyzing this distribution, potential column divide lines can be inferred. These lines act as reference markers, indicating the boundaries between columns.Once the column lines are established, the text can be extracted from the identified textboxes in a column-wise manner. Following the order of the column lines, the text can be retrieved and processed accordingly.By leveraging OCR, the line sweep algorithm, and the concept of column lines, we can effectively handle multicolumn resumes and extract the necessary information in an organized and structured manner.Code:import cv2import fitzfrom fitz import Document, Page, Rectimport pytesseractimport functoolsdef textbox_recognition(file_path):    '''Extract text_boxes from image'''    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)    ret, thresh1 = cv2.threshold(        img, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)    # kernel    kernel_size = 10    rect_kernel = cv2.getStructuringElement(        cv2.MORPH_RECT, (kernel_size, kernel_size))    # Applying dilation on the threshold image    dilation = cv2.dilate(thresh1, rect_kernel, iterations=1)    # Finding contours    contours, hierarchy = cv2.findContours(        dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)    segments = []    text_boxes = []    # Looping through the identified contours    for cnt in contours:        x, y, w, h = cv2.boundingRect(cnt)        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)        segments.append([x, x+w])        text_boxes.append((x, y, w, h))    return (segments, text_boxes)def detect_column_lines(segments):    '''Detect column lines from segments'''    mx = max(i[1] for i in segments)    line_sweep_arr = [0 for _ in range(mx+10)]    for i in segments:        line_sweep_arr[i[0] + 1] += 1        line_sweep_arr[i[1]] -= 1    for i in range(1, mx+10):        line_sweep_arr[i] += line_sweep_arr[i-1]    line_mean = sum(line_sweep_arr)/len(line_sweep_arr)    potential_points = []    for i in range(1, mx+10):        if line_sweep_arr[i] &lt; int(line_mean/2.5):            potential_points.append(i)    line_points = []    for i in potential_points:        if len(line_points) == 0:            line_points.append(i)            continue        prev = line_points[len(line_points) - 1]        if i == prev + 1:            line_points[len(line_points) - 1] = i        else:            line_points.append(i)    return line_pointsdef get_text(img, box_data):    '''Extract text from given box data'''    (x, y, w, h) = box_data    cropped_image = img[y:y+h, x:x+w]    # to show image    txt = pytesseract.image_to_string(cropped_image)    return txtdef box_coverage_percentage(x, w, line):    '''Extract coverage area in percentage for box'''    covered_width = line - x    cover_percentage = covered_width / w    return cover_percentagedef clean_text(txt):    '''Clean text'''    txt = txt.strip()    txt = txt.replace(\"•\", '')    return txtY_LIMIT = 10def custom_sort(a, b):    '''custom sort logic'''    if a[1] - Y_LIMIT &lt;= b[1] &gt;= a[1] + Y_LIMIT:        return -1 if (a[0] &lt;= b[0]) else 1    return -1 if (a[1] &lt;= b[1]) else 1def get_boxes_for_line(text_boxes, line, ordered_text_box, prev_line):    '''get boxes with line constraints'''    temp_boxes = [i for i in text_boxes]    temp_boxes.sort(key=functools.cmp_to_key(custom_sort))    res = []    # check if 90% of box is before line    for box in temp_boxes:        if box in ordered_text_box:            continue        (x, y, w, h) = box        if (x &gt;= prev_line - Y_LIMIT and x &lt; line and box_coverage_percentage(x, w, line) &gt;= 0.9):            res.append(box)    res.sort(key=lambda x: x[1])    return resdef map_size(x, org, new):    '''map box co-ordinates from image to pdf'''    return (x*new)/orgdef get_text_from_pdf(box, img_shape, pdf_shape, page):    '''extract text from pdf box'''    (x, y, w, h) = box    (height, width) = img_shape    (W, H) = pdf_shape    x = map_size(x, width, W)    w = map_size(w, width, W)    y = map_size(y, height, H)    h = map_size(h, height, W)    rect = Rect(x, y, x+w, y+h)    text = page.get_textbox(rect)    return textdef image_to_text(file_path, pdf_file_path=\"\"):    '''extract text from image'''    segments, text_boxes = textbox_recognition(file_path)    column_lines = detect_column_lines(segments)    # if single column    if len(column_lines) &lt; 3:        return \"\"    # align text boxes by column    # text boxes within columns    ordered_text_box = []    for i in range(len(column_lines)):        prev_line = column_lines[i-1] if ((i-1) &gt;= 0) else 0        boxes = get_boxes_for_line(            text_boxes, column_lines[i], ordered_text_box, prev_line)        for b in boxes:            ordered_text_box.append(b)    # boxes that are not in any column    # text boxes not in any column    non_selected_boxes = []    for i in text_boxes:        if i not in ordered_text_box:            non_selected_boxes.append(i)    for i in non_selected_boxes:        y = i[1]        if y &lt;= ordered_text_box[0][1]:            ordered_text_box.insert(0, i)        else:            ordered_text_box.append(i)    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)    ret, thresh = cv2.threshold(img, 225, 255, 0)    img_shape = img.shape    pdf_shape = (0, 0)    page = None    if pdf_file_path != \"\":        doc = fitz.open(pdf_file_path)        page = doc[0]        pdf_shape = (page.rect.width, page.rect.height)    resume_text = \"\"    for i in ordered_text_box:        if pdf_file_path != \"\":            txt = clean_text(get_text_from_pdf(i, img_shape, pdf_shape, page))        else:            txt = clean_text(get_text(thresh, i))        resume_text += txt + \"\\n\"    # clean text    txt = resume_text.split(\"\\n\")    res = []    for line in txt:        if len(line) == 0:            continue        res.append(line)    resume_text = ' \\n '.join(res)    return resume_textDockerizing the ApplicationTo make deploying the application easy we will be Dockerizing the Application.Dockerfile# syntax=docker/dockerfile:1FROM python:3.9-busterWORKDIR /resume-parser-dockerRUN mkdir input_filesRUN pip3 install --upgrade pipCOPY requirements.txt requirements.txtRUN pip3 install -r requirements.txt# download nltk requiredRUN python -m nltk.downloader punktRUN python -m nltk.downloader averaged_perceptron_taggerRUN python -m nltk.downloader maxent_ne_chunkerRUN python -m nltk.downloader wordsRUN apt-get update \\  &amp;&amp; apt-get -y install tesseract-ocrRUN apt-get update &amp;&amp; apt-get install ffmpeg libsm6 libxext6  -yCOPY . .EXPOSE 5000/tcpCMD [ \"python3\", \"-u\" , \"main.py\"]Then run following commands to create image and run it.  Build Image    docker build --tag jhamadhav/resume-parser-docker .    Run Image at port 5000    docker run -d -p 5000:5000 jhamadhav/resume-parser-docker    Check images    docker ps    Stop once done    docker stop jhamadhav/resume-parser-docker  Hosting on AWSNow that we have a docker image of our application.We can publish it to dockerHub:docker push jhamadhav/resume-parser-dockerThen login to your EC2 instance and pull the image:docker pull jhamadhav/resume-parser-dockerRun the image:docker run -d -p 5000:5000 jhamadhav/resume-parser-docker  🎉🎉🎉 We have a fully functional Resume parser ready.Future WorkWe can make use of Large Language Models (LLM), train on datasets and fine tune LLM model to make extraction of below fields more accurate:  School/Institute name  Degree  Company name  Role in a jobConclusion  In conclusion, resume parsing using NLP techniques offers a streamlined approach to extract crucial information from resumes, enhancing the efficiency and accuracy of candidate screening.  By leveraging OCR, named entity recognition, and line sweep algorithms, we can handle various resume formats, including multicolumn layouts.  The power of NLP automates the parsing process, empowering recruiters to efficiently process resumes and make informed hiring decisions.  Embracing resume parsing techniques ensures fair and objective evaluation of applicants, leading to successful recruitment outcomes.  With this skillset, you can revolutionize resume processing and contribute to more efficient hiring practices.If you have any questions, doubts, or just want to say hi, feel free to reach out to me at contact@jhamadhav.com ! I’m always ready to chat about this cool project and help you out. Don’t be shy, drop me a line and let’s geek out together!",
        "url": "/technology/resume-parsing-insights-and-steps-to-create-your-own-parser/"
      }
      ,
    
      "technology-debugging-and-fixing-mysql-deadlock-issue": {
        "title": "Debugging &amp;amp; Fixing mysql deadlock issue",
        "author": "nikhil",
        "category": "technology",
        "content": "Recently, during one of our tests, we encountered a deadlock issue that was reported by Sentry. The deadlock occurred while attempting to insert scores into a table after completing a candidate’s test. We were initially unsure about the cause of this deadlock. Upon investigation, we discovered that it was due to the interplay of various locks in our MySQL database. In this blog post, we will deep dive into the nature of these locks, understand their impact on transactions, and present the solutions we implemented to mitigate deadlock occurrences.Understanding deadlocksTo understand the deadlock situation, let’s familiarize ourselves with the different types of locks involved, as defined by the official MySQL documentation:GAP Lock:A gap lock is a lock on a gap between index records, or a lock on the gap before the first or after the last index record. A gap might span a single index value, multiple index values, or even be empty.If id is not indexed or has a nonunique index, the statement does lock the preceding gap.Next Key Lock:A next-key lock is a combination of a record lock on the index record and a gap lock on the gap before the index record. in simple words If one session has a shared or exclusive lock on record R in an index, another session cannot insert a new index record in the gap immediately before R in the index order.Insert Intention Lock:An insert intention lock is a type of gap lock set by INSERT operations prior to row insertion. This lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap.Problem ScenarioIn our case, we have two tables, table1 and table2, with a has_many relationship. All operations are performed on table2, which has an index on table1 as a foreign key.Transaction ABEGIN;DELETE FROM table2 WHERE table2.table1_id=127;Query OK, 1 row affected (0.00 sec)Resulting data locksmysql&gt; SELECT INDEX_NAME, LOCK_TYPE,LOCK_DATA,LOCK_MODE,LOCK_STATUS, EVENT_ID FROM performance_schema.data_locks;+-----------------------------------------+-----------+-----------+---------------+-------------+----------+| INDEX_NAME                | LOCK_TYPE | LOCK_DATA | LOCK_MODE     | LOCK_STATUS | EVENT_ID |+-----------------------------------------+-----------+-----------+---------------+-------------+----------+| NULL                      | TABLE     | NULL      | IX            | GRANTED     |      408 || index_table2_on_table1_id | RECORD    | 127, 92   | X             | GRANTED     |      408 || PRIMARY                   | RECORD    | 92        | X,REC_NOT_GAP | GRANTED     |      408 || index_table2_on_table1_id | RECORD    | 128, 93   | X,GAP         | GRANTED     |      408 |+-----------------------------------------+-----------+-----------+---------------+-------------+----------+4 rows in set (0.00 sec)This query acquires a gap lock on table2 and an insert intention lock on table1_id values 126 and 127.Transaction BBEGIN;INSERT INTO table2(table1_id) VALUES(126);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionResulting data locksmysql&gt; SELECT INDEX_NAME,LOCK_TYPE,LOCK_DATA,LOCK_MODE,LOCK_STATUS, EVENT_ID FROM performance_schema.data_locks;+-----------------------------------------+-----------+-----------+------------------------+-------------+----------+| INDEX_NAME                  | LOCK_TYPE | LOCK_DATA | LOCK_MODE              | LOCK_STATUS | EVENT_ID |+-----------------------------------------+-----------+-----------+------------------------+-------------+----------+| NULL                        | TABLE     | NULL      | IX                     | GRANTED     |      351 || index_table2_on_table1_id   | RECORD    | 127, 92   | X,GAP,INSERT_INTENTION | WAITING     |      351 || NULL                        | TABLE     | NULL      | IX                     | GRANTED     |      408 || index_table2_on_table1_id   | RECORD    | 127, 92   | X                      | GRANTED     |      408 || PRIMARY                     | RECORD    | 92        | X,REC_NOT_GAP          | GRANTED     |      408 || index_table2_on_table1_id   | RECORD    | 128, 93   | X,GAP                  | GRANTED     |      408 |+-----------------------------------------+-----------+-----------+------------------------+-------------+----------+6 rows in set (0.01 sec)As Transaction A holds the lock on table1_id 126 due to the gap lock, Transaction B waits for the lock. However, it eventually times out, resulting in a lock wait timeout error.To create a deadlock, one must perform a delete query in Transaction B. Then, when attempting to insert a record in Transaction A, a deadlock error is thrown, with Transaction B becoming the victim. This deadlock situation arises due to the conflicts in the next-key lock, preventing Transaction B from inserting the record.In a nutshellLets understood the above queries in nutshell to create a deadlock.  Transaction A -&gt; BEGIN;  Transaction A -&gt; DELETE records on table2 with table1_id=x.  Transaction B -&gt; BEGIN;  Transaction B -&gt; DELETE record on table2 with table1_id=y;  Transaction B -&gt; INSERT a record on table2 and table1_id is x-1.  Transaction A -&gt; INSERT a record on table2 and table1_id is y-1.  A deadlock occurs, with Transaction A being the victim.Practical example of GAP lock &amp; Next Key Lock.Gap lock is basically on range of values &amp; will be aquired on a range if we try to delete a record which does not exist.table1+----+| id |+----+| 73 || 74 || 81 || 82 |+----+table2+-----+-----------+| id  | table1_id |+-----+-----------+| 1   | 73        | | 2   | 82        |+-----+-----------+Transaction ABEGIN;DELETE from table2 where table1_id=75;Query OK, 0 rows affected (0.00 sec)This transaction will aquire a gap lock on range from 74-80.this means if we try to insert new values in table2(in another session) with table1_id ranging from 74-80 it will wait until delete transaction commits.Other issuesIn addition to addressing the deadlock issues caused by gap locks, we also encountered problems related to AASM records. We were using the AASM gem, a library that manages state transitions. In our case, this library was responsible for changing the state of the test to “completed” and executing several callback functions. These operations were performed as part of a single transaction, which sometimes resulted in prolonged transaction durations and increased the likelihood of deadlocks.Model dummy codeaasm do  state :active, initial: true  state :complete  event :complete, after: [:method1, :method2, :method3] do      transitions from: :active, to: :complete  endendWhen the test is marked as complete and the state changes, all the MySQL-related queries are executed as part of a single transaction.Due to the execution of all these methods within a single transaction, there were instances where the transaction took a considerable amount of time to complete. These prolonged transactions duration increased the risk of deadlocks occurrence and also resulted in issues related to lock wait time.FIX  To fix this we moved the insertion of records as a separate transaction out of the aasm state change.  Optimized transaction size: We optimized the other badly written queries in the transaction.  Reduced transaction duration: Only limited number of queries were part of the state change transaction (to keep the transaction short).  We further optimized the GAP lock by avoiding unnecessary delete queries when the records were not present in the table with the corresponding ID.References  Innodb Gap Lock  Innodb Next Key Lock  Innodb Insert Intention Lock  Gap lock with example medium article  Gap lock article by percona",
        "url": "/technology/debugging-and-fixing-mysql-deadlock-issue/"
      }
      ,
    
      "technology-website-monitor-using-google-app-script": {
        "title": "Website Monitor Using Google App Script",
        "author": "nikhil",
        "category": "technology",
        "content": "Recently, I was looking for a solution to notify me when a website is down and when it is back up. I found a few solutions, but they all had a learning curve. So I thought of an alternative solution using Google App Script, which I had recently learned about.Requirements  Can run every 5 minutes.  Can send emails when the website is down.  Trustworthy.I wasn’t sure if the first requirement was possible with Google App Script, but the other two were. After reading the documentation, I found that it was possible to create a time-based trigger for a script.Steps to follow:  Create a new Google App Script project.  Create a function to track the website. Here is an example:function myFunction() {   const initialUrls = [     { uri: &#39;https://mock.codes/200&#39;, status: &#39;&#39;},     { uri: &#39;https://mock.codes/500&#39;, status: &#39;&#39;}   ];  const properties = PropertiesService.getScriptProperties(); let urls =  JSON.parse(properties.getProperty(&#39;URL_LIST&#39;)) || initialUrls; const errorResponseCodes = [500, 502, 503, 504]; const alertEmail = &#39;alertmail@gmail.com&#39;;  const options = { muteHttpExceptions: true };  urls.forEach((url) =&gt; {   let responseCode = UrlFetchApp.fetch(url.uri, options).getResponseCode();    const isErrorResponse = errorResponseCodes.includes(responseCode);   const wasPreviouslyDown = url.status === &#39;down&#39;;   if (isErrorResponse &amp;&amp; !wasPreviouslyDown) {     // Site is now down for the first time     const subject = `Alert: Your site ${url.uri} is currently down`;     const body = `${url.uri} has encountered an error with status code ${responseCode}`;     MailApp.sendEmail(alertEmail, subject, body);     url.status = &#39;down&#39;;   } else if (!isErrorResponse &amp;&amp; wasPreviouslyDown) {     // Site was previously down, but is now back up     const subject = `Your site ${url.uri} is now back up`;     const body = `${url.uri} has recovered and is now back up`;     MailApp.sendEmail(alertEmail, subject, body);     url.status = &#39;&#39;;   } }); properties.setProperty(&#39;URL_LIST&#39;, JSON.stringify(urls));}  Go to the “Triggers” menu in the left sidebar of the Google App Script project.  Click the “Add Trigger” button and select the function to run.  Choose the options to run the trigger every 5 minutes and click “Save”ExplanationThis above code uses the UrlFetchApp service to make HTTP requests to the websites and check their status. it stores the value of each trigger in a variable so that whenver site goes live again it can send email of website backed up.  You can also check the logs for each trigger execution in the “Execution” menu on the left side of the project.ConclusionIn conclusion, Google App Script is a useful tool for creating a customized website tracker that can notify the user when a website is down. The process of setting up the tracker is straightforward and the logs can be easily accessed to track the execution of the function. this basic functionality can be enhanced more to record the status in a csv file. also interesting graphs and charts can be made using that data.Additional investigationsUpptime is one of the good open-source alternative which can be used to monitor a website. it uses github actions to make sure the website is up and creates a issue if website is down for some reason. it also logs the information about the website speed.",
        "url": "/technology/website-monitor-using-google-app-script/"
      }
      ,
    
      "technology-the-revamp-of-a-video-proctoring-solution-a-behind-the-scenes-look": {
        "title": "The revamp of a Video Proctoring Solution: A Behind-the-Scenes Look",
        "author": "manish",
        "category": "technology",
        "content": "The story of how we took a good platform and made it even betterBackgroundFor the past few months, the number of test takers and clients at eLitmus has increased significantly. Conducting all of these tests remotely poses a significant challenge in terms of preventing cheating. To address this issue, eLitmus has developed an in-house solution using the open-source Kurento media server. While this solution has been effective in terms of recording videos, it is not horizontally scalable.In search of a more effective solution, eLitmus turned to Amazon Kinesis and worked with the AWS team to conduct a proof-of-concept. While this approach allowed for live proctoring, it was not possible to record the exams.How did it get begin?As I was learning about WebRTC and Amazon Kinesis during this time, I had the opportunity to attend a session by a company called 100ms. This company is focused on solving problems related to live conferencing, and I was eager to learn more about their approach.After connecting with the co-founder of 100ms, I received a message from their salesperson to schedule a demo call. During the call, we determined that 100ms could be a potential solution for eLitmus’ scalability problem. However, we needed to weigh the costs of maintaining engineering time and effort to maintain the solution against the opportunity cost of using that time to build a new product, as well as overall server and bandwidth costs.Based on this analysis, we decided to proceed with a proof-of-concept for live remote proctoring. I spent the next week working on the proof-of-concept and was able to complete it successfully. From there, we saw potential synergies between 100ms and eLitmus and decided to make the product(Knights Watch) an open-source platform.Designing &amp; DevelopingI created a document outlining the requirements for the video proctoring solution, including features such as a proctoring dashboard, candidate tests screen, cheating analysis and verification dashboard, admin dashboard, and auto proctoring. For the first version (v0.1), we planned to roll out the proctoring dashboard with multiple streams visible to the proctor, storage of the video stream on an s3 server, retrieval of the video stream in the cheating analysis and verification dashboard, and admin configuration.After outlining the requirements for the video proctoring solution, I designed the architecture for the solution, diagrammatically representing how all of the components would be connected. The main components of the app were the 100ms server API, the eLitmus server, and the candidate or proctor’s browser.Next, I created a milestone on Github and listed out the issues that needed to be addressed, including the integration of the proctor dashboard, candidate test screen, algorithm for assigning candidates and proctors to rooms, and storage of videos on the eLitmus prescribed directory structure on an s3 server.I began working on these issues and was able to roll out the v0.1 of the proctoring solution within a few weeks. During this time, our team encountered various challenges and suggested various features to 100ms.Challenges FacedAs we worked on storing videos on an AWS s3 server in our prescribed directory structure, we encountered a challenge with the 100ms API. The webhook provided by 100ms was only for the composite recording of the room, not for individual recordings. However, we needed webhooks to notify us of the success of each individual recording. In addition, 100ms had the functionality for only a single webhook per account, but we needed to support multiple environments with multiple applications within a single account. We requested this feature from 100ms.While working on an algorithm to assign candidates and proctors to rooms, I faced the challenge of storing authentication tokens in the user’s browser and in Redis storage in production. I wrote an algorithm to handle the expiration of tokens from both ends and to handle multiple events.As we configured 100ms for various environments including staging, production, and edge, we encountered several issues and suggested various features to 100ms. These included the ability to delete apps and templates from the 100ms dashboard from the front-end, team management options in the dashboard, and handling of access keys and secrets for multiple environments.Testing live remote proctoring solutionAfter completing the first version (v0.1) of the video proctoring solution, we were ready to test it in production. eLitmus was conducting an internal hiring event at the time, and we used the live video proctoring feature for this event with around 400 candidates. The event went smoothly, with minor issues. The proctor was able to hear the voices of the candidates and all of the videos were recorded throughout the session.This success gave us confidence in the solution, and we made some minor tweaks. However, our main concern from the start had been scalability, and we wanted to test the solution at a larger scale. We had an in-person test at IITK with over 600 candidates, and decided to conduct the event with live proctoring. The event went smoothly, but the next day we conducted data analysis and discovered that 14 out of 600+ videos had some data loss or were not recorded.We had a meeting with 100ms to discuss this issue, and after working with their engineering team, we determined that the issue was caused by network connectivity problems. We fixed the issue and the proctoring solution became more stable, with 97% of the videos being recorded.Open-sourcing video proctoring solutionAfter this event, we had discussions with 100ms about pricing and suggested various features, including pricing on the 100ms dashboard itself and the option to opt-in or opt-out of composite recording and browser-based recording.After making the video proctoring solution an open-source project, I focused on documenting the project so that it could be used by others in the community and more developers could contribute to it. I wrote several documents, including a readme file, information on the architecture and prerequisites, installation guidelines, development guidelines, deployment guidelines, a code of conduct, and guidelines for contributing and welcoming new contributors.ConclusionIn conclusion, the development of the video proctoring solution at eLitmus was a challenging but rewarding process. By identifying a need to solve the problem of vertical scalability, we were able to explore various solutions and ultimately choose 100ms as a partner to help us build a scalable and effective video proctoring platform. Through the development process, we encountered various challenges and were able to work closely with the 100ms team to find solutions and improve the stability of the platform. We are proud to have made the video proctoring solution an open-source project and to have contributed to the community by documenting the project and welcoming new contributors. We hope that others will find this project useful and will be able to build upon it to create even better solutions in the future.",
        "url": "/technology/the-revamp-of-a-video-proctoring-solution-a-behind-the-scenes-look/"
      }
      ,
    
      "technology-fixing-capybara-flaky-tests": {
        "title": "Fixing Capybara Flaky Tests",
        "author": "nikhil",
        "category": "technology",
        "content": "When writing system tests for a user interface, it is common to encounter test cases that fail randomly. One of the common failure can occur when the JavaScript on a page takes time to render, causing issues with the test case.For example, imagine a test case that clicks a button on a page and then checks for the presence of certain content after the click.Demo Codevisit submit_pageclick_on &#39;Submit&#39;assert page.has_content &#39;Some content after clicking on submit&#39;In most cases, this test will run without any issues. However, occasionally the test may fail on the third line with the error “Expected false to be truthy”. This error can occur when the page is visited and the JavaScript on the page takes a few seconds to load. During this time, the submit button may be clicked, but because there is no JavaScript associated with the button yet, the button click does not do anything. As a result, the test is still on the submit page when it tries to assert that the expected content is present, causing the test to fail.SolutionOne solution to this problem is to increase the wait_time setting in capybara. However, this approach has several limitations. First, the wait_time setting is global and applies to all test cases, so if it is set to a high value, it will increase the overall execution time of the test suite. Additionally, the wait_time setting only waits for a fixed amount of time before moving on with the test, without checking whether the page has finished loading. This means that if the page takes longer to load than the wait_timeThe other solution is to use the execute_script method provided by Capybara to click the button instead of the click_on method. The execute_script method allows you to execute JavaScript code within the context of the current page. By using this method to click the button, the click action is added to the end of the browser’s call stack. This means that the click action will be executed after any existing JavaScript code on the page has finished running, ensuring that the button is fully initialized and ready to be interacted with before the test tries to click it.To use the execute_script method to click the button, you can use the following code:page.find_button(&#39;Submit&#39;).execute_script(&#39;this.click()&#39;)This way we can ensure that click method will run only after the page javascript is fully loaded.Browser Call Stack          |               |          |               |          |   JavaScript  |  &lt;-- existing code on the page(1)          |_______________|          |               |          |   JavaScript  |  &lt;-- existing code on the page(2)          |_______________|          |               |          |  click action |  &lt;-- added by execute_script method(3)          |_______________|",
        "url": "/technology/fixing-capybara-flaky-tests/"
      }
      ,
    
      "technology-sidekiq-process-in-production-with-systemd-and-monit": {
        "title": "Sidekiq process in production with Systemd and Monit",
        "author": "manish",
        "category": "technology",
        "content": "Recently, we have upgraded our Sidekiq version from 5.2 to 6.5. Before Sidekiq 6.0 we were managing the Sidekiq process directly using Monit. With the release of Sidekiq 6, the team has removed the daemonization, logfile, and pidfile command line arguments and sidekiqctl binary.  Managing services manually is more error-prone, let our operating system do it for us.We have three options to go with systemd, upstart, and foreman. We decided to go ahead with the systemd.SystemdSystemd is a system and service manager for linux. Systemd tasks are organized as units. Most common units are services(.service), mount points(.mount), devices(.device), sockets(.socket), or timers(.timer)SystemctlThe systemctl command is a utility which is responsible for examining and controlling the systemd system and service manager.SidekiqSimple, efficient background processing for Ruby.Sidekiq running as Systemd ServiceTo manage Sidekiq we need to create a service file for Sidekiq which can be used to start, stop or restart the Sidekiq process.Sudo nano /lib/systemd/system/sidekiq.serviceContent in the Sidekiq.service. Sidekiq has provided us with the template for the service file here Sidekiq.service. We modified it according to our use case[Unit]Description=sidekiqAfter=syslog.target network.target[Service]Type=simple# If your Sidekiq process locks up, systemd&#39;s watchdog will restart it within seconds.#WatchdogSec=10WorkingDirectory=/opt/myapp/currentExecStart=/usr/local/bin/bundle exec sidekiq -C /opt/myapp/shared/config/sidekiq.yml -e productionExecStop=/bin/kill -TSTP $MAINPIDExecStartPost=/bin/sh -c &#39;/bin/echo $MAINPID &gt; /opt/myapp/shared/pids/sidekiq.pid&#39;ExecStopPost=/bin/sh -c &#39;rm /opt/myapp/shared/pids/sidekiq.pid&#39;User=deployGroup=deployUMask=0002# Greatly reduce Ruby memory fragmentation and heap usage# https://www.mikeperham.com/2018/04/25/taming-rails-memory-bloat/Environment=MALLOC_ARENA_MAX=2# if we crash, restartRestartSec=10Restart=on-failure# output goes to /var/log/syslog (Ubuntu) or /var/log/messages (CentOS)StandardOutput=syslogStandardError=syslog# This will default to &quot;bundler&quot; if we don&#39;t specify itSyslogIdentifier=sidekiq[Install]WantedBy=multi-user.target  Our Modified Configurations:            As we were using system ruby and using Sidekiq with some custom configurations. To start Sidekiq we used.ExecStart=/usr/local/bin/bundle exec sidekiq -C /opt/myapp/shared/config/sidekiq.yml -e production            To stop Sidekiq we need to send a TSTP signal to process all the busy jobs before terminating Sidekiq.ExecStop=/bin/kill -TSTP $MAINPID            For Managing with Monit we need the process id, After starting or stopping the service we were maintaining the process id file.ExecStartPost=/bin/sh -c &#39;/bin/echo $MAINPID &gt; /opt/myapp/shared/pids/sidekiq.pid&#39;ExecStopPost=/bin/sh -c &#39;rm /opt/myapp/shared/pids/sidekiq.pid&#39;            As we want to use our app user to run this service.User=deployGroup=deployUMask=0002            And we want to restart only when there is a failure.# if we crash, restartRestartSec=10Restart=on-failure      Reload the systemctl daemon for the new created serviceSudo systemctl daemon-reload  Now we can start the Sidekiq service:sudo systemctl start|stop|restart sidekiqMonitor Sidekiq process with MonitNow we have systemd to start, stop and restart the Sidekiq process. Now we will look at how to monitor the Sidekiq process with the help of monit.MonitMonit is a utility for managing and monitoring processes, programs, files, directories and filesystems on a Unix system.Modified monitrccheck process sidekiq with pidfile &quot;/opt/myapp/shared/pids/sidekiq.pid&quot;  start program = &quot;/bin/bash -l -c  &#39;sudo systemctl start sidekiq&#39; as uid deploy and gid deploy&quot;    with timeout 20 seconds  stop program  = &quot;/bin/bash -l -c  &#39;sudo systemctl stop sidekiq&#39; as uid deploy and gid deploy&quot;    with timeout 20 seconds  if totalmem is greater than 800 MB for 3 cycles then restart  if changed pid then exec &quot;/etc/monit/slack_notifier.sh&quot;  if cpu is greater than 65% for 2 cycles then exec &quot;/etc/monit/slack_notifier.sh&quot; else if succeeded then exec &quot;/etc/monit/slack_notifier.sh&quot;We can check if sidekiq is up and running:sudo monit summary sidekiqMonit will check the Sidekiq process and it will automatically start in case of the unexpected kill of the Sidekiq process.We have successfully completed the Sidekiq process monitoring with the help of Monit and Systemd.",
        "url": "/technology/sidekiq-process-in-production-with-systemd-and-monit/"
      }
      ,
    
      "the-other-side-outliers-the-story-of-success-book-review": {
        "title": "Outliers: The story of Success - Book Review",
        "author": "manish",
        "category": "the-other-side",
        "content": "For the last two months, I have been reading the book “Outliers” by Malcolm Gladwell. “Outliers - The story of success” has two parts: Opportunity and Legacy.“In outliers author survey the ingredients for the success. He wrote about the reason behind the success of great people like Bill Gates, Bill Joy, Joseph Flom, and the musical group Beatles. And how Chris Langan and Oppenheimer ended up with different stories. And how the culture, family, and friends play a role in determining individual success.”Key Factor of Success:1. OpportunityWe all have equal opportunities. Some people recognize and take advantage of them. But as per the author, In reality, these people are benefited from hidden and extraordinary opportunities. Culture benefits, the where and in which family and the time we grew up. Values received from our ancestors.2. Environment affectThe values of the world we live in and the people around us, have a profound effect on who we are. The place we live and the people we spend time with are critical factors to success. The author explains the Roseto Mystery. Why do the people of Roseto have good health and rare heart attacks? How community benefits play a dominant role in Roseto Mystery.3. Hard workIt takes 10000 hours to master anything from beginner to world-class expert in any field. “10000” hours is a lot, so it’s a good advantage for me to start at a young age.The author explains how Bill Joy from age of 16 started with computers. And from that day in university, when introduced to the computer, the place became his life, and he programmed whenever he gets time. He wrote the UNIX and the program for TCP/IP, which allows us to connect to the Internet. In his early days he devoted his 10,000 hours with passion and abilities.How Beatles were invited to play in a club where they had to play for a lot of hours, even the whole night. By the time the Beatles reached success, they played almost 12000 times and for more than 10,000 hours.4. Legacy often drives our behaviorGladwell points out how values passed from generation to generation. He points out the cultural legacy of Asian countries where rice is the dominant crop. How the success of rice paddy depends on the amount of hard work and diligence; we put in. To have a successful paddy, wake up at dawn and work all day. Which then creates a cultural legacy of hard work.",
        "url": "/the-other-side/outliers-the-story-of-success-book-review/"
      }
      ,
    
      "technology-creating-an-npm-package-from-my-react-component": {
        "title": "Creating an npm package from my REACT Component",
        "author": "piyush",
        "category": "technology",
        "content": "So, you have created a useful, customisable, modular component in REACT. Now, you want to share it with everyone by making a package so that anyone can install it ? That is exactly what I had done and now I also wanted to create a npm package and publish it and this is how I did it.PrerequisitesSince I was going to create a npm package, I needed to have Node and npm installed on my system.Also I needed a npm account. I didn’t have one so I had to create one before I got started. You can also create one from here.Getting StartedFirst order of business was to select an unique name for my package. I settled on react-rails-pagination as the name for my package.To confirm that no package with the same name existed I had to use the following command.npm search react-rails-paginationYou can usenpm search &lt;your-package-name&gt;And if no existing package is found with the same name, then you are good to go.After I selected a package name, I had to run the following command in my terminal to initialise the package.npx create-react-library react-rails-paginationI was prompted to answer a few questions about my package nowAfter entering all the information, it will automatically setup the project. This process might take a little time.The advantage of using create-react-library is that it will initialise your project to be published along with an example where you can test your package. It will also initialise it as a local git repository which you can simply push to github after adding the URL for your remote repository.After create-react-library finishes, the folder structure looks like thisI had to run the following commands in two different terminal tabs to start the development environmentcd react-rails-pagination &amp;&amp; npm startcd react-rails-pagination/example &amp;&amp; npm startThe first command watches the src/ and recompiles it into the dist/ folder when you make changes.The second command runs the example app that links to your package.Adding my REACT ComponentNow, I had a look inside the src/ folder in my project. There was an index.js file which held an ExampleComponent that was being used in the example app.To add my own REACT Component, I placed my Pagination.jsx file that held my Pagination component inside the src/ folder. Since, my component required a css file too, I placed my css file index.css inside the same folder as well. I import this index.css file inside my Pagination component.I don’t use a separate css module in my component so I deleted the generated styles.modules.css file inside the src directory.After I had done these changes, my src directory looked something like thisNow, I need to make sure that my component is being exported from this package, so that any project that uses my package, will get to use my component as well.For this I have to make some changes to the index.js file.import Pagination from &#39;./Pagination&#39;;export default Pagination;This imports my component into the index.js file and sets it as the default export from the package. I do this because the source file or the entrypoint of my package is the src/index.js file.If you don’t want to use the index.js file or want to create a new entrypoint then open the package.json file in the root of the project and change the value of the source key in that file.This completes the process of adding my component to the package.Checking if my package is working as expectedTo check if my package is working or not, I have to go to the example/ folder.In that folder, I have to edit the App.js file which imports the ExampleComponent that I modified earlier to use my Pagination component.import React from &#39;react&#39;import Pagination from &#39;react-rails-pagination&#39;import &#39;react-rails-pagination/dist/index.css&#39;const App = () =&gt; {  let page = 1;  const totalPages = 5;  const handleChangePage = (currentPage) =&gt; {    page = currentPage;  }  return &lt;Pagination page={page} pages={totalPages} handleChangePage={handleChangePage} /&gt;}export default App;These changes allow me to import my package into this example application and check if it is working or not.Now if I open the address that the local development server is running on in my browser, I can see that my component is loaded and functioning now.Publishing my packageI need to add a few things to get this package ready for publishing.First I add a .npmignore file to stop a few things from being included in my published package to reduce it’s size. It works the same as a .gitignore file but for npm in this case.The .npmignore looks like this in my project## the src foldersrc.babelrcrollup.config.js## node modules foldernode_modules## git repository related files.git.gitignoreCVS.svn.hg.lock-wscript.wafpickle-N.DS_Storenpm-debug.log.npmrc#othersconfig.gypipackage-lock.jsonNext I opened the package.json and added a few things in there as well.{  &quot;name&quot;: &quot;react-rails-pagination&quot;,  &quot;version&quot;: &quot;1.0.0&quot;,  &quot;description&quot;: &quot;React Pagination Component for Rails and other MVC Frameworks&quot;,  &quot;license&quot;: &quot;MIT&quot;,  &quot;repository&quot;: &quot;piyushswain/react-rails-pagination&quot;,  &quot;main&quot;: &quot;dist/index.js&quot;,  &quot;module&quot;: &quot;dist/index.modern.js&quot;,  &quot;source&quot;: &quot;src/index.js&quot;,  &quot;engines&quot;: {    &quot;node&quot;: &quot;&gt;=10&quot;  },  &quot;keywords&quot;: [    &quot;react&quot;,    &quot;rails&quot;,    &quot;mvc&quot;,    &quot;react-component&quot;,    &quot;pagination&quot;  ],  &quot;author&quot;: {    &quot;name&quot;: &quot;Piyush Swain&quot;,    &quot;email&quot;: &quot;piyush.swain3@gmail.com&quot;  },  &quot;homepage&quot;: &quot;https://github.com/piyushswain/react-rails-pagination&quot;,  .  .  .  .}I updated the author field to add my email.Next, I added the keys homepage and keywords.homepage can be used to add a website link to your project. I used my github repository link for now but I will change it later when I add a demo to this project. If you have a working demo, you can add that link in it’s place.The keywords key can be used to give the npm search directory keywords to attach to your project so that people using the npm search engine can find your project more easily. It takes an array of words as an argument.Finally, I update the README.md file in the root directory to add a description and instructions for anyone using my package. You will have to update your README.md according to your package as well.I review all the changes and then first push my code to my github repository.git remote add origin https://github.com/piyushswain/react-rails-pagination.git # Sets the new remote for the local repogit add .git commit -m &#39;Initial Commit&#39;git push -u origin main  # Pushes the changes to the remote repositoryNow, my package is ready to be published. I run the following commands to start the process of publishing my package to npm.npm loginLogin command asks for the username and password of your npm account. Enter those succeessfully and it will log you in to npm. If you have already logged in to npm, then you can skip this step.npm run buildThis optimizes and creates a production build for your package. I recommend running this everytime before you issue a publish command.npm publishFinally, running this command will upload your package to npm. You can check it in your npm profile where you can find all your uploaded packages.If you wish to publish again after making some changes then open your package.json file and update the version key to publish again. Remember to build your package before publishing as it will create an optimized production build for your package.TIP: If for some reason you cannot get the css to work, then a small hack is to directly update the dist/index.css file as this is the file that is published and used by anyone importing your package  You can find this article on the author’s blog piyushswain.github.io as well.",
        "url": "/technology/creating-an-npm-package-from-my-react-component/"
      }
      ,
    
      "technology-migration-from-paperclip-to-activestorage": {
        "title": "Migration from Paperclip to ActiveStorage",
        "author": "manish",
        "category": "technology",
        "content": "How we migrated hundreds of thousands of attachments from Paperclip to ActiveStorage without downtime.At eLitmus, recently we migrated thousands of attachment records from Paperclip to Rails-owned ActiveStorage. Paperclip and Active Storage solve similar problems - uploading files to cloud storage like Amazon S3, Google Cloud Storage, or Microsoft Azure Storage. In our case, we are uploading files to Amazon s3. And then attach those files to Active Records objects. So migrating from one to another is straightforward data-rewriting.Why do we migrate from paperclip to active storage?ActiveStorage was introduced in Rails version 5.2. At the time of migration, we were at Rails version 6.0. So, we were already running behind in keeping things up to date. Active storage is a highly recommended tool for uploading files. For a long, before ActiveStorage, this functionality was provided by outside gems, including Paperclip. With the release of Active storage, Paperclip was already deprecated for some time, and we wanted to move forward with Active Storage knowing it’s not as mature as Paperclip, but it’s owned by the rails’ community behind it. So we were happy with that.How do we migrate from paperclip to active storage?After reading articles on the web and the migration guide provided by the Paperclip process seemed pretty straightforward. We had around 2 Million records belonging to 16 different Active Records. In our case, we need migration that is fast and with no downtime. We had records in millions we cannot afford to wait for days to run migrations. We decided to do it in small steps. One step at a time, migrating all attachments of one Active Record. So a total of 32 Merge Requests were merged in production during this time. For each Active Record, two Merge Requests deployed because we didn’t want to have any unavailable attachments during the whole process, we split it into two steps or Merge Requests.So both steps revolve around the Paperclip and ActiveStorage. Let us refresh our understanding of how paperclip and active storage works. Paperclip works by attaching file data to the model. At the same time, it changes the schema of the model by introducing four columns in the Active Record table. It manages rails validations based on size and presence of file data if required.  create_table &quot;users&quot;, force: :cascade do |t|    t.string &quot;image_file_name&quot;    t.string &quot;image_content_type&quot;    t.integer &quot;image_file_size&quot;    t.datetime &quot;image_updated_at&quot;  endHere’s how it would go for a User with an image, that is this in Paperclip:  class User &lt; ApplicationRecord    has_attached_file :image    validates_attachment :avatar, presence: true,      content_type: &quot;image/jpeg&quot;,      size: { in: 0..10.kilobytes }  endOn another side, we start by installing ActiveStorage. Normally, Rails 6.1 already comes with it, so all we need is run:  rails active_storage:installActiveStorage creates three database tables ActiveStorageBlobs table storing attachment metadata, the ActiveStorageAttachments table, which is a polymorphic table between the blobs table and rails model and the ActiveStorageVariantRecords table tracks the presence of variant in the database. ActiveStorage doesn’t come with validations. we found some outside gems, including active_storage_validations which works for us.  create_table :active_storage_blobs do |t|    t.string   :key,      null: false    t.string   :filename,     null: false    t.string   :content_type    t.text     :metadata    t.string   :service_name, null: false    t.bigint   :byte_size,    null: false    t.string   :checksum,     null: false    t.datetime :created_at,   null: false    t.index [ :key ], unique: true  end  create_table :active_storage_attachments do |t|    t.string     :name,     null: false    t.references :record,   null: false, polymorphic: true, index: false    t.references :blob,     null: false    t.datetime :created_at, null: false    t.index [ :record_type, :record_id, :name, :blob_id ], name: &quot;index_active_storage_attachments_uniqueness&quot;, unique: true    t.foreign_key :active_storage_blobs, column: :blob_id  end  create_table :active_storage_variant_records do |t|    t.belongs_to :blob, null: false, index: false    t.string :variation_digest, null: false    t.index %i[ blob_id variation_digest ], name: &quot;index_active_storage_variant_records_uniqueness&quot;, unique: true    t.foreign_key :active_storage_blobs, column: :blob_id  endHere’s how it would go for a User with an image, that is this in ActiveStorage:  class User &lt; ApplicationRecord    has_one_attached :image    validates :image, attached: true,       content_type: &#39;image/png&#39;,      size: { in: 0..10.kilobytes }  endLet’s deep dive into the two steps we adopted, Migrated Paperclip data and Adopted ActiveStorageMigrated Paperclip DataIn this step, we did the most crucial part of the process, running a rake job to migrate paperclip data to active storage tables. We kept everything from the Paperclip as it is and, we also added support for Active Storage. We were using both functionalities at the same time. During the time, attachments for the model were migrated from Paperclip to ActiveStorage if a user decides to upload any attachments, the user still uses the paperclip implementation, but in the background after the successful commit of all transaction related to Paperclip. We were duplicating the same attachment to active storage by using Active Record Callback after_commit.What does our rake task flow look like?In this step, we created a rake task that copies all the data produced by Paperclip to the new ActiveStorage format.  Firstly, we pushed every column_name matching the Regex containing the file_name into the array. For example, we have a UserSignature model having a column image_file_name.  Secondly, for each instance of the model, created an ActiveStorage record only if ActiveStorage doesn’t contain a record for that instance. The reason for this is that for some reason, we cancel our rake task or it gets crashes, we had a choice to restart it from the place where it left off.  So for each instance, we were first constructing the direct URL of the attachment. Direct URL is the Amazon s3 URL to download the attachment from Amazon s3. We then pass on this direct URL to ActiveStorage::Blob create_and_upload! Method, which first downloads it and re-upload it to the s3 bucket. We then created the associated polymorphic ActiveStorage record.What challenges did we face running rake tasks?At eLitmus, models with CDN bucket configurations have less than 20 thousand records. For models with a limited number of records above approach works well for us. It looks quite straightforward for us. As soon we started migrating the Default bucket, with each model with records greater than 50,000, problems came arising. We started with records in increasing order of their count. For the Default bucket, we started our journey with 56,000 records by following the approach mentioned above. It took around more than 4 hours to migrate 56,000 in a staging environment. We can’t afford to wait for hours to migrate 56,000 attachments. So we had to come up with a different approach and, this is where things become interesting.After all the specs, we found that in the above approach, we have an open URI to download the attachment from Amazon s3 and re-upload it to the s3 bucket in the transaction that prolonged the database connection time. We came up with a different approach by designing our rake task; in such a way that instead of hitting s3 of every record, we decided to just come up with a database migration that copies all of the data generated from the paperclip to the new Active Storage required format. Paperclip adds attachment columns directly to the model’s tables such as image_file_name, image_content_type, image_updated_at, image_file_size. ActiveStorage stores this information in two dedicated tables ActiveStorageBlobs table and ActiveStorageAttachments table.We loop through the records of the model and then through each attachment definition within the model. If the model record doesn’t have an uploaded attachment, skip to the next record. Otherwise, we converted the Paperclip data to ActiveStorage records. We set the values for the new ActiveStorage records based on the data from Paperclip’s field for the ActiveStorageBlobs table.For the records with limited numbers, less than 1,00,000 approach works well for us. It took only 8 minutes to migrate 96,000 records. Our next target was to migrate around 4,50,000 migrate. We started migrating with the same approach we used for 96,00,000. But things do not go as straightforward. While migrating 4,50,000 maximum number of records in our Paperclip data had missing file size. As ActiveStorageBlobs table byte_size is the required field, We had to hit s3 API to fetch file size. It took around 4 hours in staging to migrate. On optimizing the rake task, we came up with another approach instead of reading data from a Paperclip column and then writing them to ActiveStorageBlobs at, same time. We decided to first read all the data from the Paperclip and then write it back to ActiveStorage. Firstly we read all the data from paperclip model columns and made them compatible with ActiveStorage Required format in CSV. Then we write data from CSV to ActiveStorage tables. It took 2 hours for us to migrate 4,50,000 records in production.With the same approach next, we migrated around 14,00,000 records and, it took 45 minutes in staging and 18 hours in production.Adopted Active StorageAfter the job finished, we removed everything related to the paperclip and replaced its usage with active storage.  We updated config files, added Amazon s3 storage definitions to storage.yml, and removed paperclip configuration for attachments related to the model. Updated model, views, and controllers related to Active Record. The red, green, and refactor approach helped us to improve confidence that our code was working as expected.What challenges did we face during migration?  Paperclip provides us several validators to validate our attachments. Out of the box, ActiveStorage doesn’t come with validations. We need to write custom validations in ActiveStorage, to add simple validations for attachments to validate presence, content type, attachment size. After some research, we found some outside gems, including active_storage_validations, provide us validators as Paperclip. As ActiveStorage is evolving day by day, validations are on the to-do list of the rails community. As soon as it is released, we will be ready to get the outside gem replaced.  At eLitmus, we were using two Amazon s3 buckets - default bucket and CDN bucket, to store our attachments. Paperclip provides us functionality to store attachments on different buckets by giving an option bucket name while uploading attachment data. We started migrating from Paperclip to ActiveStorage with our application rails version 6.0. In Rails 6.0, there was no such tool to categorize the bucket name while uploading an attachment. Almost half of the models in our application are using CDN bucket, and the rest are using default bucket. The Rails community is behind the ActiveStorage in the rails version 6.1 service column was introduced in the ActiveStorageBlobs table for categorizing the bucket name while uploading an attachment. So we migrated the first CDN bucket attachment with rails version 6.0. Then we upgraded our rails version to 6.1 and migrated the other half records to the default bucket.  After the migration of 14,00,000 records after a week, we encountered a bug in production around 500, records key were missing from the amazon s3 bucket. After few hours of debugging, we found that between the time,  1st and 2nd MR’s merge in production. During, this period we kept everything from the paperclip as it is we, also added support for Active Storage. We were using both functionalities at the same time. During the time attachment for the model were migrated from paperclip to active storage, if a user decides to upload any attachments, the user still uses the paperclip implementation, but in the background after the successful commit of all transaction related to paperclip. We were duplicating the same attachment to active storage by using Active Record Callback after_commit. We produce the bug when the user uploads the attachment with the same filename as in our database before the migration process. We accidentally deleted the record’s key from amazon s3. After specs and debugging we, came up with a solution to recover these deleted files from amazon s3. We created a new rake task for recovering the deleted files from s3 by deleting the latest delete markers version for the key from s3. And all files were successfully recovered and working fine now on production.  After three weeks, we encountered another problem in production. Some of our users reported to us with queries that some of them were having problems uploading a resume. After specs and analysis, we figure out that for around one thousand resume records, there were two ActiveStorage attachments for them in ActiveStorage tables. As ActiveStorage works on the principle that for one ActiveRecord object, there will be one ActiveStorage attachment for has_one_attached relationship. During specs, one more problem comes to our front that on our database there were around 3 thousand active storage attachments with missing resume ActiveRecord objects. After deep-diving into the codebase, we figured out that due to our daily cron job, which deletes all inactive users from our database. So for the past three weeks, this job was deleting all the ActiveRecord objects but not ActiveStorage Attachments. On the solution part, we first decided to restrict inactive users to upload the attachments without activating their accounts and updated cron jobs to delete all the ActiveStorage attachments associated with the ActiveRecord object whenever it is deleted. On the other hand, to match the same number of our ActiveRecord objects and ActiveStorage attachments for resumes, we created three rake tasks. The first one to remove all attachments except the latest one from the ActiveStorage tables for an ActiveRecord object with more than one attachment in ActiveStorage tables. The second one, to filter out all the active storage of type resumes which doesn’t have any records for them in the resume table. And saved active storage attachment ID and resume ID in CSV. The third one, that processes CSV generated in the second rake task and deletes all the active storage records associated with them from active storage tables. It took around 15-20 min to run all three rake tasks. As a result of it, both the ActiveRecord and ActiveStorage number matched. Now, it’s running fine on production. We have not received any queries yet.ConclusionActiveStorage has now been in production for over a week, and it’s been seamless. It provided us everything we needed though they are certainly more things that need to be evolved validations for attachments, supporting directory structure for active storage blob key. Looking Forward to seeing active storage evolve. And this will conclude our journey regarding migration from paperclip to ActiveStorage.",
        "url": "/technology/migration-from-paperclip-to-activestorage/"
      }
      ,
    
      "technology-revamp-of-our-coding-platform": {
        "title": "Revamp of our Coding Platform",
        "author": "piyush",
        "category": "technology",
        "content": "The story of how we took a good platform and made it even betterBefore I start telling you this story, I want to just make this clear that this is not filled with technical details of our implementation but rather with the thought process and the journey of redeveloping our coding platform. I will definitely share my learnings and some technical details of this whole endeavour in later blog posts.So, this story starts in October 2019, with me looking at a web application that I inherited from previous developers at the organisation that I had joined 6 months earlier and thinking to myself that, “Here we have a perfectly functional web application that does it’s job pretty well, but still why does it feel so underwhelming and out of place on the modern web”.RealizationWhat I realized after 2 days of pondering on this topic was that, with the way web applications and their popularity has been growing in our times, The UX had become as important as the function of the web application.If that was not clear, then let me explain further. In broad terms we can breakdown the components of a web application into 2 areas -  Back End or Server Side components  Front End or User Side componentsThe Back End controls the functions of the web application, what it can do and how efficiently it can do that task.While the Front End dictates the interaction between the user and the application.Now, both of these components need to be as good as the other one to ensure that your web application provides a seamless experience. In the case of our coding platform, this was not true, as we had a great Back End implementation but the Front End felt like it was still stuck in the early 2010s.Identify the IssueI knew I wanted to change this platform, but it was important to focus on a few specifics instead of getting bogged down by all the things I wanted to improve. So, I sat down with my colleague Shubham Pandey (Please do checkout his blog and website. He has some amazing stuff on there) and we tried to categorise the problems in the platform under a few broad umbrellas.Experience - We used this category to encompass all the problems that were related to causing an inconvenience to the user who was using our platform.Some of the problems we put under this category were things like the user not being able to see the list of problems while coding, the user’s event time starting before they can see the editor, not being able to see the result and the problem statement at one time and a few more things similar to these problems.Interface - We brought all the issues regarding design, layouts and colours on the platform under this category.Problems like the text being too small in some places, buttons not being of a standard size, the event timer not eye catching feature of the design and again a few more problems similar to these ones.The actual list was a lot longer than mentioned here but, all of them importantly came under these two broad categories.Setting ObjectivesNow that we had our problem well-defined, we could move on to coming up with a plan of action to solve these problems. To solve these problems we started thinking like a user who had minimal technical background to give us a set of objectives.One of the biggest issues we noticed was the number of clicks that a user had to go through to reach the problem and start coding. On the old platform, a user had to go through the following steps to start their event:=&gt; Login=&gt; Find event on dashboard=&gt; Click on \"Load Challenge\"=&gt; Find/Select a problem from the list=&gt; Click on \"Start\"=&gt; Start CodingThis was a lot of clicks to start an event on a platform dedicated to hosting coding events and we needed to reduce this as every click meant a complete page reload.  Objective 1: Reduce the amount of clicks a user needs to reach the Coding Test  Objective 2: Minimise Page ReloadsAnother issue was the dated look and feel of the UI. It did not feel slick or intuitive. This might have been a very good UI by 2012 standards but for 2019 it was not up to the mark.  Objective 3: Modernise the UIWe found another issue with the editor we were using on the platform. We used Codemirror on the older platform which although was a good editor, had a few problems that were holding it back. The size of the library was huge, we had to load multiple script tags to access the full set of features, few editing options were missing and some more.P.S : After the recent Codemirror 6 updates some of these problems were solved but at that time there was no confirmation if that would be the case.  Objective 4: Use a featureful coding editor with long term supportSo, these were the 4 objectives that we set out to achieve in the first version of our new coding platform. Even though this was technically an overhaul of an existing project, we had started calling it “new” so that we start thinking for solutions from scratch instead of just updating a few things and complicating the whole code base and the project even more.Plan of Action and ExecutionTo achieve our 4 objectives, we selected the following libraries and plugins and I will also briefly explain why we opted for these:  REACT  Bootstrap  Monaco EditorTo achieve Objectives 1 &amp; 2.We decided that we had to change flow of the user journey on the platform.This was the only way that we could reduce the amount of clicks on the platform and for minimising page reloads, REACT came to our rescue.REACT allowed us to develop, what we call a SPA (Single Page Application) quite easily and without much hassle.I will explain the specific use cases and advantages of a SPA in a future blog post.Also an added benefit was that REACT had a pretty simple integration with our existing application which is a Ruby on Rails based web application. We integrated REACT into our Rails 5.2 application using the webpacker gem.After Rails 6 the webpacker gem now comes as standard with Rails so using REACT as front end for a Rails application has become easier now.Bootstrap is a very popular library that makes developing beautiful UIs very simple with its plethora of classes and functionality that it offers. So, that was a very obvious choice to achieve Objective 3.And lastly Monaco Editor is also a very popular and well-supported coding editor. It is being officially maintained by Microsoft and contains a lot of features that Virtual Studio Code Editor provides on a desktop. That makes it an obvious choice when we were deciding on an editor to use for our platform to achieve Objective 4.Now you can check out the redeveloped platform and see how we executed our plan.Remember, the number of clicks the older platform required to get to the actual coding? That has been reduced to the following now in this new platform:=&gt; Login=&gt; Find event on dashboard=&gt; Click on \"Load Challenge\"=&gt; Start EventThat’s it. Everything was compressed into a single page to provide a more intuitive and easy to use coding platform that would allow the candidate to focus on coding more than worrying about other things. We tried to make everything else like time, problem list, result etc. available at a glance whenever the candidate needs it.And if you are wondering, we did add a “Dark Mode” also, which has become quite the rage nowadays in modern web design. Notice the sun and moon icons on the right edge of the top bar that denoted the Light and Dark Modes respectively.So, that was the story of how we did a complete overhaul of our coding platform to make it fit for the modern web.It took us about 2 months to complete this project, from coming up with the concept, finalising the technical specifications, development, testing and finally deployment.The process that we followed is what I still use whenever I have to come up with a solution to any problem. That is probably the biggest learning that I took from this project along with learning REACT and developing Single Page Applications that I use quite a lot now.  You can find this article on the author’s blog piyushswain.github.io as well.",
        "url": "/technology/revamp-of-our-coding-platform/"
      }
      ,
    
      "technology-migrating-from-state-machine-to-aasm-in-rails": {
        "title": "Migrating from state_machine to aasm in Rails",
        "author": "akash",
        "category": "technology",
        "content": "First things first. State machines are awesome, be it any part of technology you use them in.Recently at work, we passed many pipelines on migrating a very large Rails app from Rails 4 to Rails 5. One of the major parts of this change was shifting from state_machine to aasm for our state transitions. We rely heavily on state machines for how our instances shift states. Much of our tasks associated with the models too are integrated with the after/before actions of our state machines.Need for transition:One and only one reason, state_machine has been dead, and for quite some time. We shifted from Rails 3.2 to Rails 4.2 last year, and since it was a really, really painful migration, we fixed our focus on changed syntax and ActiveJob, found the much famous monkeypatch for Rails 4.2 and stayed happy for the time being with state_machine. Though there is state_machines_activerecord, we wanted to move to a more reliable and tested library, and as we already use acts_as_state_machine or aasm in one of our other projects, we tried and gave it a shot, when we began our Rails 5 voyage, for which of course neither state_machine and its patch worked, nor it was recommended.What changed:As it turned out, the process was not too messy. After a small study of the way both state_machine and aasm handle state transitions, one can easily find an analogy. Here are a few things which usually are a part of a state_machine laden project and how they should be modified to work with aasm1. The gem itselfGoes without saying, remove from your Gemfile/gems.rb :  gem &#39;state_machine&#39;and add :  gem &#39;aasm&#39;2. Get rid of the state_machine monkey-patch if present  module StateMachine    module Integrations      module ActiveModel        public :around_validation      end      module ActiveRecord        public :around_save        def define_state_initializer          define_helper :instance, &lt;&lt;-end_eval, __FILE__, __LINE__ + 1            def initialize(*)              super do |*args|                self.class.state_machines.initialize_states self                yield(*args) if block_given?              end            end          end_eval        end      end    end  endYes, get rid of this if you have it, most probably in one of your config/initializers.3. Transitioning the transitions:This is the major part of the change and yet the easiest to implement. This includes code change in models. Take a look at the documentation over at aasm and start changing the code. Here are a few pointers.add include AASM to your model  class Question &lt; ActiveRecord::Base    include AASM    ...  endspecify the column name on which you are observing state transitions, for eg. if the column name is state  class Question &lt; ActiveRecord::Base    include AASM    ...    aasm.attribute_name :state    ...  endInitiate your state machine block by listing out all your states. The common way is using one line to specify your initial state, and a second line to list all your non-initial states  class Question &lt; ActiveRecord::Base    include AASM    ...    aasm.attribute_name :state    aasm do      state :authored, initial: true      state :piloted, :non_active, :active, :removed      ...    end    ...  endConvert your events. All event blocks of the form transition :a =&gt; :b will be replaced by transitions from: :a, to: :b  class Question &lt; ActiveRecord::Base    include AASM    ...    # State machine code    state_machine :state, initial: :authored do      event :pilot do        transition :authored =&gt; :piloted      end      event :activate do        transition [:piloted, :non_active] =&gt; :active      end      ..    end   # AASM code    aasm.attribute_name :state    aasm do      state :authored, initial: true      state :piloted, :non_active, :active, :removed      event :pilot do        transitions from: :authored, to: :piloted      end      event :activate do        transitions from: [:piloted, :non_active], to: :active      end      ...    end    ...  endCallbacks like before_transition and after_transition from state_machine can be converted like this:  class Question &lt; ActiveRecord::Base    include AASM    ...    # State machine code    state_machine :state, initial: :authored do      before_transition :authored =&gt; :piloted, :do =&gt; :prepare_cockpit      after_transition :authored =&gt; :piloted, :do =&gt; :fly_the_plane      event :pilot do        transition :authored =&gt; :piloted      end      ...    end    # AASM code    aasm.attribute_name :state    aasm do      state :authored, initial: true      state :piloted, :non_active, :active, :removed      event :pilot do        before do          prepare_cockpit        end        transitions from: :authored, to: :piloted, after: :fly_the_plane      end      ...    end    ...    def prepare_cockpit      ...    end    def fly_the_plane      ...    end  endHowever, in case of callbacks on a part of a transitions defined inside an event, one needs to define the transitions separately  class Question &lt; ActiveRecord::Base    include AASM    ...    # State machine code    state_machine :state, initial: :authored do      after_transition :authored =&gt; :piloted, :do =&gt; :fly      event :pilot do        transition [:inactive, :authored] =&gt; :piloted      end      ...    end    # AASM code    aasm.attribute_name :state    aasm do      state :authored, initial: true      state :piloted, :non_active, :active, :removed      event :pilot do        transitions from: :authored, to: :piloted, after: :fly        transitions from: :inactive, to: :piloted      end      ...    end    ...    def fly      ...    end  endif and unless guard blocks on transitions work the same way as in state_machine, and can also be substituted with a guard clause. The guards as well as callbacks can take arguments, lambda as well as Proc, same as the state machine guards  class Question &lt; ActiveRecord::Base    include AASM    ...    # State machine code    state_machine :state, initial: :authored do      event :pilot do        transition :authored =&gt; :piloted, if: :can_fly?      end      ...    end    # AASM code    aasm.attribute_name :state    aasm do      state :authored, initial: true      state :piloted, :non_active, :active, :removed      event :pilot do        transitions from: :authored, to: :piloted, guard: :can_fly?      end      ...    end    ...    def can_fly?      ...    end  endYes, that’s it for the models. You can take a detailed look at the docs if you have more complex needs.4. The helpers:One plus point for state_machine , it has/had a variety of useful helpers for making use of states and events in views and controllers. aasm, though lagging behind a little in this domain, still has a good pool of helpers, both class and instance to make good use of. Here are some pointers.  Question.aasm.states will give you an object list of all states available for the Question model  Question.aasm.events will give you an object list of all events available for the Question model  Question.first.aasm.states will give an object list of all states available for transitioning to for a Question object, in this case the first one.  Question.first.aasm.events will give an object list of all events that can be applied on the current state of the Question object, i.e the first  All of the above helpers will produce an object list that contains name as the name of object, so appending .map(&amp;:name) will give a symbol array of the name of objects, that will come handy in drop-downs. Eg.  pry(main)&gt; Question.last.aasm.events.map(&amp;:name)  =&gt; [:pilot, :deactivate]Another great point in favor of state_machine is its state_event attribute over the instance. For eg.  pry(main)&gt; question = Question.first  pry(main)&gt; question.state_event = :deactivate  pry(main)&gt; question.saveThe code above will end up saving the question after calling the deactivate event over it. This attribute is highly useful in rails forms where one can easily pass what event to call from, and the transition will happen without extra hassle. Unfortunately, there’s no equivalent attribute cum method in aasm . But one can always write a common ActiveRecord::Base helper for the same.On another note, the not-so-good-looking with_state / with_states scope methods of state_machine can be replaced by the enum equivalent syntax of aasm . For eg.  Question.with_state(:active) # state_machinegets replaced by a much cleaner :  Question.activeSo yes, a couple of tweaks here and there, and a good pool of existing test cases which run green, you are done and production ready. This will get you started, but do back yourself up with the aasm docs.",
        "url": "/technology/migrating-from-state-machine-to-aasm-in-rails/"
      }
      ,
    
      "technology-android-versioning-using-docker-and-git-like-a-pro": {
        "title": "Android Versioning Using Docker &amp;amp; Git Like A Pro",
        "author": "mukku",
        "category": "technology",
        "content": "Unlike web, android still lacks the ease of version deployments. Specially when you don’t want to use Play Store.IntroductionThere will be five stages:  Signing application  Versioning of application. For that we gonna use git revision and Major.Minor.Patch naming convention.  Building application using a docker. So that running environment doesn’t change.  Pushing new release to s3, while maintaining the previous versions.  Pushing new tag to git, with the new version. So, we’ll have tags for each version.Basically, we gonna use docker, git, and some simple hacks to put things in work. In the end, I’ve shared a sample application.Stage 1: Signing Our ApplicationIt’s better to start thinking about security right from the big bang.From android studio, you can generate a new keystore, a jks file. Help?Copy the keystore file details in a config.yaml file like below:key_store:  key: /xyz/xyz.jks  alias: key0  store_password: wuhoo  key_password: nibataungaStudio will take care of signing, but to generate signed apk from command line, you’ll need to make some changes in your build.gradle. The credentials we have put in above yaml file will be passed as command line args to gradle(Build stage[2]).android {    ...    signingConfigs {        release {            if (project.hasProperty(&#39;APP_RELEASE_STORE_FILE&#39;)) {                storeFile file(&quot;$APP_RELEASE_STORE_FILE&quot;)                storePassword &quot;$APP_RELEASE_STORE_PASSWORD&quot;                keyAlias &quot;$APP_RELEASE_KEY_ALIAS&quot;                keyPassword &quot;$APP_RELEASE_KEY_PASSWORD&quot;            }        }    }    buildTypes {        release {          ...          if (project.hasProperty(&#39;APP_RELEASE_STORE_FILE&#39;)) {              signingConfig signingConfigs.release          }        }    }}Stage 2: Release Versioning, Digging GitI’am here using the semantic versioning.Major.Minor.GitRevision.PatchLet’s dig into GitRevisionIt counts the number of commits from git, so you’ll get incremental values everytime you release a new version. GitRevision will make versioning easy and consistent.We’ll put the below code in build.gradle[app]def getGitRevision = { -&gt;    try {        def stdout = new ByteArrayOutputStream()        exec {            standardOutput = stdout            commandLine &#39;git&#39;, &#39;rev-list&#39;, &#39;--first-parent&#39;, &#39;--count&#39;, &#39;master&#39;        }        logger.info(&quot;Building revision #&quot;+stdout)        return stdout.toString(&quot;ASCII&quot;).trim().toInteger()    }    catch (Exception e) {        e.printStackTrace();        return 0;    }}And in build.gradle[app]    defaultConfig {        ...        versionCode = 10000000*majorVersion+10000*minorVersion + 10*revision        versionName = &#39;v&#39; + majorVersion + &#39;.&#39; + minorVersion + &#39;.&#39; + revision + patch    }Docker Image, SavageWe first need to build a docker image with minimum libraries and dependencies required.FROM openjdk:8RUN apt-get updateRUN cd /opt/RUN wget -nc https://dl.google.com/android/repository/sdk-tools-linux-4333796.zipENV ANDROID_HOME /opt/android-sdk-linuxRUN mkdir -p ${ANDROID_HOME}RUN unzip -n -d ${ANDROID_HOME} sdk-tools-linux-4333796.zipENV PATH ${PATH}:${ANDROID_HOME}/tools:${ANDROID_HOME}/tools/bin:${ANDROID_HOME}/platform-toolsRUN yes | sdkmanager --licensesRUN yes | sdkmanager \\      &quot;platform-tools&quot; \\      &quot;build-tools;27.0.3&quot; \\      &quot;platforms;android-27&quot;RUN apt-get -y install rubyRUN gem install trollopTrollop will be helpful in compiling scripts, spicing the boring command line args.We are using openjdk as base image for java environment and installed our sdk with version 27. You can change that accordingly.Building the image:docker build -t ${docker_image} -f ./scripts/Dockerfile .Or you can directly pull my latest base image.docker pull mukarramali98/androidbaseDocker container on the wayTo automate the process, let’s dig into a small script:#!/usr/bin/env bashset -xeuo pipefailapp_name=xyzcontainer_name=androidcontainerif [ ! &quot;$(docker ps -q -f name=${container_name})&quot; ]; then    if [ &quot;$(docker ps -aq -f status=exited -f name=${container_name})&quot; ]; then        # cleanup        docker rm $container_name    fi    # run your container    docker run -v ${PWD}:/${app_name}/ --name ${container_name} -w /${app_name} -d -i -t mukarramali98/androidbasefidocker exec ${container_name} ruby /${app_name}/scripts/compile.rb -k /${app_name}/config.yamlHere we first check if the container already exists. Then create accordingly.While creating the container, we mount our current project directory. So next time we run this container, our updated project will already be there in the container.Stage 3: Running container, Build StageWe run the container, with our compile script. Pass the signing config file we created earlier.config = YAML.load_file(key_config_file)key_store = config[&#39;key_store&#39;]output_file = &#39;app/build/outputs/apk/release/app-release.apk&#39;`rm #{output_file}` if File.exists?output_fileputs `#{File.dirname(__FILE__)}/../gradlew assembleRelease --stacktrace \\    -PAPP_RELEASE_STORE_FILE=#{key_store[&#39;key&#39;]} \\    -PAPP_RELEASE_KEY_ALIAS=#{key_store[&#39;alias&#39;]} \\    -PAPP_RELEASE_STORE_PASSWORD=&#39;#{key_store[&#39;store_password&#39;]}&#39; \\    -PAPP_RELEASE_KEY_PASSWORD=&#39;#{key_store[&#39;key_password&#39;]}&#39;`Stage 4: Pushing to S3So, now we have build a signed apk from a docker container. It’s time to push them.Connect with your s3 bucket and generate $HOME/.s3cfg file, and pass it to ruby script below:if File.file?(s3_config)  # Push the generate apk file with the app and version name  `s3cmd put app/build/outputs/apk/release/app-release.apk s3://#{bucket}/#{app_name}-#{version_name}.apk -m application/vnd.android.package-archive -f -P -c #{s3_config}`  # application/vnd.android.package-archive is an apk file format descriptor  # Replace the previous production file  `s3cmd put app/build/outputs/apk/release/app-release.apk s3://#{bucket}/#{app_name}.apk -m application/vnd.android.package-archive -f -P -c #{s3_config}`  # To keep the track of latest release  `echo #{version_code}&gt; latest_version.txt`  `s3cmd put latest_version.txt s3://#{bucket}/latest_version.txt -f -P -c #{s3_config}`  `rm latest_version.txt`  puts &quot;Successfully released new app version.&quot;endapplication/vnd.android.package-archive is the apk file type descriptor.Stage 5: Finally, Git Tagging The New Release Version, #hashtagdef push_new_tag version_name  `git tag #{version_name}`  `git push origin #{version_name}`  puts &quot;New tag pushed to repo.&quot;endDemo Application",
        "url": "/technology/android-versioning-using-docker-and-git-like-a-pro/"
      }
      ,
    
      "career-how-tough-is-it-to-score-well-in-board-exams": {
        "title": "How Tough is it to Score Well in Board Exams?",
        "author": "arijit",
        "category": "career",
        "content": "I completed my entire schooling (Classes I through XII) at one of Kolkata’s favoured catholic schools. In those days, discipline and academic excellence were the primary parameters that mattered and my school checked both these boxes rather well.Trouble started brewing once I graduated to Class XI and started thinking about higher education, specifically opportunities at the national level. That’s when I truly realized the impact of the education board. In my case, the impact was limited to a couple of aspects, viz. a) subjects / topics not covered in the Bengal board syllabus, and b) the frugality in awarding marks.Fortunately, some extra tuitions covered up for the former, while the latter did not come into play at all in any of the options I signed up for, or in the higher education option I finally opted for.Times have changed….For a few years, till 2016, 40% weightage was accorded to an applicant’s Class XII board marks in calculating her All India Rank in the JEE (entrance tests for admission to India’s flagship IITs, and a few other engineering schools) exams. However, since 2017, the rules were changed to treat the Class XII marks as a qualifying criterion: a minimum of 75% marks, or a rank in the top 20th percentile in the board.The 75% cut-off may appear inconsequential to folks intimately familiar with the CBSE or ISCE boards, but not all students find it amusing. The JEE implementation committee publishes the 80th percentile cut-off marks for every higher secondary educational board in the country to level the playing field. Finally, we have access to data that clearly shows the disparity in awarding marks across boards in India.According to data for the 2016 Class XII exams, the 5 most liberal boards are (percentages indicate the 80th percentile cut-off score):  Telengana Board of Secondary Education (95%)  Andhra Pradesh Board of Intermediate Education (94%)  Council for the Indian School Certificate Examinations (88.6%)  Banasthali Vidyapeeth, Rajasthan (87.4%)  Tamil Nadu Board of Higher Secondary Education (87.2)While the 4 most frugal boards are:  Tripura Board of Secondary Education (59.8%)  Jharkhand Academic Council (60.6%)  Meghalaya Board of Secondary Education (61.6%)  Odisha Council of Higher Secondary Education (62%)  Bihar Intermediate Education Council (63%)What this essentially means is that a student scoring 95% in the Telengana board exams is academically comparable to a student scoring 60.6% in the Jharkhand board exams, despite a whopping 34.6% gap is scores!The data clearly shows how a single mark-based cut-off or a mark-based weightage criterion can result in gross injustice to students from boards that are frugal in awarding marks! Thankfully, the JEE implementation committee, in its infinite wisdom, has taken steps to normalize this inherent disparity.Time will tell whether the practice of allotting an explicit or implicit weightage to board exam performance will become the norm, not only in JEE but in other national level entrance tests as well. But for now, this is definitely something for parents to consider while looking for a school for their children.But what about employment? It is common practice among potential employers to set mark-based cut-offs for board exams (while hiring entry-level talent), among others. And in almost all cases, the cut-off is a single number applicable across the board (pun intended!).Let’s say company X sets a Class XII marks cut-off at 75%. Referring to the 10 boards listed above, company X will end up considering a population far larger than the top quintile from to 5 most generous states, and a population far smaller than the top quintile in the 5 most frugal states. The playing field is not so level anymore…..",
        "url": "/career/how-tough-is-it-to-score-well-in-board-exams/"
      }
      ,
    
      "career-its-the-attitude-stupid": {
        "title": "It's the Attitude, Stupid!",
        "author": "arijit",
        "category": "career",
        "content": "Congratulations on your first job! So you cleared the selection process; cracked some tests maybe, shone through a group discussion possibly, and impressed and charmed your way through the interviews. Well done!You’ve been assessed and found suitable for the job on offer. No more evaluations, no more assessments. Right?Wrong! Let’s get one thing straight. This is just the beginning of your evaluation. What you’ve achieved (and by no means is it trivial) is to convince your future employer that you have potential. However, from day one in your first job, you will be continuously assessed on what you deliver.For whatever it’s worth, taking the liberty to share a cheat sheet that may help in ensuring that promise does translate to delivery in the first couple of years in your career. And be warned that it’s all in the attitude…Cheat 1Sumadhur was undoubtedly brilliant. Armed with a degree from India’s best engineering school, he was given complex tasks in line with his academic record and promise.However, very soon, his manager realised that he was not able to complete most of his tasks. The manager tried to find probable reasons behind such repeated failures. Soon, he realised that Sumadhur was not open to data, insights or feedback that conflicted with his own assumptions and beliefs.Tell yourself at least once each day: “I do not know anything. I am here to learn and apply.”Cheat 2Abhinav had completed just 6 months in his first job, but had been upset for a while. His friends earned twice as much as him, despite working fewer hours than he did. They seemed to be having a rollicking time! Abhinav started looking around and got a 50% higher offer. He took the offer up with glee. Not only was the pay better, but the work was less demanding and had relaxed deadlines. Life could not be better…Two years down the line, Abhinav started looking for yet another change, and yet another quantum leap in compensation. However, he found, to his dismay, that he was way out of depth and no company was willing to make him an offer.Convince yourself that reward follows performance, and not the other way around.Cheat 3Kanu was very sharp. However, four years of hostel life had turned his biological clock upside down. When he did make it to office during normal working hours, he excelled in his tasks. Unfortunately, on most days, he simply could not.Within six months, he was asked to leave.Change your college habits to the extent required, so as to ensure that you are available (and awake!) when your work and / or team needs you to be.Cheat 4Manish was a star in college. He excelled in academics and was popular among students as well. Out of sheer habit, he continued behaving in a brash manner with his peers in the workplace.He was counselled by his manager, but was unable to mend his ways in time and was asked to leave.Accept the fact that you need to interact with people across age groups and cultures. You are not expected to like or respect one and all (after all, respect is earned!). However, you are expected to treat one and all with respect.",
        "url": "/career/its-the-attitude-stupid/"
      }
      ,
    
      "the-other-side-fishing-in-troubled-waters": {
        "title": "Fishing in Troubled Waters",
        "author": "arijit",
        "category": "the-other-side",
        "content": "Back in my final year in college, I applied for a job at an FMCG major hiring for Techno-managerial roles. I cleared the eligibility criteria and the group discussion. The short and not-so-sweet interview went something like this:Q: Did you apply to any core companies?A: Yes, I applied to X and Y.Q: What happened?A: I did not clear the written test.Q: What were the tests about?A: They were technical tests.Q: Why do you think you did not clear?A: Majority of questions were from courses taught in my second year. Since I had not brushed up on concepts, I could not answer many questions.Q: We expect our hires to know their domain. We cannot train them. It’s clear that you do not know your domain. Thank you for your time.A: (Thinking)… wow what just happened???I was asked the same set questions in a subsequent interview and, needless to say, I did not repeat all my answers (though I did not lie). The irony of it all is that, not only was I made an offer, I accepted the offer, spent 8 years in that organization and did fairly well in a core technical role!Since I moved over to the other side, I have kept revisiting to my own experience in that FMCG interview. My key takeaways are:      Aptitude or natural ability matters orders more than knowledge in a particular domain, especially in entry level roles.        In today’s dynamic landscape, it is almost guaranteed that specific skills will become obsolete with time. Therefore, the relevance of employees will persist only if they have the ability to learn new skills. Once again, natural ability and mindset will come to the fore.        There was no Google in those days. Nor had anyone imagined Facebook, LinkedIn or Twitter. Today, any half-serious candidate can dig up a bunch of information on a company’s recruitment process, right down to the kind of questions asked in tests and interviews, in no time whatsoever. Generic questions will lead to rehearsed “correct” answers, which have no correlation with the actual aptitude, mindset, skill or aspirations of the candidate.  I find it amusing that many organizations still follow the same two-decade old formulae to hire people. To be sure, it may very well work for certain organizations. However, it is worth taking a dispassionate look at whether your process indeed works like you expect it to. Are you convinced that you are not ending up hiring the “wrong” folks, or worse, missing out on the “right” ones?",
        "url": "/the-other-side/fishing-in-troubled-waters/"
      }
      ,
    
      "the-other-side-hiring-may-be-rocket-science-but-its-tenets-are-basic": {
        "title": "Hiring may be Rocket Science, but its Tenets are Basic",
        "author": "arijit",
        "category": "the-other-side",
        "content": "Hiring is as old as employment itself. We are talking about a time period of a few thousand years – not a fact to be trifled with. The methods, tools and approaches may have changed and evolved with time, but I believe that it’s safe to say that the underlying principles would not have changed much.Recall instances where you were evaluating a potential candidate - and it does not matter whether you were looking for someone to help out at home, or someone to drive your vehicle, or someone to join your team at work. Consciously or subconsciously you would be ticking checkboxes against these basic principles… Not convinced? Okay, allow me to lay them out for you – I’ll stick my neck out on this one.Tenet 1Notwithstanding his ability with the bat, fitness levels and mental strength, would MS Dhoni have been successful as an opener in Test Cricket? Most likely not!What are you assessing?Does she have an aptitude for this job? In other words, does she have a natural ability to perform this job?Tenet 2An ex-colleague of mine, a product of India’s best t-school, was undoubtedly brilliant. However, he consistently failed to meet his goals. Reason: either lost interest in the last mile, or buckled under deadline pressures. Sounds familiar?What are you assessing?Does she have the right mindset? How often have we seen – and heart wrenchingly so – that aptitude alone cannot and does not guarantee success on the job?Tenet 3Ever wondered why a majority of software developers are engineers?What are you assessing?Does she have the necessary skills? Skills could be knowledge, experience or even academic qualification? Even if you have a top class training facility at your disposal, your fresh hires will need a minimum set of pre-existing skills that are critical to succeed.Tenet 4A high performing and reliable colleague decided to put in his papers because he wanted to migrate to the US.What are you assessing?Do her aspirations align with what you can offer her? And this is not just about compensation and benefits. But more importantly, a match between career aspirations and available growth paths in your organization.Would love to hear your views, especially if you disagree or have a contrarian opinion. Keep them coming…",
        "url": "/the-other-side/hiring-may-be-rocket-science-but-its-tenets-are-basic/"
      }
      ,
    
      "career-money-get-away-get-a-good-job-with-more-pay": {
        "title": "Money, get away, Get a good job with more pay and you're OK?",
        "author": "arijit",
        "category": "career",
        "content": "Many commentators, most of them far wiser than me, have coined brilliant terms to refer to the generation gap that exists in the Indian demographic today. Since the workplace is but a small subset of the overall demographic, this generation gap is equally alive and kicking there as well.In my humble view, the workforce in any organization can be divided into two generations:Those who completed schooling before the turn of the millennium, andThose who did so in the present centuryI’d like to see the former as the RKM (Roti Kapda Makaan) generation, whereas the latter is more of the YOLO/FOMO (You Only Live Once/Fear Of Missing Out) kind. I neither desire, nor possess the ability, to perform a cost-benefit analysis of either approach towards personal finance. However, running the risk of generalization, this is probably how each generation judges the other:RKM (judging YOLO/FOMO): Where are your savings? How can you blow up your entire salary within the first fortnight? Let me show you how to manage your finances.YOLO+FOMO (judging RKM): Papa don’t preach! Get a life!Irrespective of whether money is funny or not for you or whether you desire to write a suicide note on a hundred-dollar bill, you cannot escape the thrill (or chill) of earning your first few pay-checks! And this is arguably one of the primary transformations you’ll experience as you transition from being a student to a professional in your chosen place of work. But as Uncle Ben advised Peter Parker (aka Spiderman), with great power comes great responsibility…What’s fantastic is, that the power is yours to wield and whether you want to do so responsibly or not, is entirely your choice!",
        "url": "/career/money-get-away-get-a-good-job-with-more-pay/"
      }
      ,
    
      "career-to-the-goth-kids": {
        "title": "To the Goth Kids",
        "author": "cariappa",
        "category": "career",
        "content": "It seems like only recently, startups would hire folks to do just about everything, all at once, and then somehow, we’ve arrived at a ‘Product Manager of Paytm Experience’. How tunnel vision syndrome is eroding the startup spirit.Recently, a startup valued at around half a billion dollars laid off 10 of its Product Managers, leaving them with another 30. Just last year, the same startup hired Product guys at 20–30% premium from the market at average CTCs well over ₹20L. Even more intriguing, some of these guys were techie turned MBAs with less than four years’ experience. Having always been on the sales side of things, where one had to justify a minimum of 4x one’s salary to the company, I was fascinated by this lot. What did they do that was worth that much money?One product guy I spoke to said he managed Paytm experience; which meant he had to ensure there were no drop-offs when the user chose to pay through Paytm. He also said he was mandated to prioritise payment through Paytm. And there were similar folks for each of the other alternate payment processes. “But, is Paytm the most competitive payment solution?”, I asked. He didn’t care, really. All that mattered was ‘mukesh23’ got through paying on the platform without, gods forbid, choosing to click on the refresh button. Even better if he came through one of the exclusive Paytm promotions that he had brokered with his counterpart from the other side.Evidently, the metric for success — # completed transactions, is not entirely off-base. A good product guy could save the company millions, potentially. But, on closer observation it seems as though his chutzpah might also cost the company millions, if not more. In the above case, there are several problems with how the roles are structured. What if Paytm wasn’t the best payment option on the platform? What if (plain conjecture, here) it were costlier, for instance? What if these users exited the native platform at rates higher than average?Let’s leave those seemingly troubling questions aside for a moment. What does one do to improve a third party payment experience? Mainly, vary size and placement of the button, apparently. Turns out, there isn’t much you can do. But, that doesn’t mean you can’t do nothing. So, if you notice a needless “improvement” in your experience, know that some Product Manager’s review is forthcoming. Then, is it a surprise, really, that when things take a turn for the worse, these lot are first in the line of fire? I was surprised, though, to understand that a lot of them were entirely at peace with the transitional nature of their employment. 30% elsewhere, then.This is not restricted to Product folks alone, although it does seem like in recent years it has become a “get rich scheme” of sorts for some techies with an acute propensity to bullshit their way through things. I see Marketing Managers who can’t / won’t write a line of copy or tweak keywords on their website. I see Designers who can’t / won’t code simple HTML or work on user personas and flow maps. I see Engineers who can’t / won’t test their code or learn how to write coherent software requirement specifications. This is manifestly due to the over-specialisation of roles and warped organisation structures in these startups. Ergo, the bleeding disinterest.It wasn’t always like this. It used to be that startups hired people to do just about everything, all at once.Fresh out of college, I was hired as a ‘Management Trainee’, which I came to realise was code for will do whatever the hell it takes to move the needle. In the first year alone I did Sales, Product, Operations, and Marketing. I wasn’t alone; it seemed like everybody did everything. I remember our VP-Technology managing client delivery for a new initiative, and doing a damn good job at that. I remember our Operations Manager writing bizarre VBA Macros for Excel that saved hours of effort. And it seemed everybody everywhere else, too, were running an arm and a leg short of the work that was on their plates. It was synonymous with startups.It made tremendous business sense, too. First, it was easier to find (and afford) people at the median levels of overlapping skill sets than at the top 1% of their specialisations. Second, people always had a macro view of the company’s goals and everybody, more or less, aligned their personal work accordingly. Third, it had unexpected, yet, massive pay-offs for our chosen specialisations: techies wrote better code because they understood business and sales guys were more effective because they knew the real implications of that code. It wasn’t easy, but those years probably had the greatest impact in my life. It schooled my thoughts and perspectives.When people ask me what changed over the years, I give them the following analogy: a startup, back in the day, was like a goth band. It attracted the misfits, the weirdos, and those of us who just happened to stumble into the mosh pit. There was a ton of work to do and very little real money to be made. You belonged to somewhat of a cult and expected unreasonable things of yourself and your brethren. What we lacked in resources we made up for with ingenuity and perseverance. Over the years, however, the goth band got a makeover. We couldn’t have been more thrilled at that time. It seemed like the World was finally giving us our due. We didn’t have to explain to mothers, uncles and landlords, what we did for a living. We could finally afford EMIs.But, gradually, the goth kids turned cool. The makeup, now, seemed superficial and the cult constantly disowned its own — for how much wisdom can be gained from tweaking button sizes over years? The law of diminishing marginal returns applied: increasing number of new members to the cult caused the marginal product of others to be smaller than the marginal product of the previous members at this point. And that’s how we got ‘Product Manager of Paytm Experience’.But, what of the goth kids? They do lurk around. You won’t find them in conferences or hackathons or by the vending machines mooching off’ the free stuff. They’re likely in an intimate corner, head immersed in the laptop, being productive and maybe checking on Twitter once a while. If you’re ever in the position of hiring for your startup, my suggestion is for you to find and hire the goth kid. He’ll remind you why you started up in the first place. And also, he won’t ask you about your company’s pet policy.  This article was originally published  on Medium. We are republishing it here with the permission of the author",
        "url": "/career/to-the-goth-kids/"
      }
      ,
    
      "technology-setting-up-elb-plus-nginx-with-https-offloading": {
        "title": "Setting up OAuth2 callbacks in Rails with HTTPS offloading on load balancers",
        "author": "sivapraveen,akash",
        "category": "technology",
        "content": "“HTTPS everywhere” is not a luxury anymore. It is a necessity. Thankfully, obtaining an SSL certificate has become easier too, with initiatives such as Let’s Encrypt, GeoTrust, Positive SSL, StartSSL. Even cloud based services such as Cloudflare and Amazon AWS provide free SSL certificates to their customers.####Here is setting some context to help the reader appreciate the discussion:We host our rails applications on Amazon AWS. We generally use three different environments - development, staging and production. Development environment is generally local to a developer while staging and production are hosted on the cloud. There is a minor difference in the way we configure our staging and production environments. Our staging environment typically contains a single machine instance hosting our application. This single instance is exposed to internet directly (has a public IP). On the other hand, our production environment typically contains a cluster of instances for the sake of horizontal scaling. These instances typically do not have a public IP and hence not exposed to internet directly. We put this cluster behind an internet-facing Elastic Load Balancer (ELB).We use chef-solo to manage our cloud infrastructure as well as to deploy code to various environments.#####The Problem Statement:For the sake of this discussion, we shall limit ourselves to configuring SSL certificates obtained from the two free providers, namely Let’s Encrypt and Amazon AWS.Using Let’s Encrypt in a clustered setup is tricky, since you need to make one of the instances stateful, in the sense, one instance needs to be given the responsibility of obtaining and renewing SSL certificate from Let’s Encrypt. All other instances need to copy this certificate every time its renewed. This requirement unnecessarily complicates the setup and also takes away some amount of flexibility. Also, Let’s Encrypt does not issue wildcard certificates and the validity of a certificate is just 90 daysThe certificates provisioned from the other provider, Amazon AWS, can only be installed on an ELB. Hence is best suited for our clustered setup, namely production. An added advantage is that Amazon can issue wildcard certificates. We could always add an ELB to our staging environment (even though we will never have more than one instance), but that costs extra money for no reason.This leaves us with these options      Environment    Best Option        Staging    Let's Encrypt        Production    Amazon AWS  We went ahead with this choice. Using chef to manage our setup came handy.We first configured our Staging environment and everything worked as expected.However, the same application, in production environment, started throwing CSRF detected Error whenever an OAuth2 callback happened. This was really strange. Our application integrated with two different OAuth providers, and the problem was consistent with both these providers.#####What’s the issue?The only difference between our Staging and Production setups was the ELB.In production, we offloaded HTTPS at the ELB. Plain HTTP request would hit the NGINX web server, which in turn would reverse-proxy it to unicorn and rails.CSRF detected was clearly an error emitting from the rails application. Not from NGINX, and not from the ELB.A closer look would reveal that the rails application had no way to know if the callback was made on a http:// URL or a https:// URL, because it sees only HTTP (due to offloading).  Was this the reason rails was unhappy?OAuth2, by design, does not accept plain HTTP callbacks (unless it is to localhost).####How do we move forward?#####PoC to prove the theoryJust to confirm what we think is the cause, we enabled HTTPS on NGINX (like we did in our staging environment). This was in addition to HTTPS on the Load balancer. We reconfigured the Load Balancer to NOT offload HTTPS but forward the request as-is to NGINX.What do we have now? The CSRF detected errors are gone. Application behaves just like it should.This confirmed our theory.But the question now is, how do we achieve our desired configuration of offloading HTTPS at the ELB ? Is it just not possible ?The SolutionWe have been using X-Forwarded-For header while reverse proxying to unicorn so that our rails application knows the client IP address (rather than the IP address of the Load Balancer). We need this for logging and tracking.Could there be something on similar lines to tell the rails application that the request was not on HTTP but on HTTPS?Sure there is. We had to set a header in our reverse proxy configuration:X-Forwarded-Proto  to  httpsFor NGINX, we do it like this:  proxy_set_header X-Forwarded-Proto https;Voila, Rails is happy and things are back to normal!Details:Csrf detected!Rails bothers about SSL only at two places,   At environment config, force_ssl.  At external included Gem like Omniauth. In Rails environment config.  config.force_ssl = trueThis does the trick, but doesn’t seem like a good idea to enable this option in Rails because, we offload https at NGINX. For Rails, request came in http, so it does a permanent redirect to https, which ends in a infinite loop.Our stack trace gave a clue that error might be inside omniauth gem.    actionpack-4.2.7.1/lib/abstract_controller/base.rb:132 → process    actionview-4.2.7.1/lib/action_view/rendering.rb:30 → process    actionpack-4.2.7.1/lib/action_controller/metal.rb:196 → dispatch    actionpack-4.2.7.1/lib/action_controller/metal/rack_delegation.rb:13 → dispatch    actionpack-4.2.7.1/lib/action_controller/metal.rb:237 → block in action    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:74 → dispatch    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:43 → serve    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:43 → block in serve    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:30 → each    actionpack-4.2.7.1/lib/action_dispatch/journey/router.rb:30 → serve    actionpack-4.2.7.1/lib/action_dispatch/routing/route_set.rb:817 → call    omniauth-1.3.1/lib/omniauth/strategy.rb:186 → call!    omniauth-1.3.1/lib/omniauth/strategy.rb:164 → callAs we dug inside the Gem and found out that Omniauth looks at these headerslib/omniauth/strategy.rb#L493-L499def ssl?  request.env[&#39;HTTPS&#39;] == &#39;on&#39; ||  request.env[&#39;HTTP_X_FORWARDED_SSL&#39;] == &#39;on&#39; ||  request.env[&#39;HTTP_X_FORWARDED_SCHEME&#39;] == &#39;https&#39; ||  (request.env[&#39;HTTP_X_FORWARDED_PROTO&#39;] &amp;&amp; request.env[&#39;HTTP_X_FORWARDED_PROTO&#39;].split(&#39;,&#39;)[0] == &#39;https&#39;) ||  request.env[&#39;rack.url_scheme&#39;] == &#39;https&#39;endThis is where we found that setting up X_FORWARDED_PROTO to https should fix our problems.Initially, this X_FORWARDED_PROTO was set to $scheme. Which will be http for production as https is offloaded at ELB.Now, by setting X_FORWARDED_PROTO to https, we are making sure that redirects are happening on https.",
        "url": "/technology/setting-up-elb-plus-nginx-with-https-offloading/"
      }
      ,
    
      "technology-custom-capacity-buffers-in-go": {
        "title": "Custom Capacity Buffers In Go",
        "author": "surendranath",
        "category": "technology",
        "content": "At elitmus we use ruby to create most of our tools and most of our applications  are also written in ruby. Recently I started exploring ways to build our tools especially the backend tools in languages other than ruby which have much lesser memory footprint and better efficiency. One such cases was to create a sandboxed environment for running  untrusted code on our servers. After evaluating multiple languages, I decided to use golang because of it’s excellent library support coupled with the fact the docker(a sandboxed env) was also written in go.One of the many challenges we faced while creating our sandbox was  redirection of the standard output of untrusted code, as this simple code below will fill up the disk if redirected to file or use all system resources if redirected to a buffer.1while(true) printf(“I am the green monster”);So the problem is,how to limit the size of a file or buffer?, I  started with   buffers  as they are more easy to implement.  I assumed that the Write method of the Buffer struct which writes to the buffer, will panic with ErrTooLarge error if buffer size is above it’s capacity, which i hoped to catch using recover builtin function.This is the code snippet below. 1   defer func() { 2      if r := recover(); r != nil { 3         fmt.Println(&quot;Should catch if anyone panics&quot;) 4      } 5   }() 6  a := bytes.NewBuffer(make([]byte, 0, 2)) 7  for { 8    _,err := a.Write([]byte(&quot;create boom&quot;)) 9    if err != nil {10      fmt.Println(err.Error())11       return12    }1314  }On running this code, my system was frozen and crashed a little later. This is not what i expected, On further investigation by looking to source code and reading the bytes package documentation again, i found out that Write method in the bytes package is growing the capacity of the  buffer if the buffer capacity is not enough, which in turn is increasing the amount of memory and resources used by the system.After some googling and with good help from the go community(thanks to dave cheney), i decided  to create wrapper around the buffer struct and implement my own io.Writer interface by implementing Write method for the wrapper which writes to the buffer.My custom wrapper’s will take capacity as parameter when initializing and the Write method will do the required action if there is a buffer overflow, instead of increasing the capacity like the Write method from bytes package. This is done by monitoring the size of the buffer before writing to the buffer.This is code snippet of my custom wrapper. 1type MyBuffer struct { 2    cap   int 3    mybuf *bytes.Buffer 4} 5 6func (b *MyBuffer) Write(p []byte) (n int, err error) { 7    if len(p)+b.mybuf.Len() &gt; b.cap { 8        fmt.Printf(b.mybuf.String()) 9        panic(&quot;Buffer Overflow&quot;)10    } else {11        b.mybuf.Write(p)12    }13    return len(p), nil14}1516func NewBuffer(buf []byte, cap int) *MyBuffer {17    return &amp;MyBuffer{mybuf: bytes.NewBuffer(buf), cap: cap}18}1920func main() {2122    defer func() {23        if r := recover(); r != nil {24            fmt.Println(&quot;recover in yes&quot;)25        }26    }()2728    a := NewBuffer(make([]byte, 0, 100), 200)29    for {30        _, err := a.Write([]byte(&quot;Check for Buffer Overflow&quot;))31        if err != nil {32            fmt.Println(err.Error())33            return34        }35    }36}On running this code, it worked as expected, hopefully will be deployed in production.The same goes for files as well.Note: useful links, on docker,on golang bytes package",
        "url": "/technology/custom-capacity-buffers-in-go/"
      }
      ,
    
      "technology-making-airtel-3g-dongle-work-on-mac-os-10-dot-10-yosemite": {
        "title": "Making Airtel 3G dongle work on Mac OS 10.10 Yosemite",
        "author": "shireeshj",
        "category": "technology",
        "content": "If you use Airtel 3G Dongle (Mine is Huawei E173) on your Mac, and are having issue using the dongel after upgrading to Yosemite, airtel is of little help. They asked me to downgrade the OS to Mavericks!The reason why the dialer software provided by airtel does not work is, that they internally use Apple USB Modem. According to this FAQ on apple support site, your Operating system should be running in 32 bit mode for the modem to work. Yosemite however, is 64 bit.Anyway, I could find multiple ways to overcome the problem. Here I am writing about the most simple one###Step 1: Click on this link to download the new compitable driver from Huawei website Mac-V200R003B015D11SP00C983(for Mac10.10).rar###Step 2: Open the archive, you will find two files1. Mobile Partner install user guide.docx2. Mobile Partner.zipThe word document has detailed instructions with screenshots, on how to install.###Step 3: Open the zip file Mobile Partner.zip, you will find Mobile Partner.app. Double click on this file to install the app###Step 4: Once installed, start the app and go to Tools -&gt; Options###Step 5: In the Options window, choose “Profile Management” from the left side menu###Step 6: Click on “New” button to create a new profile. Give it a name, such as “airtel 3g”. Also, make sure the “Access Number” is set to *99#. Click “Save”, then “Ok”.###Step 7: Insert your Dongel into an USB port. You should see “Mobile Partner” application starting automatically. Choose the profile you created in Step 6 (“airtel 3g”) and “Connect”.That’s it.",
        "url": "/technology/making-airtel-3g-dongle-work-on-mac-os-10-dot-10-yosemite/"
      }
      ,
    
      "career-it-career-pitfalls-to-avoid": {
        "title": "IT Career - Pitfalls to avoid",
        "author": "aseem",
        "category": "career",
        "content": "It is very humbling when a youngster walks up to us and says “Thanks for helping me get my first job”. While we are delighted at one end, we are worried at the other.  Why? Because, most of the time a fledgling mind does not see the disaster ahead! Yes, we mean disaster – 75% of IT professionals of 2011-2014 batches will be unemployed 20 years from now.  And this is assuming IT industry does well !! Looks unlikely? Read on to know more.Indian IT industry has employed around 7,50,000 professionals from the four batches (2011, 2012, 2013, 2014). An estimated 1,50,000 of these will leave the Indian IT industry to pursue higher studies and never come back to work for the same industry. That leaves us with 6,00,000 professionals who will be in the industry for long. The question is: How long? Being highly paid with around 20 years of experience, in the year 2030, companies would want them to take larger responsibilities and oversee at least 100 professionals under them. Summing it up, these 6 lakh professionals should have 6 crore professionals below them. Assuming IT industry grows at 10% per annum for 20 years (caution - it may already be slowing down), the whole industry will be just 1.2 crore strong. That means at most 1.2 lakh senior professionals will be needed. What would happen to the rest 4.8 lakh professionals? They would, of course, be unemployed.Difficult to digest? In 1995, there were approximately 11,000 software professionals across all levels. Nearly 50% of them are now citizens of another country or earned enough money from the exponentially growing market (nascent market then) growing market to retire, appropriately called VIP (vested in peace). Another at most 15,500 professionals of the same era migrated into IT industry from other industries (like SAP consultants, Supply Chain, Financial professionals). So that is a conservative 21,000 senior professionals in the whole of Indian IT industry. Many of these who lose a job today struggle to find another suitable profile (20 Yrs of experience) and this is when growth rates in this period have been over 25%. You can see it happening for the current 40+ year old professionals!!Now you have a lingering doubt – could there be something wrong in the projections? Yes !! But it is unfortunately on the negative side. What if the industry grows slower than 10% (may be another bad patch of no growth for 2-4 years). What if automation makes many more jobs redundant (now in IT itself, think about it!)? Last but not the least, another country taking away jobs from India (like China did in manufacturing)?A tell-a-tale from not very long ago is the textile mills of Bombay. They were teeming with activity and nothing could go wrong for them in 1970s and early 1980s. It could only get better as population was growing and people’s ability to spend was increasing. These very mills today are malls!! It can be argued to be a ‘crowding out’ phenomenon, surely not applicable to sunrise IT industry. Or maybe it is visible only in hindsight !!!What are we doing at eLitmus to help the cause?      We are pushing companies not to lower the entry barriers. We have found that immediately after a slow down year, quality and quantity of candidates improve. Quantity ok, but how quality? You call it competition, you call it lowered demand or call it buyers market. So if companies can adopt the quality principals in this period, why not in growth phase as well. It will help students also.We strongly believe a youngster`s ability to adapt and evolve is much higher than an older person. So push them today rather than tomorrow. If you remember your grandparents had the fitness and ability to walk kilometres at their old age (not spoilt by automobiles when they were a child)        Educating students that the easy path out, though rosy for short term, will destroy them. We want them to go that extra mile. That explains our rigorous question paper which tests fundamentals and concepts. We want them to earn their job rather than get it. In the process their ability goes up. Few students who wrote pH test in the initial years of eLitmus have founded their own start-ups.        Ensuring start-ups and companies with great work environment do not struggle for lack of talent. Most of these engagements are loss making. We survive thanks to the fact that most of our colleagues at eLitmus are passionate about what they do and work at a fraction of their market salary!!  We are committed to “making India competitive” and we hope we have challenged the young reader of this blog to go the extra mile. As Steve Jobs once quoted the Whole Earth catalogue “Stay hungry, Stay foolish!”",
        "url": "/career/it-career-pitfalls-to-avoid/"
      }
      ,
    
      "technology-how-we-host-our-blog-on-github-pages-and-yet-serve-it-from-our-own-sub-url": {
        "title": "How we host our blog on GitHub pages and yet serve it from our own Sub-URL",
        "author": "mohitnegi",
        "category": "technology",
        "content": "There are umpteen number of blog posts telling you how to host your static site on GitHub Pages for free. They also tell you how to serve such a site from your own domain name.As you may have guessed, this blog is also hosted on GH Pages. Don’t believe me? try visiting this URL https://shireeshj.github.io/blog/It is easy to map a github.io url such as this, to a subdomain. For example, it is easy to map the url to https://blog.elitmus.com/blog/.  All you need to do is check-in a file named CNAME into the root folder of your git repo that contains your static site.What if you want your static site to be served from domain apex? That is easy too.  GitHub pages help explains this in a simple manner.However, If your domain apex is already taken, say by your other website, you have a problem.  To host your static site on a domain apex (or a sub-url of domain apex) the domain apex should be available exclusively for use by github pages.We had to overcome this very problem, since our business website is already hosted on elitmus.com (and www.elitmus.com).  Given that we are not in great love with subdomains. We had to find a workaround. And here is what we did:Since we use nginx to server our business website, all we had to do was to write a simple traffic-cop rule. What this rule did was, to parse the request url to see if it starts with /blog/. If yes, then the request is reverse proxied to GitHub Pages. If no, then it is served from local disk.The relevant lines from the config file are here\tlocation /blog/ {\t    proxy_pass       http://shireeshj.github.io/;\t    proxy_redirect off;\t    proxy_set_header Host &lt;shireeshj.github.io&gt;;\t    proxy_set_header X-Host &lt;shireeshj.github.io&gt;;;\t    proxy_set_header X-Real-IP $remote_addr;\t    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\t  }This is how we get free hosting for our blog, yet serve it from our official URL. If GitHub pages ever stops us from reverse proxying, we shall simply spin our own webserver to run this static site and reverse proxy to that web server.",
        "url": "/technology/how-we-host-our-blog-on-github-pages-and-yet-serve-it-from-our-own-sub-url/"
      }
      ,
    
      "technology-using-monit-to-get-email-alert-on-unauthorized-login": {
        "title": "Using Monit to get email alert on unauthorized login",
        "author": "shireeshj",
        "category": "technology",
        "content": "For a long time, we had our own custom written perl script to alert us whenever someone logged into our production servers from an ip address we do not recognize (not whitelisted). The script looked somewhat like this…#!/usr/bin/perl# script file: alert_on_login.pl#my $login_str = &quot;Accepted publickey&quot;;my $whitelist_ip = &quot;122.123.123.111&quot;;sub sendEmail{        my ($to, $from, $subject, $message) = @_;        my $sendmail = &#39;/usr/lib/sendmail&#39;;        open(MAIL, &quot;|$sendmail -oi -t&quot;);        print MAIL &quot;From: $from\\n&quot;;        print MAIL &quot;To: $to\\n&quot;;        print MAIL &quot;Subject: $subject\\n\\n&quot;;        print MAIL &quot;$message\\n&quot;;        close(MAIL);}while (&lt;&gt;) {        if (grep(/$login_str/, $_) &amp;&amp; !grep(/$whitelist_ip/, $_)) {                print $_;                chomp $_;                @arr = split(&#39; &#39;, $_);                sendEmail(&#39;recepient1@elitmus.com, recepient2@elitmus.com&#39;,                          &#39;monit@elitmus.com&#39;,                          &#39;Server login from &#39; . $arr[10],                          $_);        }}All we needed to do was to run this script in the background as a daemon, and it would send us an email alert whenever someone logged in successfully. As root user start the script like this:  # (perl alert_on_login.pl /var/log/auth.log &amp;)Ever since we started using monit for the usual purpose (monitoring processes), we have also entrusted monit to do the job of the above perl script. Monit makes this super simple…Monit is a popular opensource process monitoring tool. It is used mostly for monitoring health of any linux process and take necessary action if any of the set parameters are breached. Monit can restart a process if the process failed for some reason. Monit can also notify you of incidents and actions taken.See this to learn more about monit’s alert capabilities.Monit’s global configuration file is usually /etc/monit/monitrc. Here is what monit needs to be told about how to send email alerts:...# This is our SMTP server settings. The complete syntax is# SET MAILSERVER &lt;hostname [PORT] [USERNAME] [PASSWORD] [using SSLAUTO|SSLV2|SSLV3|TLSV11|TLSV12] [CERTMD5 checksum]&gt;, ...#          [with TIMEOUT X SECONDS]#          [using HOSTNAME hostname]## But for our purpose, localhost is good enoughSET mailserver localhost# This is the email template for alert messagesSET mail-format {  from: monit@elitmus.com  subject: $SERVICE $EVENT at $DATE  message: Monit $ACTION $SERVICE at $DATE on $HOST: $DESCRIPTION.           Yours sincerely,           monit}# Alerts can be triggered for various reasons. Successful ssh login is just one of those reasons.# Since this is a global configuration, we can tell monit to not send alerts for certain events#  We also specify the email address of the recepient who will receive the alertsset alert recepient1@elitmus.com NOT ON { action, instance, pid, ppid, nonexist }...And then we add this config file ssh_logins.conf specific to sshd related stuff:check file ssh_logins with path /var/log/auth.log  ignore match &quot;/etc/monit/whitelist_ips.regex&quot;  if match &quot;Accepted publickey&quot; then alertNotice how we tell monit to ignore logins from known ip addresses. We can now store all whitelist ip addresses in a separate file /etc/monit/whitelist_ips.regex, one address per line.Note: We have disabled password based login and hence do not monitor for passworded logins. If you use passworded login, you should change \"Accepted publickey\" to \"Accepted password\"Happy monitoring!",
        "url": "/technology/using-monit-to-get-email-alert-on-unauthorized-login/"
      }
      ,
    
      "technology-gotchas-while-syntactically-translating-aes-encryption-logic-from-php-to-ruby": {
        "title": "Gotcha's while syntactically translating AES encryption logic from PHP to Ruby",
        "author": "surendranath",
        "category": "technology",
        "content": "Our Payment Gateway service provider recently launched a new platform with some nice-to-have features. We wanted those features and so we decided to migrate. Being one of the earliest adopters of the new platform, there was no integration kit available. We had to build it ourselves. Not a problem. Since we are a Ruby On Rails shop, we built our own Ruby integration kit. All went well and we pushed it to production.A month or two later, we got an email from our gateway provider seeking our help with writing the encryption and decryption logic for the Ruby integration kit they were developing. We were a little surprised, because we noticed they had already published integration kits for PHP, Python, JAVA etc. How difficult can it be to translate that to Ruby?Turns out, syntactic transalation of code from one programming language to another does not always work. A slightly more deeper knowledge helps. We could almost guess where they were getting stuck.Before we get to the story, some backgroung on the encryption algo will add clarity.For secure communication between our server and the gateway, the prescribed cipher was AES, specifically symmetric-key block cipher with a 128 bit secret key in CBC mode. Since OpenSSL already implements this algo and is avaliable on almost all platforms, most programming languages just bundle a wrapper for OpenSSL.So if its the same OpenSSL that the wrappers call, why couldn’t the gateway service provider translate their own PHP code to Ruby?Here is why:AES works by breaking the plain text (the text to be encrypted) into blocks of 128 bits (or 16 bytes). In CBC mode, each block is XORed with the key to get cipher text of that block. The cipher text of the previous block is used for encrypting the next block… so on and so forth, until all the blocks are encrypted.Note that the length of the cipher text will be exactly same as that of the plain text.The problem occures with the last block. If the length of the plain text is not a multiple of 128. the last block will be shorter than 128 bits. Since the algo can work only on blocks of 128 bits, It is a common practice to pad the last block so that it becomes equal to 128 bits in lenght. This padding is subsequently discarded after decryption.Note: The actual algo is more complicated than this. We have deliberately left out details that are not relevent for this post.This is the encryption method in the PHP integration kit published by the gateway service provider 1function encrypt($plainText,$key) 2{ 3  $secretKey = hextobin(md5($key)); 4  $initVector = &quot;...&quot;; 5  $openMode = mcrypt_module_open(MCRYPT_RIJNDAEL_128, &#39;&#39;,&#39;cbc&#39;, &#39;&#39;); 6  $blockSize = mcrypt_get_block_size(MCRYPT_RIJNDAEL_128, &#39;cbc&#39;); 7 8  $plainPad = pkcs5_pad($plainText, $blockSize);  //  &lt;---- Padding 910  if (mcrypt_generic_init($openMode, $secretKey, $initVector) != -1) 11  {12    $encryptedText = mcrypt_generic($openMode, $plainPad);13    mcrypt_generic_deinit($openMode);      14  } 15  return bin2hex($encryptedText);16}1718// Padding method19function pkcs5_pad ($plainText, $blockSize)20{21  // padding logic here22}And here is the same implemented in Ruby1def self.encrypt(plain_text, key)2    secret_key     = Digest::MD5.digest(key)3    cipher         = OpenSSL::Cipher::AES.new(128, :CBC)4    cipher.encrypt5    cipher.key     = secret_key6    cipher.iv      = INIT_VECTOR7    encrypted_text = cipher.update(plain_text) + cipher.final8    return (encrypted_text.unpack(&quot;H*&quot;)).first9endNotice any difference?It turns out that, unlike in Python, PHP and few other languages, Ruby wrapper for OpenSSL automatically takes care of padding (default behaviour). This is clearly mentioned in the documentation. For some reason, techies at our gateway service provider overlooked this and hit a dead-end.By the they, they were gracious enough to acknowledge our contribution in their Ruby Integration Kit (accessible only to their subscribers)But We have open sourced our code here ‘cca_crypto’. We have plans of make this into a complete package - with view generators etc., and publish this as a rubygem. We shall gladly accept any pull request!",
        "url": "/technology/gotchas-while-syntactically-translating-aes-encryption-logic-from-php-to-ruby/"
      }
      ,
    
      "technology-setting-up-amazon-rds-as-a-slave-to-a-self-managed-mysql-server": {
        "title": "Setting Up Amazon RDS as a Slave to a self-managed MySQL server",
        "author": "shireeshj",
        "category": "technology",
        "content": "Last week, we migrated our MySQL database server, which was running on an EC2 instance, to RDS. We hoped the migration process would be smooth.As always, migrating a large database has its challenges. Business folks expect the minimum possible downtime.The plan was simple.  Launch an RDS instance  Load a full dump into it  Configure it to act as a slave of the self-managed server (current master)  On the D-day, pull the website down and promote the RDS instance to take over as the new masterWe soon discovered that RDS comes with curtailed root permissions. There are several commands that are disallowed. Some of these include “CHANGE MASTER TO….”What do we do now?One option was to carry out the migration in one go, while the website was offline. This meant the downtime would have been several hours, instead of minutes. Obviously, not an acceptable option at all.Some R&amp;D was all it took to discover how to proceed with the original approach.RDS comes with a bunch of stored procedures, which help you configure it as a slave. There is almost a one-to-one mapping of these stored procedures with the commands that are disallowed.MySQL CommandCorrosponding Stored ProcCHANGE MASTER TOmysql.rds_set_external_masterSTART SLAVEmysql.rds_start_replicationSTOP SLAVEmysql.rds_stop_replicationRESET MASTERmysql.rds_reset_external_master So, Using these stored procedures, you can now configure your RDS instance as a slave to your self-managed MySQL serverAfter loading a full dump to RDS, Call the stored procedure mysql.rds_set_external_master like thisCALL mysql.rds_set_external_master ('servername', port, 'user', 'password', 'binlog-file', binlog-offset, 0);ThenCALL mysql.rds_start_replication;This will make RDS a slave of your self managed mysql server. You can run “SHOW SLAVE STATUS” to see its working.When it is time to promote RDS to master. You call these stored proceduresCALL mysql.rds_stop_replication;CALL mysql.rds_reset_external_master;That’s it. Now point your applications to the RDS instance and take your site live.Note:For your RDS to work as a slave, it needs permissions to connect to port 3306 of your current master. Make sure you open this port for the RDS instance.You can run the following command to find out the ip address of your rds instanceping -c rdsname.cpesx66wwe7y.ap-southeast-1.rds.amazonaws.com",
        "url": "/technology/setting-up-amazon-rds-as-a-slave-to-a-self-managed-mysql-server/"
      }
      ,
    
      "technology-beware-of-creating-ssh-folder-by-hand-when-selinux-is-turned-on": {
        "title": "Beware of creating $HOME/.ssh folder by hand, when SELinux is turned on",
        "author": "shireeshj",
        "category": "technology",
        "content": "I was experimenting with chef to manage our Linux boxes. As a standard practice, our application user deployer is homed in /applications/deployer rather than the usual /home/deployer.To enable password less login, I appended my public key to ~/.ssh/authorized_keys ssh-copy-id -i ~/.ssh/id_rsa deployer@remote.serverThe first time I run this command, I will be prompted for a password to install my key. After this, I can run the below command to login without a password:ssh -i ~/.ssh/id_rsa deployer@remote.serverHowever, that did not work as expected.For some reason, sshd was unable to read the authorized_keys file. I checked all the usual things.. all looked fine. Everything seem to work just fine when SELinux was running in permissive mode on the remote server, but not when it was in enforcing mode.Discovered that if .ssh folder was created by hand (or even the folder containing .ssh folder), we need to do few additional things.Step 1:Open this file /etc/selinux/targeted/contexts/files/file_contexts.homedirs and append the following line to the bottom /applications/deployer/[^/]*/\\.ssh(/.*)?     system_u:object_r:ssh_home_t:s0Note: remember to adjust the path as per your needs.Step 2: run the following commandrestorecon -R -v /applications/deployer/.sshAgain, remember to adjust the path as per your needs.Now you are all set! ssh -i ~/.ssh/id_rsa deployer@remote.servershould log you in without asking for a password!",
        "url": "/technology/beware-of-creating-ssh-folder-by-hand-when-selinux-is-turned-on/"
      }
      ,
    
      "technology-importance-of-date-field-in-an-emails-header": {
        "title": "Importance of Date field in an email's Header",
        "author": "shireeshj",
        "category": "technology",
        "content": "So far, we paid little attention to email delivery issues. We knew delivering to rediffmail is a pain. So we discouraged our users from using rediffmail. Apart from that we had FCrDNS and SPF configured and working fine. We had also configured DKIM. And then a month ago, we also added DMARC in monitor mode.We were happy! Until…Recently, we started getting loads of phishing emails from what appeared to originate from our own domain name [not our servers].It told us two things.  eLitmus.com was growing in popularity  We cannot ignore email delivery issue any longerWe ran our email through Spam Assassin checks and were surprised to see that we got a score of 6. Anything above 5 is BAD. It’s a straight spam! But we knew we were not spamming. These were transactional emails triggered by our website on certain events, such as New registration, or Forgot Password.It was almost by accident, we noticed that the timezone in the Date header of the email was appearing as +0580. Indian Standard Time (IST) is 5 hours and 30 minutes ahead of UTC. So this value should have been +0530, not +0580. Apparently, that is good enough reason for Spam Assassin to treat our mails as spam.Tracing backwards, we discovered a bug in our application code and fixed it. It was a single line fix.With this change, Spam Assassin was happy to give us a score of zero.That is just one part of one header. There are ten others which have to be configured correctly.Here is an article with good insights in to how gmail calculates sender reputation. Its a little dated, but still relevent. Sender reputation in a large webmail service (PDF)By the way, here is a nice and free JSon API to check your email’s reputation.",
        "url": "/technology/importance-of-date-field-in-an-emails-header/"
      }
      
    
  };
</script>

<script src="/blog/assets/js/lunr.min.js"></script>
<script src="/blog/assets/js/search.js"></script>

			</div>

		</div>
		<footer>
			<div class="container">
				<p class="center-text small"><a>** Testing puropse only **</a></p>
				<p class="center-text small"><a>** credits: cloudcannon **</a></p>
			</div>
		</footer>

	</body>
</html>
